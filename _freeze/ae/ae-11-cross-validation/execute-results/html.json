{
  "hash": "17876bc4f68aa6b8e71c4f19a7193fa9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 11: Cross validation\"\ndate: \"Oct 23, 2023\"\neditor: visual\n---\n\n\n::: callout-important\nGo to the [course GitHub organization](https://github.com/sta210-fa23) and locate your `ae-11` repo to get started.\n\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 26 at 11:59pm.\n:::\n\n## Model statistics function\n\nYou will use this function to calculate $Adj. R^2$, AIC, and BIC in the cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalc_model_stats <- function(x) {\n  glance(extract_fit_parsnip(x)) |>\n    select(adj.r.squared, AIC, BIC)\n}\n```\n:::\n\n\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n```\n:::\n\n\n## Load data and relevel factors\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntips <- read_csv(\"data/tip-data.csv\")\n\ntips <- tips |>\n  mutate(Age = factor(Age, levels = c(\"Yadult\", \"Middle\", \"SenCit\")), \n         Meal = factor(Meal, levels = c(\"Lunch\", \"Dinner\", \"Late Night\"))\n  )\n```\n:::\n\n\n## Split data into training and testing\n\nSplit your data into testing and training sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10232023)\ntips_split <- initial_split(tips)\ntips_train <- training(tips_split)\ntips_test <- testing(tips_split)\n```\n:::\n\n\n## Specify model\n\nSpecify a linear regression model. Call it `tips_spec`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntips_spec <- linear_reg() |>\n  set_engine(\"lm\")\n\ntips_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n# Model 1\n\n## Create recipe\n\nCreate a recipe to use `Party`, `Age`, and `Meal` to predict `Tip`. Call it `tips_rec1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntips_rec1 <- recipe(Tip ~ Party + Age + Meal,\n                    data = tips_train) |>\n  step_dummy(all_nominal_predictors()) |>\n  step_zv(all_predictors())\n\ntips_rec1\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Operations \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Dummy variables from: all_nominal_predictors()\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n• Zero variance filter on: all_predictors()\n```\n\n\n:::\n:::\n\n\n## Preview recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprep(tips_rec1) |>\n  bake(tips_train) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 126\nColumns: 6\n$ Party           <dbl> 3, 2, 2, 4, 2, 7, 4, 3, 2, 4, 1, 2, 2, 1, 2, 1, 2, 3, …\n$ Tip             <dbl> 4.00, 4.92, 5.09, 8.84, 3.09, 15.00, 8.00, 4.00, 5.00,…\n$ Age_Middle      <dbl> 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, …\n$ Age_SenCit      <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ Meal_Dinner     <dbl> 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, …\n$ Meal_Late.Night <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, …\n```\n\n\n:::\n:::\n\n\n## Create workflow\n\nCreate the workflow that brings together the model specification and recipe. Call it `tips_wflow1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntips_wflow1 <- workflow() |>\n  add_model(tips_spec) |>\n  add_recipe(tips_rec1)\n\ntips_wflow1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n# Cross validation\n\n## Create folds\n\nCreate 5 folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make 10 folds\nset.seed(10232023)\nfolds <- vfold_cv(tips_train, v = 5)\n```\n:::\n\n\n## Conduct cross validation\n\nConduct cross validation on the 5 folds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model and performance statistics for each iteration\ntips_fit_rs1 <- tips_wflow1 |>\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n```\n:::\n\n\n## Take a look at `tips_fit_rs1`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntips_fit_rs1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits           id    .metrics         .notes           .extracts       \n  <list>           <chr> <list>           <list>           <list>          \n1 <split [100/26]> Fold1 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n2 <split [101/25]> Fold2 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n3 <split [101/25]> Fold3 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n4 <split [101/25]> Fold4 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n5 <split [101/25]> Fold5 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n```\n\n\n:::\n:::\n\n\n## Summarize assessment CV metrics\n\nSummarize assessment metrics from your CV iterations These statistics are calculated using the *assessment set*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(tips_fit_rs1, summarize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.09      5  0.265  Preprocessor1_Model1\n2 rsq     standard   0.673     5  0.0519 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n::: callout-tip\nSet `summarize = FALSE` to see the individual $R^2$ and RMSE for each iteration.\n:::\n\n## Summarize model fit CV metrics\n\nSummarize model fit statistics from your CV iterations These statistics are calculated using the *analysis set.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>\n  summarise(mean_adj_rsq = mean(adj.r.squared), \n            mean_aic = mean(AIC), \n            mean_bic = mean(BIC))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  mean_adj_rsq mean_aic mean_bic\n         <dbl>    <dbl>    <dbl>\n1        0.670     434.     453.\n```\n\n\n:::\n:::\n\n\n::: callout-tip\nRun the first line of code `map_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]])` to see the individual $Adj. R^2$, AIC, and BIC for each iteration.\n:::\n\n# Another model - Model 2\n\nCreate the recipe for a new model that includes `Party`, `Age`, `Meal`, and `Alcohol` (an indicator for whether the party ordered alcohol with the meal). Conduct 10-fold cross validation and summarize the metrics.\n\n## Model 2: Recipe\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Model 2: Model building workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Model 2: Conduct CV\n\n::: callout-note\nWe will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Model 2: Summarize assessment CV metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Model 2: Summarize model fit CV metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n# Compare and choose a model\n\n-   Describe how the two models compare to each other based on cross validation metrics.\n\n-   Which model do you choose for the final model? Why?\n\n## Fit the selected model\n\nFit the selected model using the entire training set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n::: callout-tip\nSee [notes](https://sta210-fa23.netlify.app/slides/12-model-workflow#/fit-model-to-training-data) for example code.\n:::\n\n## Evaluate the performance of the selected model on the testing data\n\n### Calculate predicted values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n### Calculate $RMSE$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n::: callout-tip\nSee notes [notes](https://sta210-fa23.netlify.app/slides/12-model-workflow#/make-predictions-for-testing-data) for example code.\n:::\n\n-   How does the model performance on the testing data compare to its performance on the training data?\n\n-   Is this what you expected? Why or why not?\n\n# Submission\n\n::: callout-important\nTo submit the AE:\n\n-   Render the document to produce the PDF with all of your work from today's class.\n-   Push all your work to your `ae-11` repo on GitHub. (You do not submit AEs on Gradescope).\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}