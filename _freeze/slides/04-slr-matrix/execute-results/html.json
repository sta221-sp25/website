{
  "hash": "9741cf46a221df28f009b36c639cdd92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"SLR: Matrix representation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-01-21\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 04-slr-matrix-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   Lab 01 due on **TODAY at 11:59pm**\n\n    -   Push work to GitHub repo\n\n    -   Submit final PDF on Gradescope + mark pages for each question\n\n-   HW 01 will be assigned on Thursday\n\n## Topics\n\n-   Application exercise on model assessment\n-   Matrix representation of simple linear regression\n    -   Model form\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Model assessment\n\n## Two statistics {.midi}\n\n-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n    $$\n    RMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n    $$\n\n-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n$$R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}$$\n\n# Application exercise\n\n::: appex\nðŸ“‹ [sta221-sp25.netlify.app/ae/ae-01-model-assessment.html](../ae/ae-01-model-assessment.html){.uri}\n\nOpen `ae-01` from last class. Complete Part 2.\n:::\n\n# Matrix representation of simple linear regression\n\n## SLR: Statistical model (population)\n\nWhen we have a quantitative response, $Y$, and a single quantitative predictor, $X$, we can use a **simple linear regression** model to describe the relationship between $Y$ and $X$.\n\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\n<br>\n\n-   $\\beta_1$: Population (true) slope of the relationship between $X$ and $Y$\n-   $\\beta_0$: Population (true) intercept of the relationship between $X$ and $Y$\n-   $\\epsilon$: Error terms centered at 0 with variance $\\sigma^2_{\\epsilon}$\n\n## SLR in matrix form\n\nThe simple linear regression model to describe the relationship between the response $Y$ and single quantitative predictor $X$ can be represented as\n\n::: equation\n$$\n\\large{\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}}\n$$\n:::\n\n## SLR in matrix form \n\n$$\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n= \n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n$$\n\n<br>\n\n::: question\nWhat are the dimensions of $\\mathbf{y}$, $\\mathbf{X}$, $\\boldsymbol{\\beta}$, and $\\boldsymbol{\\epsilon}$?\n:::\n\n## Derive least squares estimator for $\\boldsymbol{\\beta}$\n\n**Goal**: Find estimator $\\hat{\\boldsymbol{\\beta}}= \\begin{bmatrix}\\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix}$ that minimizes the sum of squared errors $$\n\\sum_{i=1}^n \\epsilon_i^2 = \\mathbf{\\epsilon}^\\mathsf{T}\\mathbf{\\epsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n\n## Gradient {.midi background-color=\"#ccddeb\"}\n\nLet $\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\x_k\\end{bmatrix}$be a $k \\times 1$ vector and $f(\\mathbf{x})$ be a function of $\\mathbf{x}$.\n\n. . .\n\nThen $\\nabla_\\mathbf{x}f$, the **gradient** of $f$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x}f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_k}\\end{bmatrix}\n$$\n\n## Property 1 {background-color=\"#ccddeb\"}\n\nLet $\\mathbf{x}$ be a $k \\times 1$ vector and $\\mathbf{z}$ be a $k \\times 1$ vector, such that $\\mathbf{z}$ is not a function of $\\mathbf{x}$ .\n\nThe gradient of $\\mathbf{x}^\\mathsf{T}\\mathbf{z}$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^\\mathsf{T}\\mathbf{z} = \\mathbf{z}\n$$\n\n## Side note: Property 1 {background-color=\"#ccddeb\"}\n\n$$\n\\begin{aligned}\n\\mathbf{x}^\\mathsf{T}\\mathbf{z} &= \\class{fragment}{\\begin{bmatrix}x_1 & x_2 & \\dots &x_k\\end{bmatrix}\n \\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\z_k\\end{bmatrix}} \\\\[10pt]\n &\\class{fragment}{= x_1z_1 + x_2z_2 + \\dots + x_kz_k} \\\\\n&\\class{fragment}{= \\sum_{i=1}^k x_iz_i}\n\\end{aligned}\n$$\n\n## Side note: Property 1 {.midi background-color=\"#ccddeb\"}\n\n$$\n\\nabla_\\mathbf{x}\\hspace{1mm}\\mathbf{x}^\\mathsf{T}\\mathbf{z} = \\class{fragment}{\\begin{bmatrix}\\frac{\\partial \\mathbf{x}^\\mathsf{T}\\mathbf{z}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{x}^\\mathsf{T}\\mathbf{z}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{x}^\\mathsf{T}\\mathbf{z}}{\\partial x_k}\\end{bmatrix}}  \n= \\class{fragment}{\\begin{bmatrix}\\frac{\\partial}{\\partial x_1} (x_1z_1 + x_2z_2 + \\dots + x_kz_k) \\\\ \\frac{\\partial}{\\partial x_2} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_k} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\end{bmatrix}}\n = \\class{fragment}{\\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_k\\end{bmatrix} = \\mathbf{z}}\n$$\n\n## Property 2 {background-color=\"#ccddeb\"}\n\nLet $\\mathbf{x}$ be a $k \\times 1$ vector and $\\mathbf{A}$ be a $k \\times k$ matrix, such that $\\mathbf{A}$ is not a function of $\\mathbf{x}$ .\n\nThen the gradient of $\\mathbf{x}^\\mathsf{T}\\mathbf{A}\\mathbf{x}$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^\\mathsf{T}\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x} + \\mathbf{A}^\\mathsf{T} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^\\mathsf{T})\\mathbf{x}\n$$\n\nIf $\\mathbf{A}$ is symmetric, then\n\n$$\n(\\mathbf{A} + \\mathbf{A}^\\mathsf{T})\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n$$\n\n::: question\nProof in HW 01.\n:::\n\n## Derive least squares estimator\n\nFind $\\hat{\\boldsymbol{\\beta}}$ that minimizes\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^\\mathsf{T} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T})(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\n## Derive least squares estimator\n\n$$\\begin{aligned}\n \\nabla_{\\beta}\\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} &= \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n & = -2\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}\n \\end{aligned}\n$$<br>\n\nFind $\\hat{\\boldsymbol{\\beta}}$ that satisfies\n\n$$\n-2\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}\n$$\n\n::: equation\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}$$\n:::\n\n# Did we find a minimum? \n\n## Hessian matrix {background-color=\"#ccddeb\"}\n\nThe **Hessian** matrix, $\\nabla_\\mathbf{x}^2f$ is a $k \\times k$ matrix of partial second derivatives\n\n$$\n\\nabla_{\\mathbf{x}}^2f = \\begin{bmatrix} \\frac{\\partial^2f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_1\\partial x_k} \\\\ \n\\frac{\\partial^2f}{\\partial\\ x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2f}{\\partial x_2 \\partial x_k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n\\frac{\\partial^2f}{\\partial x_k\\partial x_1} & \\frac{\\partial^2f}{\\partial x_k\\partial x_2} & \\dots & \\frac{\\partial^2f}{\\partial x_k^2} \\end{bmatrix}\n$$\n\n## Using the Hessian matrix {background-color=\"add8e6\"}\n\nIf the Hessian matrix is...\n\n-   positive-definite, then we have found a minimum.\n\n-   negative-definite, then we have found a maximum.\n\n-   neither positive or negative-definite, then we have found a saddle point\n\n## Did we find a minimum? \n\n$$\n\\begin{aligned}\n\\nabla^2_{\\boldsymbol{\\beta}} \\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} &= \\nabla_{\\boldsymbol{\\beta}} (-2\\mathbf{X}^\\mathsf{T}\\mathbf{y} + 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&{=-2\\nabla_{\\boldsymbol{\\beta}}(\\mathbf{X}^\\mathsf{T}\\mathbf{y}) + 2\\nabla_{\\boldsymbol{\\beta}}(\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\mathbf{\\beta})} \\\\[10pt]\n &{\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}}\n\\end{aligned}\n$$\n\n::: question\nShow that $\\mathbf{X}^\\mathsf{T}\\mathbf{X}$ is positive definite in HW 01.\n:::\n\n# Predicted values and residuals\n\n## Predicted (fitted) values\n\nNow that we have $\\hat{\\boldsymbol{\\beta}}$, let's predict values of $\\mathbf{y}$ using the model\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n$$\n\n. . .\n\n::: equation\n**Hat matrix**: $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}$\n:::\n\n. . .\n\n-   $\\mathbf{H}$ is an $n\\times n$ matrix\n-   Maps vector of observed values $\\mathbf{y}$ to a vector of fitted values $\\hat{\\mathbf{y}}$\n-   It is only a function of $\\mathbf{X}$ not $\\mathbf{y}$\n\n## Residuals\n\nRecall that the residuals are the difference between the observed and predicted values\n\n$$\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}} \\\\[10pt]\n&\\class{fragment}{ = \\mathbf{y} - \\mathbf{H}\\mathbf{y}} \\\\[20pt]\n\\class{fragment}{\\color{#993399}{\\mathbf{e}}} &\\class{fragment}{\\color{#993399}{=(\\mathbf{I} - \\mathbf{H})\\mathbf{y}}} \\\\[10pt]\n\\end{aligned}\n$$\n\n## Recap\n\n-   Introduced matrix representation for simple linear regression\n\n    -   Model form\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}