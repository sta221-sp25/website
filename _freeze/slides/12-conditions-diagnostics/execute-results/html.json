{
  "hash": "c41aa214f0f37d398747e1b1a8d2add4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model conditions + diagnostics\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-02-25\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[üîó STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 12-conditions-diagnostics-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n## Announcements\n\n-   Exam corrections (optional) due Tuesday, March 4 at 11:59pm on Canvas\n\n-   Project proposal due TODAY at 11:59pm\n\n## Computing set up\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n\n## Topics\n\n-   Model conditions\n\n-   Influential points\n\n-   Model diagnostics\n\n    -   Leverage\n\n    -   Studentized residuals\n\n    -   Cook's Distance\n\n## Data: Duke lemurs {.midi}\n\nToday's data contains a subset of the original Duke Lemur data set available in the [TidyTuesday GitHub repo](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-08-24/readme.md). This data includes information on ‚Äúyoung adult‚Äù lemurs from the [Coquerel‚Äôs sifaka species](https://lemur.duke.edu/discover/meet-the-lemurs/coquerels-sifaka/) (PCOQ), the largest species at the Duke Lemur Center. The analysis will focus on the following variables:\n\n-   `age_at_wt_mo`: Age in months: Age of the animal when the weight was taken, in months (((Weight_Date-DOB)/365)\\*12)\n\n-   `weight_g`: Weight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights \\>500g generally to the nearest 1-20g.\n\n**The goal of the analysis is to use the age of the lemurs to understand variability in the weight.**\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n## EDA\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n\n## EDA\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Fit model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_fit <- lm(weight_g ~ age_at_wt_mo, data = lemurs)\n\ntidy(lemurs_fit) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term         |   estimate| std.error| statistic| p.value|\n|:------------|----------:|---------:|---------:|-------:|\n|(Intercept)  | -12314.360|  4252.696|    -2.896|   0.005|\n|age_at_wt_mo |    496.591|   131.225|     3.784|   0.000|\n\n\n:::\n:::\n\n\n\n\n# Model conditions\n\n## Assumptions for regression {.midi}\n\n$$\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n$$\n\n1.  **Linearity:** There is a linear relationship between the response and predictor variables.\n2.  **Constant Variance:** The variability about the least squares line is generally constant.\n3.  **Normality:** The distribution of the errors (residuals) is approximately normal.\n4.  **Independence:** The errors (residuals) are independent from one another.\n\n. . .\n\n::: question\nHow do we know if these assumptions hold in our data?\n:::\n\n## Linearity {.midi}\n\n-   Look at plot of residuals versus fitted (predicted) values.\n-   Linearity is satisfied if there is no discernible pattern in the plot (i.e., points randomly scattered around $residuals = 0$\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n. . .\n\n<center>**Linearity is** **satisfied**</center>\n\n## Linearity not satisfied\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n. . .\n\n-   If linearity is not satisfied, examine the plots of residuals versus each predictor.\n-   Add higher order term(s), as needed.\n\n## Constant variance {.midi}\n\n-   Look at plot of residuals versus fitted (predicted) values.\n-   Constant variance is satisfied if the vertical spread of the points is approximately equal for all fitted values\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n. . .\n\n<center>**Constant variance is** **satisfied**</center>\n\n## Constant variance\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n. . .\n\n-   Condition is critical for inference\n\n-   Address violations by applying transformation on the response\n\n## Normality {.midi}\n\n-   Look at the distribution of the residuals\n-   Normality is satisfied if the distribution is approximately unimodal and symmetric. Inference robust to violations if $n > 30$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=65%}\n:::\n:::\n\n\n\n\n. . .\n\n<center>Distribution approximately unimodal and symmetric, aside from the outlier. There are 62 observations, so inference robust to departures.</center>\n\n## Independence\n\n-   We can often check the independence condition based on the context of the data and how the observations were collected.\n\n-   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\n\n-   If data has spatial element, plot residuals on a map to examine potential spatial correlation.\n\n. . .\n\n<center>**The independence condition is** **satisfied**. The lemurs could reasonably be treated as independent.</center>\n\n# Model diagnostics\n\n## Model diagnostics\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_aug <- augment(lemurs_fit)\n\nlemurs_aug |> slice(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 √ó 8\n   weight_g age_at_wt_mo .fitted .resid   .hat .sigma  .cooksd .std.resid\n      <dbl>        <dbl>   <dbl>  <dbl>  <dbl>  <dbl>    <dbl>      <dbl>\n 1     3400         32.0   3557. -157.  0.0302   494. 0.00164      -0.324\n 2     3620         33.0   4063. -443.  0.0399   491. 0.0176       -0.922\n 3     3720         32.4   3800.  -80.0 0.0163   495. 0.000224     -0.164\n 4     4440         32.6   3850.  590.  0.0177   489. 0.0132        1.21 \n 5     3770         31.8   3457.  313.  0.0458   493. 0.0102        0.652\n 6     3920         31.9   3522.  398.  0.0350   492. 0.0124        0.826\n 7     4520         32.8   3979.  541.  0.0279   490. 0.0180        1.12 \n 8     3700         33.2   4177. -477.  0.0626   491. 0.0337       -1.01 \n 9     3690         31.9   3537.  153.  0.0329   494. 0.00172       0.318\n10     3790         32.8   3949. -159.  0.0247   494. 0.00136      -0.328\n```\n\n\n:::\n:::\n\n\n\n\n## Model diagnostics in R {.midi}\n\nUse the `augment()` function in the broom package to output the model diagnostics (along with the predicted values and residuals)\n\n-   response and predictor variables in the model\n-   `.fitted`: predicted values\n-   `.se.fit`: standard errors of predicted values\n-   `.resid`: residuals\n-   `.hat`: leverage\n-   `.sigma`: estimate of residual standard deviation when the corresponding observation is dropped from model\n-   `.cooksd`: Cook's distance\n-   `.std.resid`: standardized residuals\n\n## Influential Point\n\nAn observation is **influential** if removing has a noticeable impact on the regression coefficients\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n## Influential points\n\n::: incremental\n-   Influential points have a noticeable impact on the coefficients and standard errors used for inference\n-   These points can sometimes be identified in a scatterplot if there is only one predictor variable\n    -   This is often not the case when there are multiple predictors\n-   We will use measures to quantify an individual observation's influence on the regression model\n    -   **leverage**, **standardized & studentized residuals**, and **Cook's distance**\n:::\n\n# Cook's Distance\n\n## Motivating Cook's Distance\n\n-   An observation's influence on the regression line depends on\n\n    -   How close it lies to the general trend of the data\n\n    -   Its leverage\n\n-   **Cook's Distance** is a statistic that includes both of these components to measure an observation's overall impact on the model\n\n## Cook's Distance\n\nCook's distance for the $i^{th}$ observation is\n\n$$\nD_i = \\frac{r^2_i}{p + 1}\\Big(\\frac{h_{ii}}{1 - h_{ii}}\\Big)\n$$\n\nwhere $r_i$ is the studentized residual and $h_{ii}$ is the leverage for the $i^{th}$ observation\n\n. . .\n\nThis measure is a combination of\n\n-   How well the model fits the $i^{th}$ observation (magnitude of residuals)\n\n-   How far the $i^{th}$ observation is from the rest of the data (where the point is in the $x$ space)\n\n## Using Cook's Distance\n\n-   An observation with large value of $D_i$ is said to have a strong influence on the predicted values\n\n-   General thresholds .An observation with\n\n    -   $D_i > 0.5$ is **moderately influential**\n\n    -   $D_i > 1$ is **very influential**\n\n## Cook's Distance\n\nCook's Distance is in the column `.cooksd` in the output from the `augment()` function\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\n## Comparing models {.midi}\n\n**With influential point**\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term         |   estimate| std.error| statistic| p.value|\n|:------------|----------:|---------:|---------:|-------:|\n|(Intercept)  | -12314.360|  4252.696|    -2.896|   0.005|\n|age_at_wt_mo |    496.591|   131.225|     3.784|   0.000|\n\n\n:::\n:::\n\n\n\n\n<br>\n\n**Without influential point**\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term         |  estimate| std.error| statistic| p.value|\n|:------------|---------:|---------:|---------:|-------:|\n|(Intercept)  | -6670.958|  3495.136|    -1.909|   0.061|\n|age_at_wt_mo |   321.209|   107.904|     2.977|   0.004|\n\n\n:::\n:::\n\n\n\n\n. . .\n\n::: question\nLet's better understand the influential point.\n:::\n\n# Leverage\n\n## Leverage {.midi}\n\n::: incremental\n-   Recall the **hat matrix** $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}$\n\n-   We focus on the diagonal elements\n\n    $$\n    h_{ii} = \\mathbf{x}_i^\\mathsf{T}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{x}_i\n    $$such that $\\mathbf{x}^\\mathsf{T}_i$ is the $i^{th}$ row of $\\mathbf{X}$\n\n-   $h_{ii}$ is the **leverage**: a measure of the distance of the $i^{th}$ observation from the center (or centroid) of the $x$ space\n\n-   Observations with large values of $h_{ii}$ are far away from the typical value (or combination of values) of the <u>predictors</u> in the data\n:::\n\n## Large leverage\n\n::: incremental\n-   The sum of the leverages for all points is $p + 1$, where $p$ is the number of predictors in the model . More specifically\n\n    $$\n    \\sum_{i=1}^n h_{ii} = \\text{rank}(\\mathbf{H}) = \\text{rank}(\\mathbf{X}) = p+1\n    $$\n\n-   The average value of leverage, $h_{ii}$, is $\\bar{h} =  \\frac{(p+1)}{n}$\n\n-   An observation has **large leverage** if $$h_{ii} > \\frac{2(p+1)}{n}$$\n:::\n\n## Lemurs: Leverage\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_threshold <- 2 * 2 / nrow(lemurs)\nh_threshold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06451613\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_aug |>\n  filter(.hat > h_threshold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 8\n  weight_g age_at_wt_mo .fitted .resid   .hat .sigma .cooksd .std.resid\n     <dbl>        <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1     4040         33.5   4336.  -296. 0.107    493.  0.0244     -0.639\n2     6519         33.4   4272.  2247. 0.0871   389.  1.10        4.79 \n```\n\n\n:::\n:::\n\n\n\n\n<br>\n\n. . .\n\n::: question\nWhy do you think these points have large leverage?\n:::\n\n## Let's look at the data\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n## Large leverage\n\nIf there is point with high leverage, ask\n\n-   ‚ùì Is there a data entry error?\n\n-   ‚ùì Is this observation within the scope of individuals for which you want to make predictions and draw conclusions?\n\n-   ‚ùì Is this observation impacting the estimates of the model coefficients? (Need more information!)\n\n. . .\n\nJust because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore we need to check other measures.\n\n# Scaled residuals\n\n## Scaled residuals\n\n::: incremental\n-   What is the best way to identify outlier points that don't fit the pattern from the regression line?\n\n    -   Look for points that have large residuals\n\n-   We can rescale residuals and put them on a common scale to more easily identify \"large\" residuals\n\n-   We will consider two types of scaled residuals: standardized residuals and studentized residuals\n:::\n\n## Standardized residuals\n\n::: incremental\n-   The variance of the residuals can be estimated by the mean squared residuals (MSR) $= \\frac{SSR}{n - p - 1} = \\hat{\\sigma}^2_{\\epsilon}$\n\n-   We can use MSR to compute **standardized residuals**\n\n    $$\n    std.res_i = \\frac{e_i}{\\sqrt{MSR}}\n    $$\n\n-   Standardized residuals are produced by `augment()` in the column `.std.resid`\n:::\n\n------------------------------------------------------------------------\n\n## Using standardized residuals\n\nWe can examine the standardized residuals directly from the output from the `augment()` function\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/std residuals vs fitted-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n-   An observation is a *potential outlier* if its standardized residual is beyond $\\pm 3$\n\n## Digging in to the data\n\nLet's look at the value of the response variable to better understand potential outliers\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n\n## Studentized residuals {.midi}\n\n::: incremental\n-   MSR is an approximation of the variance of the residuals.\n\n-   The variance of the residuals is $Var(\\mathbf{e}) = \\sigma^2_{\\epsilon}(\\mathbf{I} - \\mathbf{H})$\n\n    -   The variance of the $i^{th}$ residual is $Var(e_i) = \\sigma^2_{\\epsilon}(1 - h_{ii})$\n\n-   The **studentized residual** is the residual rescaled by the more exact calculation for variance\n:::\n\n$$\nr_i = \\frac{e_{i}}{\\sqrt{\\hat{\\sigma}^2_{\\epsilon}(1 - h_{ii})}}\n$$\n\n-   Standardized and studentized residuals provide similar information about which points are outliers in the response.\n    -   Studentized residuals are used to compute Cook's Distance.\n\n## Using these measures\n\n-   Standardized residuals, leverage, and Cook's Distance should all be examined together\n\n-   Examine plots of the measures to identify observations that are outliers, high leverage, and may potentially impact the model.\n\n## Back to the influential point \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-conditions-diagnostics_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## What to do with outliers/influential points?\n\n::: incremental\n-   First consider if the outlier is a result of a data entry error.\n\n-   If not, you may consider dropping an observation if it's an outlier in the <u>predictor</u> variables if...\n\n    -   It is meaningful to drop the observation given the context of the problem\n\n    -   You intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions\n:::\n\n## What to do with outliers/influential points?\n\n::: incremental\n-   It is generally **not** good practice to drop observations that ar outliers in the value of the <u>response</u> variable\n\n    -   These are legitimate observations and should be in the model\n\n    -   You can try transformations or increasing the sample size by collecting more data\n\n-   A general strategy when there are influential points is to fit the model with and without the influential points and compare the outcomes\n:::\n\n------------------------------------------------------------------------\n\n## Recap\n\n-   Model conditions\n\n-   Influential points\n\n-   Model diagnostics\n\n    -   Leverage\n\n    -   Studentized residuals\n\nCook's Distance\n",
    "supporting": [
      "12-conditions-diagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}