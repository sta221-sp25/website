{
  "hash": "722e39aa4778d7369fc61bde33578575",
  "result": {
    "markdown": "---\ntitle: \"Multinomial logistic regression\"\nsubtitle: \"Prediction + inference\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2023-11-27\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\n  warning: false\n  message: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements {.midi}\n\n-   Due dates\n\n    -   HW 05 due Wed, Nov 29 at 11:59pm\n\n-   Project\n\n    -   (Optional) [Round 1 submission](https://sta210-fa23.netlify.app/project-instructions#round1-submission) due Fri, Dec 01 at 11:59pm\n    -   [Presentations](https://sta210-fa23.netlify.app/project-instructions#presentation) in lab Dec 5 & 7\n\n-   Exam 02 dates\n\n    -   In-class: Mon, Dec 4\n    -   Take-home: Mon, Dec 4 - Wed, Dec 6\n    -   Review: Wed, Nov 29\n\n-   [Click here](https://docs.google.com/spreadsheets/d/1dUD4SOpM79BL2kdvue9I2TP-ibcdxfpOIkHkMreYx64/edit?usp=sharing) to access lecture recordings. Available until Mon, Dec 04 at 9am\n\n## Statistician of the day: Mike Dairyko {.small}\n\n::: columns\n::: {.column width=\"60%\"}\n*Mike Dairyko was a student at Pomona College where he studied Mathematics. Through a linear algebra class, he found his way to two different summer REU programs and eventually to a PhD in Applied Mathematics from Iowa State University (2018). Being introduced to machine learning methods caused him to pursue data science jobs after graduation.*\n\n*Dr.Â Dairyko served as a Senior Manager of Data Science at the Milwaukee Brewers and is now the Director of Ticketing Analytics at the Milwaukee Bucks. Helping the organization get the most out of budgeting, revenue, and ticket sales allows him to fully use his training in mathematics and data science.*\n:::\n\n::: {.column width=\"40%\"}\n![](images/statistician-of-the-day/dairyko.jpg){fig-alt=\"Headshot of Mike Dairyko\" width=\"70%\"}\n:::\n:::\n\nSource: [hardin47.github.io/CURV/scholars/dairyko](https://hardin47.github.io/CURV/scholars/dairyko.html)\n\n## Statistician of the day {.small}\n\nAbout his role as a Senior Manager of Data Science for the MilwaukeeBrewers:\\\n\\\n*\"During the season, one of my main priorities is to produce game-by-game ticket and revenue projections. To do so, my group incorporates historical data---such as team performance, weather, and schedules---into multiple regression-based models and then consolidates the outputs in an easily-digestible format. A large codebase both automates and maintains this process; the codebase is regularly tweaked to ensure that it is agile enough to handle the constant usage and flow of new information.\"\\\n\\\n\"Our projections are most accurate when we utilize both qualitative and quantitative forecasts.\"\\\n*\n\n::: aside\nSource: [From Academia to Major League Baseball: The Journey of a Data Scientist](https://sinews.siam.org/Details-Page/from-academia-to-major-league-baseball-the-journey-of-a-data-scientist)\n:::\n\n## Statistician of the day {.small}\n\nAbout his role as a Senior Manager of Data Science for the Milwaukee Brewers:\n\n*\"Mathematicians are ultimately trained to develop problem-solving skills and apply them with persistence and creativity...Carefully reviewing the work---and perhaps redoing it a different way or approaching the issue from another angle---eventually leads to success...I do use the problem-solving strategies, persistence, and creativity that I have honed throughout my mathematical journey every single day.\"*\n\n*\"I would encourage students to become comfortable with **navigating a programming language such as R or Python**...Briefly stepping outside of mathematics and **establishing computer science and statistics expertise** is also useful...Finally, **participating in conferences with data science content** is an excellent way to gain exposure to more advanced topics in the field and build a network within the community.\"*\n\n::: aside\nSource: [From Academia to Major League Baseball: The Journey of a Data Scientist](https://sinews.siam.org/Details-Page/from-academia-to-major-league-baseball-the-journey-of-a-data-scientist)\n:::\n\n## Topics\n\n::: nonincremental\n-   Predictions\n-   Model selection\n-   Checking conditions\n:::\n\n## Computational setup\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(colorblindr)\nlibrary(pROC)\nlibrary(Stat2Data)\nlibrary(nnet)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))\n```\n:::\n\n\n## NHANES Data\n\n::: nonincremental\n-   [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/index.htm) is conducted by the National Center for Health Statistics (NCHS).\n-   The goal is to *\"assess the health and nutritional status of adults and children in the United States\".*\n-   This survey includes an interview and a physical examination.\n:::\n\n## Variables\n\n**Goal:** Use a person's age and whether they do regular physical activity to predict their self-reported health rating.\n\n-   Outcome: `HealthGen`: Self-reported rating of participant's health in general. Excellent, Vgood, Good, Fair, or Poor.\n\n-   Predictors:\n\n    -   `Age`: Age at time of screening (in years). Participants 80 or older were recorded as 80.\n    -   `PhysActive`: Participant does moderate to vigorous-intensity sports, fitness or recreational activities.\n\n## The data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnhanes_adult <- NHANES |>\n  filter(Age >= 18) |>\n  select(HealthGen, Age, PhysActive, Education) |>\n  drop_na() |>\n  mutate(obs_num = 1:n())\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglimpse(nhanes_adult)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 6,465\nColumns: 5\n$ HealthGen  <fct> Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, â€¦\n$ Age        <int> 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,â€¦\n$ PhysActive <fct> No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, â€¦\n$ Education  <fct> High School, High School, High School, Some College, Collegâ€¦\n$ obs_num    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, â€¦\n```\n:::\n:::\n\n\n## Model in R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_fit <- multinom_reg() |>\n  set_engine(\"nnet\") |>\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)\n```\n:::\n\n\n## Model summary {.small}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(health_fit) |> kable(digits = 3)\n```\n\n::: {.cell-output-display}\n|y.level |term          | estimate| std.error| statistic| p.value|\n|:-------|:-------------|--------:|---------:|---------:|-------:|\n|Vgood   |(Intercept)   |    1.265|     0.154|     8.235|   0.000|\n|Vgood   |Age           |    0.000|     0.003|    -0.014|   0.989|\n|Vgood   |PhysActiveYes |   -0.332|     0.095|    -3.496|   0.000|\n|Good    |(Intercept)   |    1.989|     0.150|    13.285|   0.000|\n|Good    |Age           |   -0.003|     0.003|    -1.187|   0.235|\n|Good    |PhysActiveYes |   -1.011|     0.092|   -10.979|   0.000|\n|Fair    |(Intercept)   |    1.033|     0.174|     5.938|   0.000|\n|Fair    |Age           |    0.001|     0.003|     0.373|   0.709|\n|Fair    |PhysActiveYes |   -1.662|     0.109|   -15.190|   0.000|\n|Poor    |(Intercept)   |   -1.338|     0.299|    -4.475|   0.000|\n|Poor    |Age           |    0.019|     0.005|     3.827|   0.000|\n|Poor    |PhysActiveYes |   -2.670|     0.236|   -11.308|   0.000|\n:::\n:::\n\n\n# Predictions\n\n## Calculating probabilities {.smaller}\n\n-   Suppose the repsonse variable has $K$ categories and $k = 1$ is the baseline category. For categories $2,\\ldots,K$, the probability that the $i^{th}$ observation is in the $j^{th}$ category is\n\n    $$\n    \\hat{\\pi}_{ij} = \\frac{\\exp\\{\\hat{\\beta}_{0j} + \\hat{\\beta}_{1j}x_{i1} + \\dots + \\hat{\\beta}_{pj}x_{ip}\\}}{1 + \\sum\\limits_{k=2}^K \\exp\\{\\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x_{i1} + \\dots \\hat{\\beta}_{pk}x_{ip}\\}}\n    $$\n\n-   For the baseline category, $k=1$, we calculate the probability $\\hat{\\pi}_{i1}$ as\n\n    $$\n    \\hat{\\pi}_{i1} = 1- \\sum\\limits_{k=2}^K \\hat{\\pi}_{ik}\n    $$\n\n## Predicted health rating {.smaller}\n\nWe can use our model to predict a person's perceived health rating given their age and whether they exercise.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_aug <- augment(health_fit, new_data = nhanes_adult)\nhealth_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,465 Ã— 11\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   <fct>                 <dbl>       <dbl>      <dbl>      <dbl>      <dbl>\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# â„¹ 6,455 more rows\n# â„¹ 5 more variables: HealthGen <fct>, Age <int>, PhysActive <fct>,\n#   Education <fct>, obs_num <int>\n```\n:::\n:::\n\n\n## Actual vs. predicted health rating\n\nFor each observation, the predicted perceived health rating is the category with the highest predicted probability.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_aug |> select(contains(\"pred\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6,465 Ã— 6\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   <fct>                 <dbl>       <dbl>      <dbl>      <dbl>      <dbl>\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# â„¹ 6,455 more rows\n```\n:::\n:::\n\n\n## Confusion matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_conf <- health_aug |> \n  count(HealthGen, .pred_class, .drop = FALSE) |>\n  pivot_wider(names_from = .pred_class, values_from = n)\n\nhealth_conf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 6\n  HealthGen Excellent Vgood  Good  Fair  Poor\n  <fct>         <int> <int> <int> <int> <int>\n1 Excellent         0   528   210     0     0\n2 Vgood             0  1341   743     0     0\n3 Good              0  1226  1316     0     0\n4 Fair              0   296   625     0     0\n5 Poor              0    24   156     0     0\n```\n:::\n:::\n\n\n## Actual vs. predicted health rating\n\n::: question\nWhy do you think no observations were predicted to have a rating of \"Excellent\", \"Fair\", or \"Poor\"?\n:::\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-11-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## ROC curves\n\nROC curves for multiclass outcomes use a one-vs-all approach: calculate multiple curves, one per level vs.Â all other levels.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_aug |> \n  roc_curve(\n    truth = HealthGen, \n    .pred_Excellent:.pred_Poor\n  ) |> \n  autoplot()\n```\n:::\n\n\n## ROC curves\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## ROC curve: under the hood\n\nAn additional column, `.level`, identifies the \"one\" column in the one-vs-all calculation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhealth_aug |> \n  roc_curve(\n    truth = HealthGen, \n    .pred_Excellent:.pred_Poor\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 620 Ã— 4\n   .level    .threshold specificity sensitivity\n   <chr>          <dbl>       <dbl>       <dbl>\n 1 Excellent  -Inf          0             1    \n 2 Excellent     0.0681     0             1    \n 3 Excellent     0.0682     0.00664       0.997\n 4 Excellent     0.0682     0.0138        0.992\n 5 Excellent     0.0683     0.0192        0.989\n 6 Excellent     0.0683     0.0243        0.981\n 7 Excellent     0.0684     0.0297        0.970\n 8 Excellent     0.0684     0.0353        0.969\n 9 Excellent     0.0684     0.0426        0.951\n10 Excellent     0.0685     0.0492        0.947\n# â„¹ 610 more rows\n```\n:::\n:::\n\n\n# Model selection for inference\n\n## Comparing nested models {.midi}\n\n-   Suppose there are two models:\n    -   Reduced model includes predictors $x_1, \\ldots, x_q$\n    -   Full model includes predictors $x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p$\n-   We want to test the following hypotheses:\n    -   $H_0: \\beta_{q+1} = \\dots = \\beta_p = 0$\n    -   $H_A: \\text{ at least 1 }\\beta_j \\text{ is not } 0$\n-   To do so, we will use the **drop-in-deviance test** (very similar to logistic regression)\n\n## Add `Education` to the model? {.midi}\n\n-   We consider adding the participants' `Education` level to the model.\n    -   Education takes values `8thGrade`, `9-11thGrade`, `HighSchool`, `SomeCollege`, and `CollegeGrad`\n-   Models we're testing:\n    -   Reduced model: `Age`, `PhysActive`\n    -   Full model: `Age`, `PhysActive`, `Education`\n\n. . .\n\n$$\n\\begin{align}\n&H_0: \\beta_{9-11thGrade} = \\beta_{HighSchool} = \\beta_{SomeCollege} = \\beta_{CollegeGrad} = 0\\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to }0\n\\end{align}\n$$\n\n## Add `Education` to the model?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreduced_fit <- multinom_reg() |>\n  set_engine(\"nnet\") |>\n  fit(HealthGen ~ Age + PhysActive,\n  data = nhanes_adult)\n\nfull_fit <- multinom_reg() |>\n  set_engine(\"nnet\") |>\n  fit(HealthGen ~ Age + PhysActive + Education,\n  data = nhanes_adult)\n```\n:::\n\n\n## Add `Education` to the model? {.midi}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(reduced_fit$fit, full_fit$fit, test = \"Chisq\") |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n|Model                        | Resid. df| Resid. Dev|Test   |    Df| LR stat.| Pr(Chi)|\n|:----------------------------|---------:|----------:|:------|-----:|--------:|-------:|\n|Age + PhysActive             |     25848|   16994.23|       |    NA|       NA|      NA|\n|Age + PhysActive + Education |     25832|   16505.10|1 vs 2 |    16|  489.132|       0|\n:::\n:::\n\n\n. . .\n\nAt least one coefficient associated with `Education` is non-zero. Therefore, we will include `Education` in the model.\n\n## Model with `Education` {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(full_fit, conf.int = T) |> print(n = 28) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 28 Ã— 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   <chr>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n 1 Vgood   (Intercept)   5.82e-1   0.301      1.93   5.36e- 2 -0.00914   1.17   \n 2 Vgood   Age           1.12e-3   0.00266    0.419  6.75e- 1 -0.00411   0.00634\n 3 Vgood   PhysActiveYâ€¦ -2.64e-1   0.0985    -2.68   7.33e- 3 -0.457    -0.0711 \n 4 Vgood   Education9 â€¦  7.68e-1   0.308      2.49   1.27e- 2  0.164     1.37   \n 5 Vgood   EducationHiâ€¦  7.01e-1   0.280      2.51   1.21e- 2  0.153     1.25   \n 6 Vgood   EducationSoâ€¦  7.88e-1   0.271      2.90   3.71e- 3  0.256     1.32   \n 7 Vgood   EducationCoâ€¦  4.08e-1   0.268      1.52   1.28e- 1 -0.117     0.933  \n 8 Good    (Intercept)   2.04e+0   0.272      7.51   5.77e-14  1.51      2.57   \n 9 Good    Age          -1.72e-3   0.00263   -0.651  5.15e- 1 -0.00688   0.00345\n10 Good    PhysActiveYâ€¦ -7.58e-1   0.0961    -7.88   3.16e-15 -0.946    -0.569  \n11 Good    Education9 â€¦  3.60e-1   0.275      1.31   1.90e- 1 -0.179     0.899  \n12 Good    EducationHiâ€¦  8.52e-2   0.247      0.345  7.30e- 1 -0.399     0.569  \n13 Good    EducationSoâ€¦ -1.13e-2   0.239     -0.0472 9.62e- 1 -0.480     0.457  \n14 Good    EducationCoâ€¦ -8.91e-1   0.236     -3.77   1.65e- 4 -1.35     -0.427  \n15 Fair    (Intercept)   2.12e+0   0.288      7.35   1.91e-13  1.55      2.68   \n16 Fair    Age           3.35e-4   0.00312    0.107  9.14e- 1 -0.00578   0.00645\n17 Fair    PhysActiveYâ€¦ -1.19e+0   0.115    -10.4    3.50e-25 -1.42     -0.966  \n18 Fair    Education9 â€¦ -2.24e-1   0.279     -0.802  4.22e- 1 -0.771     0.323  \n19 Fair    EducationHiâ€¦ -8.32e-1   0.252     -3.31   9.44e- 4 -1.33     -0.339  \n20 Fair    EducationSoâ€¦ -1.34e+0   0.246     -5.46   4.71e- 8 -1.82     -0.861  \n21 Fair    EducationCoâ€¦ -2.51e+0   0.253     -9.91   3.67e-23 -3.00     -2.01   \n22 Poor    (Intercept)  -2.00e-1   0.411     -0.488  6.26e- 1 -1.01      0.605  \n23 Poor    Age           1.79e-2   0.00509    3.53   4.21e- 4  0.00797   0.0279 \n24 Poor    PhysActiveYâ€¦ -2.27e+0   0.242     -9.38   6.81e-21 -2.74     -1.79   \n25 Poor    Education9 â€¦ -3.60e-1   0.353     -1.02   3.08e- 1 -1.05      0.332  \n26 Poor    EducationHiâ€¦ -1.15e+0   0.334     -3.44   5.86e- 4 -1.81     -0.494  \n27 Poor    EducationSoâ€¦ -1.07e+0   0.316     -3.40   6.77e- 4 -1.69     -0.454  \n28 Poor    EducationCoâ€¦ -2.32e+0   0.366     -6.34   2.27e-10 -3.04     -1.60   \n```\n:::\n:::\n\n\n## Compare models using AIC & BIC\n\n::: columns\n::: {.column width=\"50%\"}\n**AIC**\n\nReduced model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(reduced_fit)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17018.23\n```\n:::\n:::\n\n\nFull model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(full_fit)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16561.1\n```\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n**BIC**\n\nReduced model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(reduced_fit)$deviance + log(nrow(nhanes_adult)) * glance(reduced_fit)$edf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17099.52\n```\n:::\n:::\n\n\nFull model:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(full_fit)$deviance + log(nrow(nhanes_adult)) * glance(full_fit)$edf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16750.77\n```\n:::\n:::\n\n:::\n:::\n\n# Checking conditions for inference\n\n## Conditions for inference\n\nWe want to check the following conditions for inference for the multinomial logistic regression model:\n\n1.  Linearity: Is there a linear relationship between the log-odds and the predictor variables?\n\n2.  Randomness: Was the sample randomly selected? Or can we reasonably treat it as random?\n\n3.  Independence: Are the observations independent?\n\n## Checking linearity\n\nSimilar to logistic regression, we will check linearity by examining empirical logit plots between each level of the response and the quantitative predictor variables.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnhanes_adult <- nhanes_adult |>\n  mutate(\n    Excellent = factor(if_else(HealthGen == \"Excellent\", \"1\", \"0\")),\n    Vgood = factor(if_else(HealthGen == \"Vgood\", \"1\", \"0\")),\n    Good = factor(if_else(HealthGen == \"Good\", \"1\", \"0\")),\n    Fair = factor(if_else(HealthGen == \"Fair\", \"1\", \"0\")),\n    Poor = factor(if_else(HealthGen == \"Poor\", \"1\", \"0\"))\n  )\n```\n:::\n\n\n## Checking linearity\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nemplogitplot1(Excellent ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Excellent vs. Age\")\n```\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nemplogitplot1(Vgood ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Vgood vs. Age\")\n```\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-23-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Checking linearity\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nemplogitplot1(Good ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Good vs. Age\")\n```\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\nemplogitplot1(Fair ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Fair vs. Age\")\n```\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-24-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n## Checking linearity\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nemplogitplot1(Poor ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Poor vs. Age\")\n```\n\n::: {.cell-output-display}\n![](22-multinomial-logistic-pt2_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n. . .\n\nâœ… The linearity condition is satisfied. There is generally a linear relationship between the empirical logit and the quantitative predictor variable, Age, for each level of the response.\n\n## Checking randomness\n\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\n-   Was the sample randomly selected?\n\n-   If the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n. . .\n\nâœ… The randomness condition is satisfied. The participants were randomly selected, and thus we do not have reason to believe that the participants in this study differ systematically from adults in the U.S.\n\n## Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\n\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n. . .\n\nâœ… The independence condition is satisfied. The participants were randomly selected, so it is reasonable to conclude that the participants' health and behavior characteristics are independent of one another.\n\n## Recap\n\n-   Predictions\n-   Model selection for inference\n-   Checking conditions for inference\n\n## Full multinomial modeling workflow\n\n-   [juliasilge.com/blog/multinomial-volcano-eruptions](https://juliasilge.com/blog/multinomial-volcano-eruptions/)\n\n-   [juliasilge.com/blog/nber-papers](https://juliasilge.com/blog/nber-papers/)\n\n## Questions for Exam 02 review\n\n::: question\nSubmit your questions for the Exam 02 review: <https://forms.office.com/r/cTyeRjVmfb>\\\n\\\nExam 02 will cover multiple linear regression, logistic regression, and multinomial logistic regression.\n:::\n",
    "supporting": [
      "22-multinomial-logistic-pt2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}