{
  "hash": "af04711364af1683e55c0bde05d9617b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression: Prediction\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-04-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 20-logistic-prediction-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   Next project milestone: Analysis and draft in April 11 lab\n\n-   Team Feedback (email from TEAMMATES) due Tuesday, April 8 at 11:59pm (check email)\n\n-   Statistics experience due April 22\n\n## Computational set up\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(pROC)       # make ROC curves\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n\n## Topics\n\n-   Calculating predicted probabilities from the logistic regression model\n\n-   Using predicted probabilities to classify observations\n\n-   Make decisions and assess model performance using\n\n    -   Confusion matrix\n    -   ROC curve\n\n## Data: Risk of coronary heart disease\n\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\n-   `high_risk`: 1 = High risk of having heart disease in next 10 years, 0 = Not high risk of having heart disease in next 10 years\n\n-   `age`: Age at exam time (in years)\n\n-   `totChol`: Total cholesterol (in mg/dL)\n\n-   `currentSmoker`: 0 = nonsmoker; 1 = smoker\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Modeling risk of coronary heart disease\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term           | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:--------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)    |   -6.638|     0.372|   -17.860|   0.000|   -7.374|    -5.917|\n|age            |    0.082|     0.006|    14.430|   0.000|    0.071|     0.093|\n|totChol        |    0.002|     0.001|     2.001|   0.045|    0.000|     0.004|\n|currentSmoker1 |    0.457|     0.092|     4.951|   0.000|    0.277|     0.639|\n\n\n:::\n:::\n\n\n\n\n::: question\n-   Interpret `totChol` in terms of the odds of being high risk for heart disease.\n\n-   Interpret `currentSmoker1` in terms of the odds of being high risk for heart disease.\n:::\n\n## Prediction and classification\n\n::: incremental\n-   We are often interested in using the model to classify observations, i.e., predict whether a given observation will have a 1 or 0 response\n\n-   For each observation\n\n    -   Use the logistic regression model to calculate the predicted log-odds the response for the $i^{th}$ observation is 1\n    -   Use the log-odds to calculate the predicted probability the $i^{th}$ observation is 1\n    -   Then, use the predicted probability to classify the observation as having a 1 or 0 response using some predefined threshold\n:::\n\n## Augmented data frame\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naugment(heart_disease_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,190 Ã— 10\n   high_risk   age totChol currentSmoker .fitted .resid     .hat .sigma  .cooksd\n   <fct>     <dbl>   <dbl> <fct>           <dbl>  <dbl>    <dbl>  <dbl>    <dbl>\n 1 0            39     195 0              -3.06  -0.302 0.000594  0.890  6.94e-6\n 2 0            46     250 0              -2.38  -0.420 0.000543  0.890  1.25e-5\n 3 0            48     245 1              -1.77  -0.560 0.000527  0.890  2.24e-5\n 4 1            61     225 1              -0.751  1.51  0.00164   0.889  8.70e-4\n 5 0            46     285 1              -1.86  -0.539 0.000830  0.890  3.25e-5\n 6 0            43     228 0              -2.67  -0.366 0.000546  0.890  9.43e-6\n 7 1            63     205 0              -1.08   1.66  0.00154   0.889  1.15e-3\n 8 0            45     313 1              -1.88  -0.532 0.00127   0.890  4.86e-5\n 9 0            52     260 0              -1.87  -0.535 0.000542  0.890  2.08e-5\n10 0            43     225 1              -2.22  -0.454 0.000532  0.890  1.44e-5\n# â„¹ 4,180 more rows\n# â„¹ 1 more variable: .std.resid <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n## Predicted log-odds\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheart_disease_aug <- augment(heart_disease_fit)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 1\n  .fitted\n    <dbl>\n1  -3.06 \n2  -2.38 \n3  -1.77 \n4  -0.751\n5  -1.86 \n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Observation 1**\n\n$$\n\\text{predicted log-odds} = \\log\\Big(\\frac{\\hat{\\pi}}{1- \\hat{\\pi}}\\Big) = -3.06\n$$\n\n## Predicted odds\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 1\n  .fitted\n    <dbl>\n1  -3.06 \n2  -2.38 \n3  -1.77 \n4  -0.751\n5  -1.86 \n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Observation 1**\n\n$$\n\\text{predicted odds} =  \\frac{\\hat{\\pi}}{1- \\hat{\\pi}} = \\exp\\{-3.06\\} = 0.0469\n$$\n\n## Predicted probability\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 1\n  .fitted\n    <dbl>\n1  -3.06 \n2  -2.38 \n3  -1.77 \n4  -0.751\n5  -1.86 \n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Observation 1**\n\n$$\n\\text{predicted prob.} = \\hat{\\pi} = \\frac{\\hat{\\text{odds}}}{1+\\hat{\\text{odds}}} = \\frac{\\exp\\{-3.06\\}}{1 + \\exp\\{-3.06\\}}= 0.045\n$$\n\n. . .\n\n::: question\nWould you classify this individual as high risk $(\\hat{y} = 1)$ or not high risk $(\\hat{y} = 0)$?\n:::\n\n## Another individual\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 1\n  .fitted\n    <dbl>\n1  -3.06 \n2  -2.38 \n3  -1.77 \n4  -0.751\n5  -1.86 \n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Observation 4**\n\n$$\n\\text{predicted prob.} = \\hat{\\pi} = \\frac{\\hat{\\text{odds}}}{1+\\hat{\\text{odds}}} = \\frac{\\exp\\{-0.751\\}}{1 + \\exp\\{-0.751\\}}= 0.321\n$$\n\n. . .\n\n::: question\nWould you classify this individual as high risk $(\\hat{y} = 1)$ or not high risk $(\\hat{y} = 0)$?\n:::\n\n## Predicted probabilities in R\n\nWe can calculate predicted probabilities using the `predict.glm()` function. Use `type = \"response\"` to get probabilities.[^1]\n\n[^1]: The default is `type = \"link\"`, which produces the predicted log-odds.\n\n<br>\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict.glm(heart_disease_fit, type = \"response\")\n```\n:::\n\n\n\n\n. . .\n\n**Predicted probabilities for Observations 1 -5**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         1          2          3          4          5 \n0.04459439 0.08445209 0.14523257 0.32065849 0.13515474 \n```\n\n\n:::\n:::\n\n\n\n\n## Predictions in R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_prob <- predict.glm(heart_disease_fit, type = \"response\")\n\nheart_disease_aug <- heart_disease_aug |> \n  bind_cols(pred_prob = pred_prob)\n```\n:::\n\n\n\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 3\n  high_risk .fitted pred_prob\n  <fct>       <dbl>     <dbl>\n1 0          -3.06     0.0446\n2 0          -2.38     0.0845\n3 0          -1.77     0.145 \n4 1          -0.751    0.321 \n5 0          -1.86     0.135 \n```\n\n\n:::\n:::\n\n\n\n\n## Classifying observations\n\n::: question\nYou would like to determine a threshold for classifying individuals as high risk or not high risk.\n\nWhat considerations would you make in determining the threshold?\n:::\n\n## Classify using 0.5 as threshold\n\nWe can use a threshold of 0.5 to classify observations.\n\n-   If $\\hat{\\pi} > 0.5$, classify as 1\n\n-   If $\\hat{\\pi} \\leq 0.5$, classify as 0\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 Ã— 4\n  high_risk .fitted pred_prob pred_class\n  <fct>       <dbl>     <dbl> <fct>     \n1 0          -3.06     0.0446 0         \n2 0          -2.38     0.0845 0         \n3 0          -1.77     0.145  0         \n4 1          -0.751    0.321  0         \n5 0          -1.86     0.135  0         \n```\n\n\n:::\n:::\n\n\n\n\n## Confusion matrix\n\nA **confusion matrix** is a $2 \\times 2$ table that compares the predicted and actual classes. We can produce this matrix using the `conf_mat()` function in the **yardstick** package (part of tidymodels).\n\n<br>\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheart_disease_aug |>\n  conf_mat(high_risk, pred_class) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 3553  635\n         1    2    0\n```\n\n\n:::\n:::\n\n\n\n\n## Visualize confusion matrix\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheart_conf_mat <- heart_disease_aug |>\n  conf_mat(high_risk, pred_class)\n\nautoplot(heart_conf_mat, type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](20-logistic-prediction_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n## Using the confusion matrix\n\n::: center\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 3553  635\n         1    2    0\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<br>\n\n. . .\n\nThe **accuracy** of this model with a classification threshold of 0.5 is\n\n$$\n\\text{accuracy} = \\frac{3553 + 0}{3553 + 635 + 2 + 0} = 0.848\n$$\n\n## Using the confusion matrix\n\n::: center\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 3553  635\n         1    2    0\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<br>\n\n. . .\n\nThe **misclassification rate** of this model with a threshold of 0.5 is\n\n$$\n\\text{misclassification} = \\frac{635 + 2}{3553 + 635 + 2 + 0} = 0.152\n$$\n\n## Using the confusion matrix\n\n::: center\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 3553  635\n         1    2    0\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<br>\n\nAccuracy is 0.848 and the misclassification rate is 0.152.\n\n. . .\n\n::: question\n-   What is the limitation of solely relying on accuracy and misclassification to assess the model performance?\n\n-   What is the limitation of using a single confusion matrix to assess the model performance?\n:::\n\n# Sensitivity and specificity\n\n## True/false positive/negative {.midi}\n\n+----------------------------------------------------------------------------+---------------------------+-----------------------+\n|                                                                            | Not high risk $(y_i = 0)$ | High risk $(y_i = 1)$ |\n+============================================================================+===========================+=======================+\n| **Classified** **not** **high risk** $(\\hat{\\pi}_i \\leq \\text{threshold})$ | True negative (TN)        | False negative (FN)   |\n+----------------------------------------------------------------------------+---------------------------+-----------------------+\n| **Classified high risk** $(\\hat{\\pi}_i > \\text{threshold})$                | False positive (FP)       | True positive (TP)    |\n+----------------------------------------------------------------------------+---------------------------+-----------------------+\n\n<br>\n\n-   $\\text{accuracy} = \\frac{TN + TP}{TN + TP + FN + FP}$\n\n-   $\\text{misclassification} = \\frac{FN + FP}{TN+ TP + FN + FP}$\n\n## False negative rate {.midi}\n\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n|                                                                    | Not high risk $(y_i = 0)$ | High risk $(y_i = 1)$ |\n+====================================================================+===========================+=======================+\n| **Classified not high risk** $(\\hat{\\pi}_i \\leq \\text{threshold})$ | True negative (TN)        | False negative (FN)   |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n| **Classified high risk** $(\\hat{\\pi}_i > \\text{threshold})$        | False positive (FP)       | True positive (TP)    |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n\n<br>\n\n. . .\n\n**False negative rate**: Proportion of actual positives that were classified as negatives\n\n-   P(classified not high risk \\| high risk) = $\\frac{FN}{TP + FN}$\n\n## False positive rate {.midi}\n\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n|                                                                    | Not high risk $(y_i = 0)$ | High risk $(y_i = 1)$ |\n+====================================================================+===========================+=======================+\n| **Classified not high risk** $(\\hat{\\pi}_i \\leq \\text{threshold})$ | True negative (TN)        | False negative (FN)   |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n| **Classified high risk** $(\\hat{\\pi}_i > \\text{threshold})$        | False positive (FP)       | True positive (TP)    |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n\n<br>\n\n. . .\n\n**False positive rate**: Proportion of actual negatives that were classified as positives\n\n-   P(classified high risk \\| not high risk) = $\\frac{FP}{TN + FP}$\n\n## Sensitivity {.midi}\n\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n|                                                                    | Not high risk $(y_i = 0)$ | High risk $(y_i = 1)$ |\n+====================================================================+===========================+=======================+\n| **Classified not high risk** $(\\hat{\\pi}_i \\leq \\text{threshold})$ | True negative (TN)        | False negative (FN)   |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n| **Classified high risk** $(\\hat{\\pi}_i > \\text{threshold})$        | False positive (FP)       | True positive (TP)    |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n\n<br>\n\n. . .\n\n**Sensitivity**: Proportion of actual positives that were correctly classified as positive\n\n-   Also known as *true positive rate* (TPR) and *recall*\n\n-   **P(classified high risk \\| high risk) = 1 âˆ’ False negative rate**\n\n## Specificity {.midi}\n\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n|                                                                    | Not high risk $(y_i = 0)$ | High risk $(y_i = 1)$ |\n+====================================================================+===========================+=======================+\n| **Classified not high risk** $(\\hat{\\pi}_i \\leq \\text{threshold})$ | True negative (TN)        | False negative (FN)   |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n| **Classified high risk** $(\\hat{\\pi}_i > \\text{threshold})$        | False positive (FP)       | True positive (TP)    |\n+--------------------------------------------------------------------+---------------------------+-----------------------+\n\n<br>\n\n. . .\n\n**Specificity**: Proportion of actual negatives that were correctly classified as negative\n\n-   **P(classified not high risk \\| not high risk) = 1 âˆ’ False positive rate**\n\n## Practice\n\n::: center\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction    0    1\n         0 3553  635\n         1    2    0\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: question\nCalculate the\n\n-   False negative rate\n-   False positive rate\n-   Sensitivity\n-   Specificity\n:::\n\n## Using metrics to select model and threshold {.midi}\n\n+----------------------------------+---------------------------------------------------------------------+\n| Metric                           | Guidance for use                                                    |\n+==================================+=====================================================================+\n| Accuracy                         | For balanced data, use only in combination with other metrics.      |\n|                                  |                                                                     |\n|                                  | Avoid using for imbalanced data.                                    |\n+----------------------------------+---------------------------------------------------------------------+\n| Sensitivity (true positive rate) | Use when false negatives are more \"expensive\" than false positives. |\n+----------------------------------+---------------------------------------------------------------------+\n| False positive rate              | Use when false positives are more \"expensive\" than false negatives. |\n+----------------------------------+---------------------------------------------------------------------+\n| Precision = $\\frac{TP}{TP + FP}$ | Use when it's important for positive predictions to be accurate.    |\n+----------------------------------+---------------------------------------------------------------------+\n\n::: small\nThis table is a modification of work created and shared by Google in the [Google Machine Learning Crash Course](#0).\n:::\n\n## Choosing a classification threshold\n\n::: question\nA doctor plans to use your model to determine which patients are high risk for heart disease. The doctor will recommend a treatment plan for high risk patients.\n\n-   Would you want sensitivity to be high or low? What about specificity?\n\n-   What are the trade-offs associated with each decision?\n:::\n\n## ROC curve {.midi}\n\nSo far the model assessment has depended on the model and selected threshold. The **receiver operating characteristic (ROC) curve** allows us to assess the model performance across a range of thresholds.\n\n. . .\n\n::::: columns\n::: {.column width=\"65%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-logistic-prediction_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"35%\"}\n-   x-axis: 1 - Specificity (False positive rate)\n\n-   y-axis: Sensitivity (True positive rate)\n\nWhich corner of the plot indicates the best model performance?\n:::\n:::::\n\n## ROC curve\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-logistic-prediction_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## ROC curve in R {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# calculate sensitivity and specificity at each threshold\nroc_curve_data <- heart_disease_aug |>\n  roc_curve(high_risk, pred_prob, \n            event_level = \"second\") \n\n# plot roc curve\nautoplot(roc_curve_data)\n```\n:::\n\n\n\n\n## ROC curve in R\n\n::::: columns\n::: {.column width=\"40%\"}\n**Sample from underlying data**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1     0.0545       0.103       0.980\n 2     0.0660       0.181       0.959\n 3     0.0832       0.305       0.909\n 4     0.136        0.583       0.715\n 5     0.193        0.754       0.501\n 6     0.221        0.805       0.411\n 7     0.221        0.805       0.411\n 8     0.262        0.881       0.287\n 9     0.270        0.901       0.254\n10     0.279        0.915       0.227\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-logistic-prediction_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n:::\n:::::\n\n## Area under the curve\n\nThe **area under the curve (AUC)** can be used to assess how well the logistic model fits the data\n\n-   AUC=0.5: model is a very bad fit (no better than a coin flip)\n\n-   AUC close to 1: model is a good fit\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nheart_disease_aug |>\n  roc_auc(high_risk, pred_prob,\n    event_level = \"second\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.695\n```\n\n\n:::\n:::\n\n\n\n\n## Recap\n\n-   Calculated predicted probabilities from the logistic regression model\n\n-   Used predicted probabilities to classify observations\n\n-   Made decisions and assessed model performance using\n\n    -   Confusion matrix\n    -   ROC curve\n\n## Further reading\n\n[Classification](https://developers.google.com/machine-learning/crash-course/classification) module in Google Machine Learning Crash Course\n",
    "supporting": [
      "20-logistic-prediction_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}