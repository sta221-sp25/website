{
  "hash": "e0cd7b304eeca1144c8534681e50f0c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum likelihood estimation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-03-18\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[üîó STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 16-mle-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n## Announcements\n\n-   HW 03 due March 20 at 11:59pm\n\n-   Project exploratory data analysis due March 20 at 11:59pm\n\n    -   Next milestone: Project presentations in lab March 28\n\n-   Statistics experience due April 22\n\n## Topics\n\n-   Likelihood\n\n-   Maximum likelihood estimation (MLE)\n\n-   MLE for linear regression\n\n## Motivation {.midi}\n\n-   We've discussed how to find the estimators of $\\boldsymbol{\\beta}$ and $\\sigma^2_{\\epsilon}$ for the model\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{10mm} \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2_\\epsilon\\mathbf{I})\n$$using least-squares estimation\n\n-   Today we will introduce another way to find these estimators - **maximum likelihood estimation.**\n\n-   We will see the least-squares estimator is equal to the maximum likelihood estimator when certain assumptions hold\n\n# Maximum likelihood estimation\n\n## Example: Basketball shots \n\nSuppose the a basketball player shoots the ball, such that the probability of making the basket (successfully making the shot) is $p$\n\n. . .\n\n::: incremental\n-   What is the probability distribution for this random phenomenon?\n\n-   Suppose the probability is $p = 0.5$? What is the probability the player makes a single basket, given this value of $p$?\n\n-   Suppose the probability is $p = 0.8$? What is the probability the player makes a single basket, given this value of $p$?\n:::\n\n## Shooting the ball three times\n\nSuppose the player shoots the ball three times. They are all independent and the player has the same probability $p$ of making each basket.\n\nLet $B$ represent a made basket, and $M$ represent a missed basket. The player shoots the ball three times with the outcome $BBM$.\n\n. . .\n\n::: incremental\n-   Suppose the probability is $p = 0.5$? What is the probability of observing the data $BBM$, given this value of $p$?\n\n-   Suppose the probability is $p = 0.3$? What is the probability of observing the data $BBM$, given this value of $p$ ?\n:::\n\n## Shooting the ball three times\n\nSuppose the player shoots the ball three times. They are all independent and the player has the same probability \\$p\\$ of making each basket.\n\nThe player shoots the ball three times with the outcome $BBM$.\n\n. . .\n\n::: incremental\n-   **New question:** What parameter value of $p$ do you think maximizes the probability of observing this data?\n\n-   We will use a **likelihood** function to answer this question.\n:::\n\n## Likelihood {.midi}\n\n::: incremental\n-   A **likelihood function** is a measure of how likely we are to observe our data under each possible value of the parameter(s)\n\n-   Note that this is **not** the same as the probability function.\n\n-   **Probability function**: Fixed parameter value(s) + input possible outcomes\n\n    -   *Given* $p=0.8$ , *what is the probability of observing* $BBM$ *in three basketball shots?*\n\n-   **Likelihood function**: Fixed data + input possible parameter values\n\n    -   *Given we've observed* $BBM$, *what is the most plausible value of* $p$?\n:::\n\n## Likelihood: Three basketball shots\n\nThe likelihood function for the probability of a basket $p$ given we observed $BBM$ when shooting the ball three independent times $$\nL(p|BBM) = p \\times p \\times (1 - p)\n$$\n\n<br>\n\n. . .\n\nThus, if the likelihood for $p = 0.8$ is\n\n$$\nL(p = 0.8|BBM) = 0.8 \\times 0.8 \\times (1 - 0.8) = 0.128\n$$\n\n## Likelihood: Three basketball shots\n\n<br>\n\n::: question\n-   What is the general formula for the likelihood function for $p$ given the observed data $BBM$?\n\n-   How does assuming independence simply things?\n\n-   How does having identically distributed data simplify things?\n:::\n\n## Likelihood: Three basketball shots {.midi}\n\nThe likelihood function for $p$ given the data $BBM$ is\n\n$$\nL(p|BBM) = p \\times p \\times (1 - p) = p^2 \\times (1 - p)\n$$\n\n. . .\n\n::: incremental\n-   We want of the value of $p$ that maximizes this likelihood function, i.e., the value of $p$ that is most likely given the observed data.\n\n-   The process of finding this value is **maximum likelihood estimation**.\n\n-   There are three primary ways to find the maximum likelihood estimator\n\n    -   Approximate using a graph\n\n    -   Using calculus\n\n    -   Numerical approximation\n:::\n\n## Finding the MLE using graphs\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](16-mle_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n\n::: question\nWhat do you think is the approximate value of the MLE of $p$ given the data?\n:::\n\n## Finding the MLE using calculus\n\n-   Find the MLE using the first derivative of the likelihood function.\n\n<!-- -->\n\n-   This can be tricky because of the product rule, so we can maximize the **log(Likelihood)** instead. The same value maximizes the likelihood and log(Likelihood).\n\n::: question\nUse calculus to find the MLE of $p$ given the data $BBM$.\n:::\n\n## Shooting the ball $n$ times\n\nSuppose the player shoots the ball $n$ times. They are all independent and the player has the same probability $p$ of making each one.\n\nSuppose the player makes $k$ baskets out of the $n$ shots. This is the observed data.\n\n. . .\n\n::: question\n-   What is the formula for the probability distribution to describe this random phenomenon?\n-   What is the formula for the likelihood function for $p$ given the observed data?\n-   For what value of $p$ do we maximize the likelihood given the observed data? Use calculus to find the response.\n:::\n\n# MLE in linear regression\n\n## Why maximum likelihood estimation? {.midi}\n\n-   *\"Maximum likelihood estimation is, by far, the most popular technique for deriving estimators.\"* [@casella2024statistical, pp. 315]\n\n-   MLEs have nice statistical properties (more on this next class)\n\n    -   Consistent\n\n    -   Efficient\n\n    -   Asymptotically normal\n\n. . .\n\n::: callout-note\nIf the normality assumption holds, the least squares estimator is the maximum likelihood estimator for $\\beta$. Therefore, it has all the properties of the MLE.\n:::\n\n## Linear regression\n\nRecall the linear model\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{10mm} \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I}) \n$$\n\n. . .\n\n::: incremental\n-   We have discussed least-squares estimation to find $\\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\sigma}_\\epsilon^2$\n-   We have used the fact that $\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1})$ when doing hypothesis testing and confidence intervals.\n-   Now we will discuss how we know $\\hat{\\boldsymbol{\\beta}}$ is normally distributed, as we introduce MLE for linear regression\n:::\n\n## Simple linear regression model\n\nSuppose we have the simple linear regression (SLR) model\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\hspace{10mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n$$\n\nsuch that $\\epsilon_i$ are independently and identically distributed.\n\n<br>\n\n. . .\n\nWe can write this model in the form below and use this to find the MLE\n\n$$\ny_i | x_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2_{\\epsilon})\n$$\n\n## Side note: Normal distribution {background-color=\"#ccddeb\"}\n\nLet $Z$ be a random variable, such that $Z \\sim N(\\mu, \\sigma^2)$. Then the probability function is\n\n$$\nP(Z = z | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big\\{-{\\frac{1}{2\\sigma^2}(z - \\mu)^2}\\Big\\}\n$$\n\n## SLR: Likelihood for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ {.midi}\n\nThe likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\nL(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} &| x_1, \\ldots, x_n, y_1, \\ldots, y_n) \\\\ &= p(y_1|x_1, \\beta_0, \\beta_1, \\sigma^2_{\\epsilon}) \\dots  p(y_n|x_n, \\beta_0, \\beta_1, \\sigma^2_{\\epsilon}) \\\\[5pt]\n& = \\class{fragment}{\\prod_{i=1}^n p(y_i | x_i, \\beta_0, \\beta_1, \\sigma^2_{\\epsilon})} \\\\[5pt]\n &= \\class{fragment}{\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_\n\\epsilon^2}}\\exp\\Big\\{{-\\frac{1}{2\\sigma_\\epsilon^2}(y_i - [\\beta_0 + \\beta_1x_i])^2}\\Big\\}} \\\\[10pt]\n& = \\class{fragment}{(2\\pi\\sigma^2_{\\epsilon})^{-\\frac{n}{2}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\Big\\}}\n\\end{aligned}\n$$\n\n## Log-Likelihood for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$\n\nThe log-likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\n\\log &L(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} | x_1, \\ldots, x_n, y_1, \\ldots, y_n) \\\\[8pt]\n& = \\class{fragment}{\\log\\Big((2\\pi\\sigma^2_{\\epsilon})^{-\\frac{n}{2}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\Big\\}\\Big)} \\\\[8pt]\n& = \\class{fragment}{-\\frac{n}{2}\\log(2\\pi\\sigma^2_{\\epsilon}) -\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2}\n\\end{aligned}\n$$\n\n## MLE for $\\beta_0$\n\n1Ô∏è‚É£ Take derivative of $\\log L$ with respect to $\\beta_0$ and set it equal to 0\n\n$$\n\\frac{\\partial \\log L}{\\partial \\beta_0} = -\\frac{2}{2\\sigma^2_\\epsilon}\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)(-1) = 0\n$$\n\n## MLE for $\\beta_0$ {.midi}\n\n2Ô∏è‚É£ Find the $\\tilde{\\beta}_0$ that satisfies the equality on the previous slide\n\n. . .\n\nAfter a few steps...\n\n$$\n\\begin{aligned}\n&\\Rightarrow \\class{fragment}{\\sum_{i=1}^ny_i - n\\tilde{\\beta}_0 - \\tilde{\\beta}_1\\sum_{i=1}^n x_i = 0} \\\\[8pt]\n&\\Rightarrow \\class{fragment}{\\sum_{i=1}^ny_i  - \\tilde{\\beta}_1\\sum_{i=1}^n x_i = n\\tilde{\\beta}_0} \\\\[8pt]\n&\\Rightarrow \\class{fragment}{ \\frac{1}{n}\\sum_{i=1}^ny_i  - \\frac{1}{n}\\tilde{\\beta}_1\\sum_{i=1}^n x_i = \\tilde{\\beta}_0}\n\\end{aligned}\n$$\n\n## MLE for $\\beta_0$ {.midi}\n\n3Ô∏è‚É£ We can use the second derivative to show we've found the maximum\n\n$$\n\\frac{\\partial^2 \\log L}{\\partial \\beta_0^2} = -\\frac{n}{2\\tilde{\\sigma}^2_\\epsilon}  < 0\n$$\n\n<br>\n\n. . .\n\nTherefore, we have found the maximum. Thus, MLE for $\\beta_0$ is\n\n$$\n\\tilde{\\beta}_0 = \\bar{y} - \\tilde{\\beta}_1\\bar{x}\n$$\n\n. . .\n\nNote that $\\tilde{\\beta}_0$ is equal to $\\hat{\\beta}_0$, the least-squares estimate\n\n## MLE for $\\beta_1$ and $\\sigma^2_{\\epsilon}$\n\nWe can use a similar process to find the MLEs for $\\beta_1$ and $\\sigma^2_{\\epsilon}$\n\n$$\n\\tilde{\\beta}_1 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\n$$ <br>\n\n. . .\n\n$$\n\\tilde{\\sigma}^2_{\\epsilon} = \\frac{\\sum_{i=1}^n(y_i - \\tilde{\\beta}_0 - \\tilde{\\beta}_1x_i)^2}{n} = \\frac{\\sum_{i=1}^ne_i^2}{n}\n$$\n\n. . .\n\nNote: $\\tilde{\\beta}_1 = \\hat{\\beta}_1$ and $\\tilde{\\sigma}^2_{\\epsilon} \\approx \\hat{\\sigma}^2_{\\epsilon}$\n\n# MLE in matrix form\n\n## MLE for linear regression in matrix form {.midi}\n\n$$\nL(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{(2\\pi)^{n/2}\\sigma^n_{\\epsilon}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\Big\\}\n$$ <br>\n\n. . .\n\n$$\n\\begin{aligned}\n\\log L(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon &| \\mathbf{X}, \\mathbf{y}) \\\\ \n& = -\\frac{n}{2}\\log(2\\pi) - n \\log(\\sigma_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\n\\end{aligned}\n$$ \n\n. . .\n\n::: question\n1.  For a fixed value of $\\sigma_\\epsilon$ , we know that $\\log L$ is maximized when what is true about $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ ?\n2.  What does this tell us about the relationship between the MLE and least-squares estimator for $\\boldsymbol{\\beta}$?\n:::\n\n## Putting it all together\n\n::: incremental\n-   The MLE $\\tilde{\\boldsymbol{\\beta}}$ is equivalent to the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ , when the errors follow independent and identical normal distributions\n\n-   MLEs have nice properties, so this means the least-squares estimator $\\hat{\\mathbf{\\boldsymbol{\\beta}}}$ inherits all the nice properties of MLEs\n\n-   The MLE $\\tilde{\\sigma}^2_{\\epsilon}$ is approximately equal to the least-squares estimator $\\hat{\\sigma}_\\epsilon$. When $n >> p$, the difference is trivial\n:::\n\n## References\n",
    "supporting": [
      "16-mle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}