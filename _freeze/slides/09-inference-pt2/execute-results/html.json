{
  "hash": "94d8d0b2411518a6c8078d038a7f5e4d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Inference for regression\"\nsubtitle: \"Cont'd\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-02-06\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 09-inference-pt2-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   HW 02 due Tuesday, February 11 at 11:59pm\n\n    -   Released after class\n\n<!-- -->\n\n-   [Click here](https://prodduke.sharepoint.com/:p:/s/ARCStaff839/EZ4PKTRTlCVMpFZiR6XoRycB4UUlRMuI2_Rda9hKxNZtsA) to learn more about the Academic Resource Center\n\n-   [Statistics experience](../hw/stats-experience.html) due **Tuesday, April 22**\n\n## Topics\n\n-   Understand statistical inference in the context of regression\n\n-   Describe the assumptions for regression\n\n-   Understand connection between distribution of residuals and inferential procedures\n\n-   Conduct inference on a single coefficient\n\n## Computing setup\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n\n## Data: NCAA Football expenditures {.midi}\n\nToday's data come from [Equity in Athletics Data Analysis](https://ope.ed.gov/athletics/#/datafile/list) and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a [March 2022 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md).\n\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\n-   `total_exp_m`: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\n\n-   `enrollment_th`: Total student enrollment in the 2019 - 2020 academic year (in thousands)\n\n-   `type`: institution type (Public or Private)\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfootball <- read_csv(\"data/ncaa-football-exp.csv\")\n```\n:::\n\n\n\n\n## Univariate EDA\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-inference-pt2_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Bivariate EDA\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-inference-pt2_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Regression model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp_fit <- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|\n|enrollment_th |    0.780|     0.110|     7.074|       0|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|\n\n\n:::\n:::\n\n\n\n\n<br>\n\nFor every additional 1,000 students, we expect an institution's total expenditures on football to increase by \\$780,000, on average, holding institution type constant.\n\n## From sample to population {.midi}\n\n> For every additional 1,000 students, we expect an institution's total expenditures on football to increase by \\$780,000, on average, holding institution type constant.\n\n. . .\n\n::: incremental\n-   This estimate is valid for the single sample of 127 higher education institutions in the 2019 - 2020 academic year.\n-   But what if we're not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?\n-   What if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?\n:::\n\n# Inference for regression\n\n## Statistical inference\n\n:::::: columns\n:::: {.column width=\"40%\"}\n::: midi\n-   **Statistical inference** provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\n\n-   For our inferences to be valid, the sample should be representative (ideally random) of the population we're interested in\n:::\n::::\n\n::: {.column width=\"60%\"}\n![Image source: Eugene Morgan Â© Penn State](images/08/inference.png){fig-align=\"center\"}\n:::\n::::::\n\n## Linear regression model\n\n$$\\begin{aligned}\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{8mm} \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I})\n\\end{aligned}\n$$\n\nsuch that the errors are independent and normally distributed.\n\n. . .\n\n-   **Independent**: Knowing the error term for one observation doesn't tell us about the error term for another observation\n-   **Normally distributed**: The distribution follows a particular mathematical model that is unimodal and symmetric\n\n## Visualizing distribution of $\\mathbf{y}|\\mathbf{X}$ {.midi}\n\n$$\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n$$\n\n![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align=\"center\"}\n\n$Var(\\mathbf{z}) = \\begin{bmatrix}Var(z_1) & Cov(z_1, z_2) & \\dots & Cov(z_1, z_p)\\\\ Cov(z_2, z_1) & Var(z_2) & \\dots & Cov(z_2, z_p) \\\\ \\vdots & \\vdots & \\dots & \\cdot \\\\ Cov(z_p, z_1) & Cov(z_p, z_2) & \\dots & Var(z_p)\\end{bmatrix}$\n\n## Linear transformation of normal random variable {background-color=\"#ccddeb\"}\n\nSuppose $\\mathbf{z}$ is a (multivariate) normal random variable such that $\\mathbf{z} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$\n\n<br>\n\nA linear transformation of $\\mathbf{z}$ is also multivariate normal, such that\n\n$$\n\\mathbf{A}\\mathbf{z} + \\mathbf{B} \\sim N(\\mathbf{A}\\boldsymbol{\\mu} + \\mathbf{B}, \\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^\\mathsf{T})\n$$\n\n::: question\nExplain why $\\mathbf{y}|\\mathbf{X}$ is normally distributed.\n:::\n\n## Assumptions for regression\n\n::::: columns\n::: {.column width=\"50%\"}\n$$\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n$$\n\n![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n1.  **Linearity:** There is a linear relationship between the response and predictor variables.\n2.  **Constant Variance:** The variability about the least squares line is generally constant.\n3.  **Normality:** The distribution of the residuals is approximately normal.\n4.  **Independence:** The residuals are independent from one another.\n:::\n:::::\n\n## Estimating $\\sigma^2_{\\epsilon}$ {.midi}\n\n-   Once we fit the model, we can use the residuals to estimate $\\sigma_{\\epsilon}^2$\n\n-   The estimated value $\\hat{\\sigma}^2_{\\epsilon}$ is needed for hypothesis testing and constructing confidence intervals for regression\n\n$$\n\\hat{\\sigma}^2_\\epsilon = \\frac{SSR}{n - p - 1} = \\frac{\\mathbf{e}^\\mathsf{T}\\mathbf{e}}{n-p-1} \n$$\n\n. . .\n\n-   The **regression standard error** $\\hat{\\sigma}_{\\epsilon}$ is a measure of the average distance between the observations and regression line\n\n$$\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{SSR}{n - p - 1}} = \\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\mathbf{e}^\\mathsf{T}\\mathbf{e}}{n - p - 1}}\n$$\n\n# Inference for a single coefficient\n\n## Inference for $\\beta_j$\n\nWe often want to conduct inference on individual model coefficients\n\n-   **Hypothesis test:** Is there a linear relationship between the response and $x_j$?\n\n-   **Confidence interval**: What is a plausible range of values $\\beta_j$ can take?\n\n. . .\n\nBut first we need to understand the distribution of $\\hat{\\beta}_j$\n\n## Sampling distribution of $\\hat{\\beta}$ {.midi}\n\n-   A **sampling distribution** is the probability distribution of a statistic for a large number of random samples of size $n$ from a population\n\n-   The sampling distribution of $\\hat{\\boldsymbol{\\beta}}$ is the probability distribution of the estimated coefficients if we repeatedly took samples of size $n$ and fit the regression model\n\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1})\n$$\n\n. . .\n\nThe estimated coefficients $\\hat{\\boldsymbol{\\beta}}$ are **normally distributed** with\n\n$$\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta} \\hspace{10mm} Var(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2_{\\epsilon}(\\boldsymbol{X}^\\mathsf{T}\\boldsymbol{X})^{-1}\n$$\n\n## Expected value of $\\boldsymbol{\\hat{\\beta}}$\n\n::: question\nShow\n\n$$E(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}$$\n:::\n\n<br>\n\nWill show $Var(\\hat{\\boldsymbol{\\beta}})$ in homework\n\n## Sampling distribution of $\\hat{\\beta}_j$\n\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1})\n$$\n\nLet $\\mathbf{C} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}$. Then, for each coefficient $\\hat{\\beta}_j$,\n\n::: incremental\n-   $E(\\hat{\\beta}_j) = \\boldsymbol{\\beta}_j$, the $j^{th}$ element of $\\boldsymbol{\\beta}$\n\n-   $Var(\\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{jj}$\n\n-   $Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = \\sigma^2_{\\epsilon}C_{ij}$\n:::\n\n## $Var(\\hat{\\boldsymbol{\\beta}})$ for NCAA data\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- model.matrix(total_exp_m ~ enrollment_th + type, \n                  data = football)\nsigma_sq <- glance(exp_fit)$sigma^2\n\nvar_beta <- sigma_sq * solve(t(X) %*% X)\nvar_beta\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              (Intercept) enrollment_th typePublic\n(Intercept)     8.9054556   -0.13323338 -6.0899556\nenrollment_th  -0.1332334    0.01216984 -0.1239408\ntypePublic     -6.0899556   -0.12394079  9.9388370\n```\n\n\n:::\n:::\n\n\n\n\n## $SE(\\hat{\\boldsymbol{\\beta}})$ for NCAA data\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|\n|enrollment_th |    0.780|     0.110|     7.074|       0|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|\n\n\n:::\n:::\n\n\n\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsqrt(diag(var_beta))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) enrollment_th    typePublic \n     2.984201      0.110317      3.152592 \n```\n\n\n:::\n:::\n\n\n\n\n# Hypothesis test for $\\beta_j$\n\n## Steps for a hypothesis test\n\n1.  State the null and alternative hypotheses.\n2.  Calculate a test statistic.\n3.  Calculate the p-value.\n4.  State the conclusion.\n\n## Hypothesis test for $\\beta_j$: Hypotheses\n\nWe will generally test the hypotheses:\n\n$$\n\\begin{aligned}\n&H_0: \\beta_j = 0 \\\\\n&H_a: \\beta_j \\neq 0\n\\end{aligned}\n$$\n\n::: question\nState these hypotheses in words.\n:::\n\n## Hypothesis test for $\\beta_j$: Test statistic {.midi}\n\n**Test statistic:** Number of standard errors the estimate is away from the null\n\n$$\n\\text{Test Statistic} = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n$$\n\n. . .\n\nIf $\\sigma^2_{\\epsilon}$ was known, the test statistic would be\n\n$$Z = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\sigma^2_\\epsilon C_{jj}}} ~\\sim ~ N(0, 1)\n$$\n\n. . .\n\nIn general, $\\sigma^2_{\\epsilon}$ is [**not**]{.underline} known, so we use $\\hat{\\sigma}_{\\epsilon}^2$ to calculate $SE(\\hat{\\beta}_j)$\n\n$$T = \\frac{\\hat{\\beta}_j - 0}{SE(\\hat{\\beta}_j)} ~ = ~\\frac{\\hat{\\beta}_j - 0}{\\sqrt{\\hat{\\sigma}^2_\\epsilon C_{jj}}} ~\\sim ~ t_{n-p-1}\n$$\n\n## Hypothesis test for $\\beta_j$: Test statistic\n\n-   The test statistic $T$ follows a $t$ distribution with $n - p -1$ degrees of freedom.\n\n-   We need to account for the additional variability introduced by calculating $SE(\\hat{\\beta}_j)$ using an estimated value instead of a constant\n\n## *t* vs. N(0,1)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Standard normal vs. t distributions](09-inference-pt2_files/figure-revealjs/fig-normal-t-curves-1.png){#fig-normal-t-curves fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Hypothesis test for $\\beta_j$: P-value\n\nThe **p-value** is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\n$$\np-value = P(|t| > |\\text{test statistic}|),\n$$\n\ncalculated from a $t$ distribution with $n- p - 1$ degrees of freedom\n\n. . .\n\n::: question\nWhy do we take into account \"extreme\" on both the high and low ends?\n:::\n\n## Understanding the p-value\n\n| Magnitude of p-value    | Interpretation                        |\n|:------------------------|:--------------------------------------|\n| p-value \\< 0.01         | strong evidence against $H_0$         |\n| 0.01 \\< p-value \\< 0.05 | moderate evidence against $H_0$       |\n| 0.05 \\< p-value \\< 0.1  | weak evidence against $H_0$           |\n| p-value \\> 0.1          | effectively no evidence against $H_0$ |\n\n<br>\n\n**These are general guidelines. The strength of evidence depends on the context of the problem.**\n\n## Hypothesis test for $\\beta_j$: Conclusion\n\n**There are two parts to the conclusion**\n\n-   Make a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( $\\alpha$ level)\n\n    -   If $\\text{P-value} < \\alpha$: Reject $H_0$\n\n    -   If $\\text{P-value} \\geq \\alpha$: Fail to reject $H_0$\n\n-   State the conclusion in the context of the data\n\n# Application exercise\n\n::: appex\nðŸ“‹ [sta221-sp25.netlify.app/ae/ae-03-inference](../ae/ae-03-inference.html)\n:::\n\n# Confidence interval for $\\beta_j$\n\n## Confidence interval for $\\beta_j$ {.midi}\n\n::: incremental\n-   A plausible range of values for a population parameter is called a **confidence interval**\n\n-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\n    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\n\n    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter\n:::\n\n## What \"confidence\" means {.midi}\n\n::: incremental\n-   We will construct $C\\%$ confidence intervals.\n\n    -   The confidence level impacts the width of the interval\n\n<br>\n\n-   \"Confident\" means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate $C\\%$ CIs for the coefficient of $x_j$, then $C\\%$ of those intervals will contain the true value of the coefficient $\\beta_j$\n\n<br>\n\n-   Balance precision and accuracy when selecting a confidence level\n:::\n\n## Confidence interval for $\\beta_j$\n\n$$\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n$$\n\n<br>\n\n. . .\n\n$$\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n$$\n\nwhere $t^*$ is calculated from a $t$ distribution with $n-p-1$ degrees of freedom\n\n## Confidence interval: Critical value\n\n::: {.fragment fragment-index=\"1\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.97928\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<br>\n\n::: {.fragment fragment-index=\"2\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.657235\n```\n\n\n:::\n:::\n\n\n\n:::\n\n<br>\n\n::: {.fragment fragment-index=\"3\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.61606\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## 95% CI for $\\beta_j$: Calculation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|\n|enrollment_th |    0.780|     0.110|     7.074|       0|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|\n\n\n:::\n:::\n\n\n\n\n## 95% CI for $\\beta_j$ in R\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:-------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|   13.426|    25.239|\n|enrollment_th |    0.780|     0.110|     7.074|       0|    0.562|     0.999|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|  -19.466|    -6.986|\n\n\n:::\n:::\n\n\n\n\n<br>\n\n**Interpretation**: We are 95% confident that for each additional 1,000 students enrolled, the institution's expenditures on football will be greater by \\$562,000 to \\$999,000, on average, holding institution type constant.\n\n## Recap\n\n-   Introduced statistical inference in the context of regression\n\n-   Described the assumptions for regression\n\n-   Connected the distribution of residuals and inferential procedures\n\n-   Conducted inference on a single coefficient\n\n## Next class\n\n-   Hypothesis testing based on ANOVA\n",
    "supporting": [
      "09-inference-pt2_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}