{
  "hash": "e1099c36ba59c90f019b367fb2692661",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression: Model comparison\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-04-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 21-logistic-comparison-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements {.midi}\n\n-   HW 04 due April 10 - released later today\n\n-   Team Feedback (email from TEAMMATES) due Tuesday, April 8 at 11:59pm (check email)\n\n-   Next project milestone: Analysis and draft in April 11 lab\n\n-   Statistics experience due April 22\n\n## Questions from this week's content?\n\n<center>\n\n\n\n\n```{=html}\n<iframe width=\"640px\" height=\"480px\" src=\"https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTBw0r2_bKCVMs9lST1_-2sxUQ1JTSFBZNlFMWUJZSDcwTUdaVzgwWUhBMC4u&embed=true\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" style=\"border: none; max-width:100%; max-height:100vh\" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>\n```\n\n\n\n\n</center>\n\n## Topics\n\n-   Comparing models using AIC and BIC\n\n-   Test of significance for a subset of predictors\n\n## Computational setup\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(pROC)      \nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n\n## Risk of coronary heart disease {.midi}\n\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\n-   `high_risk`:\n\n    -   1: High risk of having heart disease in next 10 years\n    -   0: Not high risk of having heart disease in next 10 years\n\n-   `age`: Age at exam time (in years)\n\n-   `totChol`: Total cholesterol (in mg/dL)\n\n-   `currentSmoker`: 0 = nonsmoker, 1 = smoker\n\n-   `education`: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Modeling risk of coronary heart disease\n\nUsing `age`, `totChol`, and `currentSmoker`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term           | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:--------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)    |   -6.673|     0.378|   -17.647|   0.000|   -7.423|    -5.940|\n|age            |    0.082|     0.006|    14.344|   0.000|    0.071|     0.094|\n|totChol        |    0.002|     0.001|     1.940|   0.052|    0.000|     0.004|\n|currentSmoker1 |    0.443|     0.094|     4.733|   0.000|    0.260|     0.627|\n\n\n:::\n:::\n\n\n\n\n## Review: ROC Curve + Model fit\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-logistic-comparison_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.697\n```\n\n\n:::\n:::\n\n\n\n\n## Review: Classification\n\nWe will use a threshold of 0.2 to classify observations\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-logistic-comparison_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n## Review: Classification {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-logistic-comparison_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n::: question\n1.  Compute the misclassification rate.\n\n2.  Compute sensitivity and explain what it means in the context of the data.\n\n3.  Compute specificity and explain what it means in the context of the data.\n:::\n\n# Model comparison\n\n## Which model do we choose?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n::::: columns\n::: {.column width=\"50%\"}\n<center>**Model 1**</center>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term           | estimate|\n|:--------------|--------:|\n|(Intercept)    |   -6.673|\n|age            |    0.082|\n|totChol        |    0.002|\n|currentSmoker1 |    0.443|\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n<center>**Model 2**</center>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term           | estimate|\n|:--------------|--------:|\n|(Intercept)    |   -6.456|\n|age            |    0.080|\n|totChol        |    0.002|\n|currentSmoker1 |    0.445|\n|education2     |   -0.270|\n|education3     |   -0.232|\n|education4     |   -0.035|\n\n\n:::\n:::\n\n\n\n:::\n:::::\n\n## AIC & BIC\n\nEstimators of prediction error and *relative* quality of models:\n\n. . .\n\n**Akaike's Information Criterion (AIC)**[^1]: $$AIC = -2\\log L + 2 (p+1)$$\n\n[^1]: Akaike, Hirotugu. [\"A new look at the statistical model identification.\"](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1100705) *IEEE transactions on automatic control* 19.6 (1974): 716-723.\n\n. . .\n\n**Schwarz's Bayesian Information Criterion (BIC)**[^2]: $$ BIC = -2\\log L + \\log(n)\\times(p+1)$$\n\n[^2]: Schwarz, Gideon. [\"Estimating the dimension of a model.\"](https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Faos%2F1176344136) *The annals of statistics* (1978): 461-464.\n\n## AIC & BIC\n\n$$\n\\begin{aligned} \n& AIC = \\color{blue}{-2\\log L}  \\color{black}{+ 2(p+1)} \\\\\n& BIC = \\color{blue}{-2\\log L}  + \\color{black}{\\log(n)\\times(p+1)}\n\\end{aligned}\n$$\n\n. . .\n\n<br>\n\nFirst Term: Decreases as *p* increases\n\n## AIC & BIC\n\n$$\n\\begin{aligned} & AIC = -2\\log L  + \\color{blue}{2(p+1)} \\\\\n& BIC = -2\\log L + \\color{blue}{\\log(n)\\times (p+1)} \n\\end{aligned}\n$$\n\n<br>\n\nSecond term: Increases as *p* increases\n\n## Using AIC & BIC\n\n$$\n\\begin{aligned} & AIC = -2\\log L  + \\color{red}{2(p+1)} \\\\\n& BIC = -2 \\log L  + \\color{red}{\\log(n)\\times(p+1)} \n\\end{aligned}\n$$\n\n-   Choose model with the smaller value of AIC or BIC\n\n-   If $n \\geq 8$, the **penalty** for BIC is larger than that of AIC, so BIC tends to favor *more parsimonious* models (i.e. models with fewer terms)\n\n## AIC from the `glance()` function\n\nLet's look at the AIC for the model that includes `age`, `totChol`, and `currentSmoker`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(high_risk_fit)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3232.812\n```\n\n\n:::\n:::\n\n\n\n\n<br>\n\n. . .\n\n**Calculating AIC**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n- 2 * glance(high_risk_fit)$logLik + 2 * (3 + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3232.812\n```\n\n\n:::\n:::\n\n\n\n\n## Comparing the models using AIC\n\nLet's compare the full and reduced models using AIC.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(high_risk_fit_reduced)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3232.812\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(high_risk_fit_full)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3231.6\n```\n\n\n:::\n:::\n\n\n\n\n<br>\n\n::: question\nBased on AIC, which model would you choose?\n:::\n\n## Comparing the models using BIC\n\nLet's compare the full and reduced models using BIC\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(high_risk_fit_reduced)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3258.074\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(high_risk_fit_full)$BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3275.807\n```\n\n\n:::\n:::\n\n\n\n\n<br>\n\n::: question\nBased on BIC, which model would you choose?\n:::\n\n# Drop-in-deviance test\n\n## Drop-in-deviance test\n\nWe will use a **drop-in-deviance test** (aka Likelihood Ratio Test) to test\n\n-   the overall statistical significance of a logistic regression model\n\n-   the statistical significance of a subset of coefficients in the model\n\n## Log-Likelihood \n\nRecall the log-likelihood function\n\n$$\n\\begin{aligned}\n\\log L&(\\boldsymbol{\\beta}|x_1, \\ldots, x_n, y_1, \\dots, y_n) \\\\\n&= \\sum\\limits_{i=1}^n[y_i \\log(\\pi_i) + (1 - y_i)\\log(1 - \\pi_i)]\n\\end{aligned}\n$$\n\nwhere $\\pi_i = \\frac{\\exp\\{x_i^\\mathsf{T}\\boldsymbol{\\beta}\\}}{1 + \\exp\\{x_i^\\mathsf{T}\\boldsymbol{\\beta}\\}}$\n\n## Deviance {.midi}\n\nThe **deviance** is a measure of the degree to which the predicted values are different from the observed values (compares the current model to a \"saturated\" model)\n\nIn logistic regression,\n\n$$\nD = -2 \\log L \n$$\n\n<br>\n\n$D \\sim \\chi^2_{n - p - 1}$ ( $D$ follows a Chi-square distribution with $n - p - 1$ degrees of freedom)\n\n<br>\n\nNote: $n - p - 1$ a the degrees of freedom associated with the error in the model (like residuals)\n\n## $\\chi^2$ distribution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-logistic-comparison_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n## Test for overall significance\n\nWe can test the overall significance for a logistic regression model, i.e., whether there is at least one predictor with a non-zero coefficient\n\n$$\n\\begin{aligned}\n&H_0: \\beta_1 = \\dots = \\beta_p = 0 \\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one } j\n\\end{aligned}\n$$\n\n. . .\n\nThe **drop-in-deviance test for overall significance** compares the fit of a model with no predictors to the current model.\n\n## Drop-in-deviance test statistic\n\nLet $L_0$ and $L_a$ be the likelihood functions of the model under $H_0$ and $H_a$, respectively. The **test statistic** is\n\n$$\n\\begin{aligned}\nG = D_0 - D_a &= (-2\\log L_0) - (-2\\log L_a)\\\\[5pt] \n& = -2(\\log L_0 - \\log L_a) \\\\[5pt]\n&= -2\\sum_{i=1}^n \\Big[ y_i \\log \\Big(\\frac{\\hat{\\pi}^0}{\\hat{\\pi}^a_i}\\Big) + (1 - y_i)\\log \\Big(\\frac{1-\\hat{\\pi}^0}{1-\\hat{\\pi}^a_i}\\Big)\\Big]\n\\end{aligned}\n$$\n\nwhere $\\hat{\\pi}^0$ is the predicted probability under $H_0$ and $\\hat{\\pi}_i^a = \\frac{\\exp \\{x_i^\\mathsf{T}\\boldsymbol{\\beta}\\}}{1 + \\exp \\{x_i^\\mathsf{T}\\boldsymbol{\\beta}\\}}$ is the predicted probability under $H_a$ [^3]\n\n[^3]: See @wilks1935likelihood for explanation of why -2 is included.\n\n## Drop-in-deviance test statistic\n\n$$\nG = -2\\sum_{i=1}^n \\Big[ y_i \\log \\Big(\\frac{\\hat{\\pi}^0}{\\hat{\\pi}^a_i}\\Big) + (1 - y_i)\\log \\Big(\\frac{1-\\hat{\\pi}^0}{1-\\hat{\\pi}^a_i}\\Big)\\Big]\n$$\n\n. . .\n\n::: incremental\n-   When $n$ is large, $G \\sim \\chi^2_p$, ( $G$ follows a Chi-square distribution with $p$ degrees of freedom)\n\n-   The p-value is calculated as $P(\\chi^2 > G)$\n\n-   Large values of $G$ (small p-values) indicate at least one $\\beta_j$ is non-zero\n:::\n\n## Heart disease model: drop-in-deviance test\n\n$$\n\\begin{aligned}\n&H_0: \\beta_{age} = \\beta_{totChol} = \\beta_{currentSmoker} = 0 \\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}$$\n\n. . .\n\n**Fit the null model** (we've already fit the alternative model)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnull_model <- glm(high_risk ~ 1, data = heart_disease, family = \"binomial\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) | -1.72294| 0.0436342|   -39.486|       0|\n\n\n:::\n:::\n\n\n\n\n## Heart disease model: drop-in-deviance test\n\n**Calculate the log-likelihood for the null and alternative models**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(L_0 <- glance(null_model)$logLik)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1737.735\n```\n\n\n:::\n\n```{.r .cell-code}\n(L_a <- glance(high_risk_fit)$logLik)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1612.406\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Calculate the likelihood ratio test statistic**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(G <- -2 * (L_0 - L_a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 250.6572\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n## Heart disease model: likelihood ratio test\n\n**Calculate the p-value**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(p_value <- pchisq(G, df = 3, lower.tail = FALSE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.717158e-54\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Conclusion**\n\nThe p-value is small, so we reject $H_0$. The data provide evidence that at least one predictor in the model has a non-zero coefficient.\n\n## Why use overall test? {.midi}\n\nWhy do we use a test for overall significance instead of just looking at the test for individual coefficients?[^4]\n\n[^4]: Example from *Introduction to Statistical Learning*\n\n. . .\n\nSuppose we have a model such that $p = 100$ and $H_0: \\beta_1 = \\dots = \\beta_{100} = 0$ is true\n\n. . .\n\n::: incremental\n-   About 5% of the p-values for individual coefficients will be below 0.05 by chance.\n\n-   So we expect to see 5 small p-values if even no linear association actually exists.\n\n-   Therefore, it is very likely we will see at least one small p-value by chance.\n\n-   The overall test of significance does not have this problem. There is only a 5% chance we will get a p-value below 0.05, if a relationship truly does not exist.\n:::\n\n# Test a subset of coefficients\n\n## Testing a subset of coefficients\n\n-   Suppose there are two models:\n\n    -   Reduced Model: includes predictors $x_1, \\ldots, x_q$\n\n    -   Full Model: includes predictors $x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p$\n\n-   We can use a **drop-in-deviance test** to determine if any of the new predictors are useful\n\n. . .\n\n$$\n\\begin{aligned}\n&H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n$$\n\n## Drop-in-deviance test\n\n$$\n\\begin{aligned}\n&H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n$$\n\n. . .\n\nThe test statistic is\n\n$$\n\\begin{aligned}\nG = D_{reduced} - D_{full} &= (-2\\log L_{reduced}) - (-2 \\log L_{full}) \\\\\n&= -2(\\log L_{reduced} - \\log L_{full})\n\\end{aligned}\n$$\n\n. . .\n\nThe p-value is calculated using a $\\chi_{\\Delta df}^2$ distribution, where $\\Delta df$ is the number of parameters being tested (the difference in number of parameters between the full and reduced model).\n\n## Example: Include `education`?\n\nShould we include `education` in the model?\n\n-   Reduced model: `age`, `totChol`, `currentSmoker`\n\n-   Full model: `age`, `totChol`, `currentSmoker` , `education`\n\n. . .\n\n$$\n\\begin{aligned}\n&H_0: \\beta_{ed2} = \\beta_{ed3} = \\beta_{ed4} = 0 \\\\\n&H_a: \\beta_j \\neq 0 \\text{ for at least one }j\n\\end{aligned}\n$$\n\n## Example: Include `education`?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreduced_model <- glm(high_risk ~ age + totChol + currentSmoker, \n              data = heart_disease, family = \"binomial\")\n\nfull_model <- glm(high_risk ~ age + totChol + currentSmoker + education, \n              data = heart_disease, family = \"binomial\")\n```\n:::\n\n\n\n\n. . .\n\n**Calculate deviances**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(deviance_reduced <- -2 * glance(reduced_model)$logLik)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3224.812\n```\n\n\n:::\n\n```{.r .cell-code}\n(deviance_full <- -2 * glance(full_model)$logLik)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3217.6\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Calculate test statistic**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n(G <- deviance_reduced - deviance_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.212113\n```\n\n\n:::\n:::\n\n\n\n\n## Example: Include `education`?\n\n**Calculate p-value**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npchisq(G, df = 3, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06543567\n```\n\n\n:::\n:::\n\n\n\n\n<br>\n\n. . .\n\n::: question\nWhat is your conclusion? Would you include `education` in the model that already has `age`, `totChol`, `currentSmoker`?\n:::\n\n## Drop-in-deviance test in R {.midi}\n\nConduct the drop-in-deviance test using the `anova()` function in R with option `test = \"Chisq\"`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(reduced_model, full_model, test = \"Chisq\") |> \n  tidy() |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term                                                  | df.residual| residual.deviance| df| deviance| p.value|\n|:-----------------------------------------------------|-----------:|-----------------:|--:|--------:|-------:|\n|high_risk ~ age + totChol + currentSmoker             |        4082|          3224.812| NA|       NA|      NA|\n|high_risk ~ age + totChol + currentSmoker + education |        4079|          3217.600|  3|    7.212|   0.065|\n\n\n:::\n:::\n\n\n\n\n## Add interactions with `currentSmoker`?  {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term                                                                                      | df.residual| residual.deviance| df| deviance| p.value|\n|:-----------------------------------------------------------------------------------------|-----------:|-----------------:|--:|--------:|-------:|\n|high_risk ~ age + totChol + currentSmoker                                                 |        4082|          3224.812| NA|       NA|      NA|\n|high_risk ~ age + totChol + currentSmoker + currentSmoker * age + currentSmoker * totChol |        4080|          3222.377|  2|    2.435|   0.296|\n\n\n:::\n:::\n\n\n\n\n## Questions from this week's content?\n\n<center>\n\n\n\n\n```{=html}\n<iframe width=\"640px\" height=\"480px\" src=\"https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTBw0r2_bKCVMs9lST1_-2sxUQ1JTSFBZNlFMWUJZSDcwTUdaVzgwWUhBMC4u&embed=true\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" style=\"border: none; max-width:100%; max-height:100vh\" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>\n```\n\n\n\n\n</center>\n\n## Recap \n\n-   Introduced model comparison for logistic regression using\n\n    -   AIC and BIC\n\n    -   Drop-in-deviance test\n\n## References\n",
    "supporting": [
      "21-logistic-comparison_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}