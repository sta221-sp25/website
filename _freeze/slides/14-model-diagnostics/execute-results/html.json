{
  "hash": "bd807678161dfc0eff6de54e332b3112",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model diagnostics\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-10-17\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n## Announcements\n\n-   Exam corrections (optional) due Thursday, October 24 at 11:59pm\n\n-   Labs resume on Monday\n\n-   Project: Exploratory data analysis due October 31\n\n-   Statistics experience due Tuesday, November 26\n\n## Computing set up \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n## Topics\n\n-   Review: Maximum likelihood estimation\n\n-   Influential points\n\n-   Model diagnostics\n\n    -   Leverage\n\n    -   Studentized residuals\n\n    -   Cook's Distance\n\n# Maximum likelihood estimation\n\n## Likelihood\n\n::: incremental\n-   A **likelihood** is a function that tells us how likely we are to observe our data for a given parameter value (or values). <!--# Find a better definition - Casella Berger maybe?-->\n\n-   Note that this is **not** the same as the probability function.\n\n    -   **Probability function**: Fixed parameter value(s) + input possible outcomes $\\Rightarrow$ probability of seeing the different outcomes given the parameter value(s)\n\n    -   **Likelihood function**: Fixed data + input possible parameter values $\\Rightarrow$ probability of seeing the fixed data for each parameter value\n:::\n\n## Maximum likelihood estimation\n\n-   Maximum likelihood estimation is the process of finding the values of the parameters that maximize the likelihood function , i.e., the values that are most likely given the observed data.\n\n-   There are three primary ways to find the maximum likelihood estimator\n\n    -   Approximate using a graph\n    -   Using calculus\n    -   Numerical approximation\n\n## Simple linear regression model\n\nSuppose we have the simple linear regression (SLR) model\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\hspace{10mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n$$\n\nsuch that $\\epsilon_i$ are independently and identically distributed.\n\n<br>\n\n. . .\n\nWe can write this model in the form below and use this to find the MLE\n\n$$\ny_i | x_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2_{\\epsilon})\n$$\n\n## Likelihood for SLR\n\nThe likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\nL&(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} | x_i, \\dots, x_n, y_i, \\dots, y_n) \\\\[5pt]\n &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_\n\\epsilon^2}}\\exp\\Big\\{{-\\frac{1}{2\\sigma_\\epsilon^2}(y_i - [\\beta_0 + \\beta_1x_i])^2}\\Big\\} \\\\[10pt]\n& = (2\\pi\\sigma^2_{\\epsilon})^{-\\frac{n}{2}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\Big\\}\n\\end{aligned}\n$$\n\n## Log-likelihood for SLR\n\nThe log-likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\n\\log &L(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} | x_i, \\dots, x_n, y_i, \\dots, y_n) \n  \\\\[8pt]\n& = -\\frac{n}{2}\\log(2\\pi\\sigma^2_{\\epsilon}) -\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\n\\end{aligned}\n$$\n\n<br>\n\n. . .\n\nWe will use the log-likelihood function to find the MLEs\n\n## MLE for $\\beta_0,\\beta_1, \\sigma^2_{\\epsilon}$\n\n$$\n\\tilde{\\beta}_0 = \\frac{1}{n}\\sum_{i=1}^ny_i  - \\frac{1}{n}\\tilde{\\beta}_1\\sum_{i=1}^n x_i \n$$\n\n<br>\n\n$$\n\\tilde{\\beta}_1 = \\frac{\\sum_{i=1}^n y_i(x_i - \\bar{x})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\n$$\n\n<br>\n\n$$\n\\tilde{\\sigma}^2_{\\epsilon} = \\frac{\\sum_{i=1}^n(y_i - \\tilde{\\beta}_0 - \\tilde{\\beta}_1x_i)^2}{n} = \\frac{\\sum_{i=1}^ne_i^2}{n}\n$$\n\n## MLE for linear regression in matrix form {.midi}\n\n$$\nL(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{(2\\pi)^{n/2}\\sigma^n_{\\epsilon}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\Big\\}\n$$\n\n. . .\n\n$$\n\\log L(\\boldsymbol{\\beta}, \\sigma^2_\\epsilon | \\mathbf{X}, \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - n \\log(\\sigma_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})\n$$\n\n. . .\n\n::: question\n1.  For a fixed value of $\\sigma_\\epsilon$ , we know that $\\log L$ is maximized when what is true about $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ ?\n2.  What does this tell us about the relationship between the MLE and least-squares estimator for $\\boldsymbol{\\beta}$?\n:::\n\n## Why maximum likelihood estimation?\n\n-   *\"Maximum likelihood estimation is, by far, the most popular technique for deriving estimators.\"* [@casella2024statistical, pp. 315]\n\n-   MLEs have nice statistical properties. They are\n\n    -   Consistent\n\n    -   Efficient - Have the smallest MSE among all consistent estimators\n\n    -   Asymptotically normal\n\n## Putting it all together\n\n::: incremental\n-   The MLE $\\tilde{\\boldsymbol{\\beta}}$ is equivalent to the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ , when the errors follow independent and identical normal distributions\n\n-   This means the least-squares estimator $\\hat{\\mathbf{\\boldsymbol{\\beta}}}$ and inherit all the nice properties of MLEs\n\n    -   Consistency\n    -   Efficiency - minimum variance among all consistent estimators\n    -   Asymptotically normal\n:::\n\n## Putting it all together {.incremental}\n\n-   From previous work, we also know $\\hat{\\boldsymbol{\\beta}}$ is unbiased and thus the MLE $\\tilde{\\boldsymbol{\\beta}}$ is unbiased\n-   Note that the MLE $\\tilde{\\sigma}^2_{\\epsilon}$ is *asymptotically unbiased*\n    -   The estimate from least-squares $\\hat{\\sigma}_{\\epsilon}^2$ is unbiased\n\n# Model diagnostics\n\n## Data: Duke lemurs {.midi}\n\nToday's data contains a subset of the original Duke Lemur data set available in the [TidyTuesday GitHub repo](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-08-24/readme.md). This data includes information on â€œyoung adultâ€ lemurs from the [Coquerelâ€™s sifaka species](https://lemur.duke.edu/discover/meet-the-lemurs/coquerels-sifaka/) (PCOQ), the largest species at the Duke Lemur Center. The analysis will focus on the following variables:\n\n-   `age_at_wt_mo`: Age in days: Age of the animal when the weight was taken, in days (Wei ght_Date-DOB)\n\n-   `weight_g`: Weight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights \\>500g generally to the nearest 1-20g.\n\n**The goal of the analysis is to use the age of the lemurs to understand variability in the weight.**\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## EDA\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Fit model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_fit <- lm(weight_g ~ age_at_wt_mo, data = lemurs)\n\ntidy(lemurs_fit) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  | 3133.284|   353.499|     8.864|   0.000|\n|age_at_wt_mo |   19.558|    10.083|     1.940|   0.056|\n\n\n:::\n:::\n\n\n\n## Model conditions\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n-   Linearity\n\n-   Constant variance\n\n## Model conditions \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n-   Normality\n\n-   Independence\n\n## Model diagnostics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_aug <- augment(lemurs_fit)\n\nlemurs_aug |> slice(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 8\n   weight_g age_at_wt_mo .fitted .resid   .hat .sigma    .cooksd .std.resid\n      <dbl>        <dbl>   <dbl>  <dbl>  <dbl>  <dbl>      <dbl>      <dbl>\n 1     3400         32.0   3758. -358.  0.0158   516. 0.00396       -0.703 \n 2     4143         46.2   4037.  106.  0.0655   517. 0.00159        0.213 \n 3     3581         43.1   3977. -396.  0.0414   515. 0.0134        -0.787 \n 4     3620         33.0   3778. -158.  0.0141   517. 0.000690      -0.310 \n 5     3720         32.4   3768.  -47.9 0.0149   517. 0.0000668     -0.0940\n 6     3540         35.4   3825. -285.  0.0134   516. 0.00212       -0.559 \n 7     4440         37.3   3863.  577.  0.0161   513. 0.0105         1.13  \n 8     4440         32.6   3770.  670.  0.0147   511. 0.0129         1.31  \n 9     3770         31.8   3754.   15.6 0.0162   517. 0.00000767     0.0305\n10     3920         31.9   3757.  163.  0.0159   517. 0.000828       0.320 \n```\n\n\n:::\n:::\n\n\n\n## Model diagnostics in R {.midi}\n\nUse the `augment()` function in the broom package to output the model diagnostics (along with the predicted values and residuals)\n\n-   response and predictor variables in the model\n-   `.fitted`: predicted values\n-   `.se.fit`: standard errors of predicted values\n-   `.resid`: residuals\n-   `.hat`: leverage\n-   `.sigma`: estimate of residual standard deviation when the corresponding observation is dropped from model\n-   `.cooksd`: Cook's distance\n-   `.std.resid`: standardized residuals\n\n## Influential Point\n\nAn observation is **influential** if removing has a noticeable impact on the regression coefficients\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n\n## Influential points\n\n::: incremental\n-   Influential points have a noticeable impact on the coefficients and standard errors used for inference\n-   These points can sometimes be identified in a scatterplot if there is only one predictor variable\n    -   This is often not the case when there are multiple predictors\n-   We will use measures to quantify an individual observation's influence on the regression model\n    -   **leverage**, **standardized residuals**, and **Cook's distance**\n:::\n\n# Leverage \n\n## Hat matrix\n\n-   Recall the **hat matrix** $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n\n-   We've seen that $\\mathbf{H}$ is used to compute $Var(\\hat{\\mathbf{y}}) = \\sigma^2_{\\epsilon}\\mathbf{H}$ and $Var(\\mathbf{e}) = \\sigma^2_{\\epsilon}(\\mathbf{I} - \\mathbf{H})$\n\n-   An element of $\\mathbf{H}$, $h_{ij}$, is the leverage of the observation $y_i$ on the fitted values $\\hat{y}_{ij}$\n\n## Leverage \n\n-   We focus on the diagonal elements\n\n    $$\n    h_{ii} = \\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i\n    $$such that $\\mathbf{x}^T_i$ is the $i^{th}$ row of $\\mathbf{X}$\n\n-   $h_{ii}$ is the **leverage**: a measure of the distance of the $i^{th}$ observation from the center (or centroid) of $x$ space\n\n-   Observations with large values of $h_{ii}$ are far away from the typical value (or combination of values) of the <u>predictors</u> in the data\n\n## Large leverage\n\n-   The sum of the leverages for all points is $p + 1$, where $p$ is the number of predictors in the model . More specifically\n\n    $$\n    \\sum_{i=1}^n h_{ii} = \\text{rank}(\\mathbf{H}) = \\text{rank}(\\mathbf{X}) = p+1\n    $$\n\n-   The average value of leverage, $h_{ii}$, is $\\bar{h} =  \\frac{(p+1)}{n}$\n\n-   An observation has **large leverage** if $$h_{ii} > \\frac{2(p+1)}{n}$$\n\n## Lemurs: Leverage\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nh_threshold <- 2 * 2 / nrow(lemurs)\nh_threshold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05263158\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlemurs_aug |>\n  filter(.hat > h_threshold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 Ã— 8\n  weight_g age_at_wt_mo .fitted .resid   .hat .sigma .cooksd .std.resid\n     <dbl>        <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n1     4143         46.2   4037.  106.  0.0655   517. 0.00159     0.213 \n2     4313         58.2   4272.   41.1 0.229    517. 0.00123     0.0910\n3     4640         54.9   4208.  432.  0.173    514. 0.0895      0.925 \n4     3677         47.4   4061. -384.  0.0770   515. 0.0253     -0.778 \n5     4319         58.2   4272.   47.1 0.229    517. 0.00161     0.104 \n6     3610         47.4   4061. -451.  0.0770   514. 0.0348     -0.914 \n7     3597         48.6   4084. -487.  0.0889   514. 0.0480     -0.992 \n```\n\n\n:::\n:::\n\n\n\n. . .\n\n::: question\nWhy do you think these points have large leverage?\n:::\n\n## Let's look at the data\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n## Large leverage\n\nIf there is point with high leverage, ask\n\n-   â“ Is there a data entry error?\n\n-   â“ Is this observation within the scope of individuals for which you want to make predictions and draw conclusions?\n\n-   â“ Is this observation impacting the estimates of the model coefficients? (Need more information!)\n\n. . .\n\nJust because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore we need to check other measures.\n\n## \n\n# Scaled residuals\n\n## Scaled residuals\n\n-   What is the best way to identify outlier points that don't fit the pattern from the regression line?\n    -   Look for points that have large residuals\n-   We can rescale residuals and put them on a common scale to more easily identify \"large\" residuals\n-   We will consider two types of scaled residuals: standardized residuals and studentized residuals\n\n## Standardized residuals\n\n-   The variance of the residuals can be estimated by the mean squared residuals (MSR) $= \\frac{SSR}{n - p - 1} = \\hat{\\sigma}^2_{\\epsilon}$\n\n-   We can use MSR to compute **standardized residuals**\n\n    $$\n    std.res_i = \\frac{e_i}{\\sqrt{MSR}}\n    $$\n\n-   Standardized residuals are produced by `augment()` in the column `.std.resid`\n\n------------------------------------------------------------------------\n\n## Studentized residuals\n\n-   MSR is an approximation of the variance of the residuals.\n\n-   The variance of the residuals is $Var(\\mathbf{e}) = \\sigma^2_{\\epsilon}(\\mathbf{I} - \\mathbf{H})$\n\n    -   The variance of the $i^{th}$ residual is $Var(e_i) = \\sigma^2_{\\epsilon}(1 - h_{ii})$\n\n-   The **studentized residual** is the residual rescaled by the more exact calculation for variance\n\n$$\nr_i = \\frac{e_{i}}{\\sqrt{\\hat{\\sigma}^2_{\\epsilon}(1 - h_{ii})}}\n$$\n\n-   Standardized and studentized residuals provide similar information about which points are outliers in the response.\n\n## Using standardized residuals\n\nWe can examine the standardized residuals directly from the output from the `augment()` function\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/std residuals vs fitted-1.png){width=960}\n:::\n:::\n\n\n\n-   An observation is a *potential outlier* if its standardized residual is beyond $\\pm 3$\n\n## Digging in to the data \n\nLet's look at the value of the response variable to better understand potential outliers\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n# Cook's Distance\n\n## Motivating Cook's Distance\n\n-   An observation's influence on the regression line depends on\n\n    -   How close it lies to the general trend of the data\n\n    -   Its leverage\n\n-   **Cook's Distance** is a statistic that includes both of these components to measure an observation's overall impact on the model\n\n## Cook's Distance\n\nCook's distance for the $i^{th}$ observation is\n\n$$\nD_i = \\frac{r^2_i}{p + 1}\\Big(\\frac{h_{ii}}{1 - h_{ii}}\\Big)\n$$\n\n. . .\n\nThis measure is a combination of\n\n-   How well the model fits the $i^{th}$ observation (magnitude of residuals)\n\n-   How far the $i^{th}$ observation is from the rest of the data (where the point is in the $x$ space)\n\n## Using Cook's Distance\n\n-   An observation with large $D_i$ is said to have a strong influence on the predicted values\n\n-   General thresholds .An observation with\n\n    -    $D_i > 0.5$ is **moderately influential**\n\n    -   $D_i > 1$ is **very influential**\n\n## Cook's Distance\n\nCook's Distance is in the column `.cooksd` in the output from the `augment()` function\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-model-diagnostics_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Using these measures\n\n-   Standardized residuals, leverage, and Cook's Distance should all be examined together\n\n-   Examine plots of the measures to identify observations that are outliers, high leverage, and may potentially impact the model.\n\n## What to do with outliers/influential points?\n\n-   First consider if the outlier is a result of a data entry error.\n\n-   If not, you may consider dropping an observation if it's an outlier in the predictor variables if...\n\n    -   It is meaningful to drop the observation given the context of the problem\n\n    -   You intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions\n\n## What to do with outliers/influential points?\n\n-   It is generally **not** good practice to drop observations that ar outliers in the value of the response variable\n\n    -   These are legitimate observations and should be in the model\n\n    -   You can try transformations or increasing the sample size by collecting more data\n\n-   A general strategy when there are influential points is to fit the model with and without the influential points and compare the outcomes\n\n------------------------------------------------------------------------\n\n## Recap\n\n-   Reviewed Maximum likelihood estimation\n\n-   Influential points\n\n-   Model diagnostics\n\n    -   Leverage\n\n    -   Studentized residuals\n\n    -   Cook's Distance\n\n## References\n",
    "supporting": [
      "14-model-diagnostics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}