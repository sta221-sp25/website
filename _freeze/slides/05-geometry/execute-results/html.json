{
  "hash": "25220121738af144e43e261b035a9b32",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Geometric interpretation of least-squares regression\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-01-23\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 05-geometry-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n## Announcements\n\n-   HW 01 due **Thursday, January 30 at 11:59pm**\n\n    -   Released after class.\n\n    -   Make sure you are a member of the course GitHub organization\n\n        -   If you can see the number of people in the org, then you are a member!\n\n## Topics\n\n-   Geometric interpretation of least-squares regression\n\n## Recap: Regression in matrix from\n\nThe simple linear regression model can be represented using vectors and matrices as\n\n::: equation\n$$\n\\large{\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}}\n$$\n:::\n\n-   $\\mathbf{y}$ : Vector of responses\n\n-   $\\mathbf{X}$: Design matrix (columns for predictors + intercept)\n\n-   $\\boldsymbol{\\beta}$: Vector of model coefficients\n\n-   $\\boldsymbol{\\epsilon}$: Vector of error terms centered at $\\mathbf{0}$ with variance $\\sigma^2_{\\epsilon}\\mathbf{I}$\n\n## Recap: Derive $\\hat{\\boldsymbol{\\beta}}$\n\nWe used matrix calculus to derive the estimator $\\hat{\\boldsymbol{\\beta}}$ that minimizes $\\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon}$\n\n::: equation\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}$$\n:::\n\n. . .\n\nNow let's consider how to derive the least-squares estimator using a geometric interpretation of regression\n\n## Geometry of least-squares regression\n\n::: incremental\n-   Let $\\text{Col}(\\mathbf{X})$ be the **column space** of $\\mathbf{X}$: the set all possible linear combinations (span) of the columns of $\\mathbf{X}$\n\n-   The vector of responses $\\mathbf{y}$ is not in $\\text{Col}(\\mathbf{X})$.\n\n-   **Goal:** Find another vector $\\mathbf{z} = \\mathbf{Xb}$ that is in $\\text{Col}(\\mathbf{X})$ and is as close as possible to $\\mathbf{y}$.\n\n    -   $\\mathbf{z}$ is a **projection** of $\\mathbf{y}$ onto $\\text{Col}(\\mathbf{X})$ .\n:::\n\n## Geometry of least-squares regression\n\n::: incremental\n-   For any $\\mathbf{z} = \\mathbf{Xb}$ in $\\text{Col}(\\mathbf{X})$, the vector $\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}$ is the difference between $\\mathbf{y}$ and $\\mathbf{Xb}$.\n\n    -   We want to find $\\mathbf{b}$ such that $\\mathbf{z} = \\mathbf{Xb}$ is as close as possible to $\\mathbf{y}$, i.e, we want to minimize the difference $\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}$\n\n-   This distance is minimized when $\\mathbf{e}$ is orthogonal to $\\text{Col}(\\mathbf{X})$\n:::\n\n## Geometry of least-squares regression\n\n-   **Note:** If $\\mathbf{A}$, an $n \\times k$ matrix, is orthogonal to an $n \\times 1$ vector $\\mathbf{c}$, then $\\mathbf{A}^\\mathsf{T}\\mathbf{c} = \\mathbf{0}$\n\n-   Therefore, we have $\\mathbf{X}^\\mathsf{T}\\mathbf{e} = \\mathbf{0}$ , and thus\n\n    $$\n    \\mathbf{X}^\\mathsf{T}(\\mathbf{y} - \\mathbf{Xb}) = \\mathbf{0}\n    $$\n\n::: question\nSolve for $\\mathbf{b}$ .\n:::\n\n## Hat matrix\n\n-   Recall the hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}$.\n\n-   $\\hat{\\mathbf{y}} = \\mathbf{Hy}$, so $\\mathbf{H}$ is a projection of $\\mathbf{y}$ onto $\\mathbf{Xb}$\n\n-   Properties of $\\mathbf{H}$, a projection matrix\n\n    -   $\\mathbf{H}$ is symmetric ($\\mathbf{H}^\\mathsf{T} = \\mathbf{H}$)\n\n    -   $\\mathbf{H}$ is idempotent ($\\mathbf{H}^2 = \\mathbf{H}$)\n\n    -   If $\\mathbf{v}$ in $\\text{Col}(\\mathbf{X})$, then $\\mathbf{Hv} = \\mathbf{v}$\n\n    -   If $\\mathbf{v}$ is orthogonal to $\\text{Col}(\\mathbf{X})$, then $\\mathbf{Hv} = \\mathbf{0}$\n",
    "supporting": [
      "05-geometry_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}