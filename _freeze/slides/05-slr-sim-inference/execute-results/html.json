{
  "hash": "d00da60bb3769e24013e7d6669e99076",
  "result": {
    "markdown": "---\ntitle: \"SLR: Simulation-based inference\"\nsubtitle: \"Bootstrap confidence intervals for the slope\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2022-09-12\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— Week 03](https://sta210-fa22.netlify.app/weeks/week-03.html)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\n  warning: false\n  message: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Announcements\n\n-   Lab 01 due\n\n    -   TODAY, 11:59pm (Thursday labs)\n\n    -   Tuesday, 11:59pm (Friday labs)\n\n    -   Make sure all work is pushed to GitHub and the PDF is submitted on Gradescope by the deadline\n\n<!-- -->\n\n-   See [Week 03](https://sta210-fa22.netlify.app/weeks/week-03.html) for this week's activities.\n\n## Topics\n\n-   Assess model's predictive importance using data splitting and bootstrapping\n\n-   Find range of plausible values for the slope using bootstrap confidence intervals\n\n## Computational setup\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n```\n:::\n\n\n# Recap of last class\n\n## Uninsurance vs. HS graduation rates\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )\n```\n\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/nc-uninsured-hsgrad-scatter-line-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Fitting the model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnc_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7\n```\n:::\n:::\n\n\n## Augmenting the data\n\nWith `augment()` to add columns for predicted values (`.fitted`), residuals (`.resid`), etc.:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnc_aug <- augment(nc_fit$fit)\nnc_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 Ã— 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>      <dbl>      <dbl>\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# â€¦ with 90 more rows\n```\n:::\n:::\n\n\n## Statistics for model evaluation {.midi}\n\n::: incremental\n-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n    $$\n    R^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n    $$\n\n-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n    $$\n    RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n    $$\n:::\n\n## Obtaining $R^2$ and RMSE\n\n::: incremental\nUse `rsq()` and `rmse()`, respectively\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.243\n```\n:::\n\n```{.r .cell-code}\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        2.07\n```\n:::\n:::\n\n:::\n\n## Purpose of model evaluation\n\n-   $R^2$ tells us how our model is doing to predict the data we *already have*\n-   But generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. **out-of-sample prediction**\n-   We have a couple ways of *simulating* out-of-sample prediction before actually getting new data to evaluate the performance of our models\n\n# Splitting data\n\n## Spending our data\n\n::: incremental\n-   There are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\n-   Doing all of this on the entire data we have available leaves us with no other data to assess our choices\n-   We can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we've done so far)\n:::\n\n## Simulation: data splitting {.smaller}\n\n::: columns\n::: {.column width=\"30%\"}\n::: nonincremental\n-   Take a random sample of 10% of the data and set aside (testing data)\n-   Fit a model on the remaining 90% of the data (training data)\n-   Use the coefficients from this model to make predictions for the testing data\n-   Repeat 10 times\n:::\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Predictive performance {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n::: question\n::: nonincremental\n-   How consistent are the predictions for different testing datasets?\n-   How consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?\n:::\n:::\n:::\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n# Bootstrapping\n\n## Bootstrapping our data\n\n::: incremental\n-   The idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\n-   With bootstrapping, we simulate resampling from the population by resampling from the sample we observed\n-   Bootstrap samples are the sampled *with replacement* from the original sample and same size as the original sample\n    -   For example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc.\n:::\n\n## Simulation: bootstrapping {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n::: nonincremental\n-   Take a bootstrap sample -- sample with replacement from the original data, same size as the original data\n-   Fit model to the sample and make predictions for that sample\n-   Repeat many times\n:::\n:::\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Predictive performance {.smaller}\n\n::: columns\n::: {.column width=\"25%\"}\n::: question\n::: nonincremental\n-   How consistent are the predictions for different bootstrap datasets?\n-   How consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?\n:::\n:::\n:::\n\n::: {.column width=\"75%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Recap\n\n-   Motivated the importance of model evaluation\n\n-   Described how $R^2$ and RMSE are used to evaluate models\n\n-   Assessed model's predictive importance using data splitting and bootstrapping\n\n# Simulation-based inference\n\n## Data: Houses in Duke Forest\n\n::: columns\n::: {.column width=\"50%\"}\n-   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\n-   Scraped from Zillow\n-   Source: [`openintro::duke_forest`](http://openintrostat.github.io/openintro/reference/duke_forest.html)\n:::\n\n::: {.column width=\"50%\"}\n![](images/05/duke_forest_home.jpg){fig-alt=\"Home in Duke Forest\"}\n:::\n:::\n\n**Goal**: Use the area (in square feet) to understand variability in the price of houses in Duke Forest.\n\n## Exploratory data analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar()) \n```\n\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Modeling {.midi}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|5|6\"}\ndf_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |>\n  kable(digits = 2) #neatly format table to 2 digits\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 116652.33 </td>\n   <td style=\"text-align:right;\"> 53302.46 </td>\n   <td style=\"text-align:right;\"> 2.19 </td>\n   <td style=\"text-align:right;\"> 0.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> area </td>\n   <td style=\"text-align:right;\"> 159.48 </td>\n   <td style=\"text-align:right;\"> 18.17 </td>\n   <td style=\"text-align:right;\"> 8.78 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n. . .\n\n<br>\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n-   **Intercept:** Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\n-   **Slope:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n## From sample to population {.midi}\n\n> For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n<br>\n\n-   This estimate is valid for the single sample of 98 houses.\n-   But what if we're not interested quantifying the relationship between the size and price of a house in this single sample?\n-   What if we want to say something about the relationship between these variables for all houses in Duke Forest?\n\n## Statistical inference\n\n-   **Statistical inference** provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\n\n-   For our inferences to be valid, the sample should be random and representative of the population we're interested in\n\n## Inference for simple linear regression\n\n-   Calculate a confidence interval for the slope, $\\beta_1$\n\n-   Conduct a hypothesis test for the slope,$\\beta_1$\n\n# Confidence interval for the slope\n\n## Confidence interval {.midi}\n\n::: incremental\n-   A plausible range of values for a population parameter is called a **confidence interval**\n-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\n    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter\n:::\n\n## Confidence interval for the slope {.midi}\n\nA confidence interval will allow us to make a statement like \"*For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus X dollars.*\"\n\n. . .\n\n-   Should X be \\$10? \\$100? \\$1000?\n\n-   If we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by \\$10? \\$100? \\$1000?\n\n-   The answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\n\n-   We need a way to quantify the variability of the sample statistic\n\n## Quantify the variability of the slope {.midi}\n\n**for estimation**\n\n::: incremental\n-   Two approaches:\n    1.  Via simulation (what we'll do today)\n    2.  Via mathematical models (what we'll do in the next class)\n-   **Bootstrapping** to quantify the variability of the slope for the purpose of estimation:\n    -   Bootstrap new samples from the original sample\n    -   Fit models to each of the samples and estimate the slope\n    -   Use features of the distribution of the bootstrapped slopes to construct a confidence interval\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Bootstrap sample 1\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Bootstrap sample 2\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Bootstrap sample 3\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Bootstrap sample 4\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Bootstrap sample 5\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n. . .\n\n*so on and so forth...*\n\n## Bootstrap samples 1 - 5\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Bootstrap samples 1 - 100\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-27-1.png){fig-align='center' width=100%}\n:::\n:::\n\n:::\n:::\n\n## Slopes of bootstrap samples\n\n::: question\n**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus \\_\\_\\_ dollars.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Slopes of bootstrap samples\n\n::: question\n**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus \\_\\_\\_ dollars.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## Confidence level\n\n::: question\nHow confident are you that the true slope is between \\$0 and \\$250? How about \\$150 and \\$170? How about \\$90 and \\$210?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-30-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n## 95% confidence interval {.midi}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-slr-sim-inference_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n::: incremental\n-   A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\n-   We are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77.\n:::\n\n## Application exercise\n\n::: appex\nðŸ“‹ [AE 03: Bootstrap confidence intervals](https://sta210-fa22.netlify.app/ae/ae-03-bootstrap.html)\n:::\n\n## Computing the CI for the slope I\n\nCalculate the observed slope:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nobserved_fit <- duke_forest |>\n  specify(price ~ area) |>\n  fit()\n\nobserved_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159.\n```\n:::\n:::\n\n\n## Computing the CI for the slope II {.smaller}\n\nTake `100` bootstrap samples and fit models to each one:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,5,6\"}\nset.seed(1120)\n\nboot_fits <- duke_forest |>\n  specify(price ~ area) |>\n  generate(reps = 100, type = \"bootstrap\") |>\n  fit()\n\nboot_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 Ã— 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       <int> <chr>        <dbl>\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# â€¦ with 190 more rows\n```\n:::\n:::\n\n\n## Computing the CI for the slope III\n\n**Percentile method:** Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528.\n```\n:::\n:::\n\n\n## Computing the CI for the slope IV\n\n**Standard error method:** Alternatively, compute the 95% CI as the point estimate $\\pm$ \\~2 standard deviations of the bootstrap distribution:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"se\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          90.8     228.\n2 intercept -56788.   290093.\n```\n:::\n:::\n\n\n## Precision vs. accuracy\n\n::: question\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n:::\n\n. . .\n\n![](images/05/garfield.png)\n\n## Precision vs. accuracy\n\n::: question\nHow can we get best of both worlds -- high precision and high accuracy?\n:::\n\n## Changing confidence level\n\n::: question\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|4\"}\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528.\n```\n:::\n:::\n\n\n## Changing confidence level {.midi}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          104.     212.\n2 intercept  -24380.  256730.\n```\n:::\n\n```{.r .cell-code}\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          56.3     226.\n2 intercept -61950.   370395.\n```\n:::\n:::\n\n\n## Recap {.smaller}\n\n-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)\n\n-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)\n\n-   Sample statistic $\\ne$ population parameter, but if the sample is good, it can be a good estimate\n\n-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\n\n-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\n\n-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n",
    "supporting": [
      "05-slr-sim-inference_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}