{
  "hash": "25d646aaf49486ec2d231a7f97a264a2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Properties of estimators\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-09-26\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n\n\n## Announcements\n\n-   Project\n\n    -   Research questions due **TODAY**\n\n    -   Proposal due Thursday, October 3 at 11:59pm\n\n-   Lab 03 due Thursday, October 3 at 11:59pm\n\n-   HW 02 due Thursday, October 3 at 11:59pm (released after class)\n\n-   [Statistics experience](https://sta221-fa24.netlify.app/hw/stats-experience) due **Tue, Nov 26 at 11:59pm**\n\n## Topics\n\n-   Compute and interpret confidence interval for a single coefficient\n\n-   Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n-   Define \"linear\" model\n\n## Computing setup\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(kableExtra)  \nlibrary(patchwork)   \n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())\n```\n:::\n\n\n\n## Data: NCAA Football expenditures {.midi}\n\nToday's data come from [Equity in Athletics Data Analysis](https://ope.ed.gov/athletics/#/datafile/list) and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a [March 2022 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md).\n\nWe will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :\n\n-   `total_exp_m`: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\n\n-   `enrollment_th`: Total student enrollment in the 2019 - 2020 academic year (in thousands)\n\n-   `type`: institution type (Public or Private)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n```         \n```\n\n## Regression model\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nexp_fit <- lm(total_exp_m ~ enrollment_th + type, data = football)\ntidy(exp_fit) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|\n|enrollment_th |    0.780|     0.110|     7.074|       0|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|\n\n\n:::\n:::\n\n\n\n## Inference for $\\beta_j$ {.midi}\n\nWe often want to conduct inference on individual model coefficients\n\n-   **Hypothesis test:** Is there a linear relationship between the response and $x_j$?\n\n-   **Confidence interval**: What is a plausible range of values $\\beta_j$ can take?\n\n# Confidence interval for $\\beta_j$\n\n## Confidence interval for $\\beta_j$ {.midi}\n\n::: incremental\n-   A plausible range of values for a population parameter is called a **confidence interval**\n\n-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\n    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\n\n    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter\n:::\n\n## What \"confidence\" means {.midi}\n\n::: {.incremental .extrapad}\n-   We will construct $C\\%$ confidence intervals\n\n    -   The confidence level impacts the width of the interval\n\n-   \"Confidence\" means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate $C\\%$ CIs for the coefficient of $x_j$, then $C\\%$ of those intervals will contain the true value of the coefficient $\\beta_j$\n\n-   Need to balance precision and accuracy when selecting a confidence level\n:::\n\n## Confidence interval for $\\beta_j$\n\n$$\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n$$\n\n<br>\n\n. . .\n\n$$\n\\hat{\\beta}_1 \\pm t^* \\times SE({\\hat{\\beta}_j})\n$$\n\nwhere $t^*$ is calculated from a $t$ distribution with $n-p-1$ degrees of freedom\n\n## Computing $t^*$ in R\n\n::: {.fragment fragment-index=\"1\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 95%\nqt(0.975, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.97928\n```\n\n\n:::\n:::\n\n\n:::\n\n<br>\n\n::: {.fragment fragment-index=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 90%\nqt(0.95, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.657235\n```\n\n\n:::\n:::\n\n\n:::\n\n<br>\n\n::: {.fragment fragment-index=\"3\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# confidence level: 99%\nqt(0.995, df = nrow(football) - 2 - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.61606\n```\n\n\n:::\n:::\n\n\n:::\n\n## 95% CI for coefficient of enrollment\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|\n|enrollment_th |    0.780|     0.110|     7.074|       0|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n$$\n\\hat{\\beta}_j \\pm t^* \\times SE(\\hat{\\beta}_j)\n$$\n\n. . .\n\n$$\n0.7804 \\pm 1.9793 \\times 0.1103\n$$\n\n. . .\n\n$$\n[0.562, 0.999]\n$$\n\n## Interpreting the CI\n\nðŸ”— [edstem.org/us/courses/62513/discussion/648045](https://edstem.org/us/courses/62513/discussion/648045)\n\n\n\n```{=html}\n<iframe allowfullscreen frameborder=\"0\" height=\"100%\" mozallowfullscreen style=\"min-width: 500px; min-height: 355px\" src=\"https://edstem.org/us/courses/62513/discussion/648045\" width=\"100%\"></iframe>\n```\n\n\n## Computing CI in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1\"}\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |> \n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:-------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)   |   19.332|     2.984|     6.478|       0|   13.426|    25.239|\n|enrollment_th |    0.780|     0.110|     7.074|       0|    0.562|     0.999|\n|typePublic    |  -13.226|     3.153|    -4.195|       0|  -19.466|    -6.986|\n\n\n:::\n:::\n\n\n\n# Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n## Motivation {.midi}\n\n::: incremental\n-   We have discussed how to use least squares to find an estimator of $\\hat{\\boldsymbol{\\beta}}$\n\n-   How do we know whether our least squares estimator is a \"good\" estimator?\n\n-   When we consider what makes an estimator \"good\", we'll look at three criteria:\n\n    -   Bias\n    -   Variance\n    -   Mean squared error\n\n-   We'll take a look at these over the course of a few lectures and motivate why we might prefer using least squares to compute $\\hat{\\boldsymbol{\\beta}}$ versus other methods\n:::\n\n## Bias and variance\n\nSuppose you are throwing darts at a target\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\n![Image source: [Analytics Vidhya](https://medium.com/analytics-vidhya/bias-variance-tradeoff-regularization-5543d2d1ad8a)](images/10/bias-variance.webp)\n:::\n\n::: {.column width=\"50%\"}\n-   **Unbiased**: Darts distributed around the target\n\n-   **Biased**: Darts systematically away from the target\n\n-   **Variance**: Darts could be widely spread (high variance) or generally clustered together (low variance)\n:::\n:::\n\n## Bias and variance\n\n-   **Ideal scenario**: Darts are clustered around the target (unbiased and low variance)\n\n-   **Worst case scenario**: Darts are widely spread out and systematically far from the target (high bias and high variance)\n\n-   **Acceptable scenario:** There's some trade-off between the bias and variance. For example, it may be acceptable for the darts to be clustered around a point that is close to the target (low bias and low variance)\n\n## Bias and variance\n\n::: incremental\n-   Each time we take a sample of size $n$, we can find the least squares estimator (throw dart at target)\n\n-   Suppose we take many independent samples of size $n$ and find the least squares estimator for each sample (throw many darts at the target). Ideally,\n\n    -   The estimators are centered at the true parameter (unbiased)\n\n    -   The estimators are clustered around the true parameter (unbiased with low variance)\n:::\n\n. . .\n\nLet's take a look at the mean and variance of the least squares estimator\n\n## Expected value of $\\hat{\\boldsymbol{\\beta}}$\n\nThe **bias** of an estimator is the difference between the estimator's expected value and the true value of the parameter\n\n. . .\n\nLet $\\hat{\\theta}$ be an estimator of the parameter $\\theta$. Then\n\n$$\nBias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n$$\n\n. . .\n\nAn estimator is **unbiased** if the bias is 0 and thus $E(\\hat{\\theta}) = \\theta$\n\n## Finding expected value and variance\n\nLet $\\mathbf{A}$ be a $n \\times p$ matrix of constants and $\\mathbf{b}$ a $p \\times 1$ vector of random variables. Then\n\n$$\nE(\\mathbf{Ab}) = \\mathbf{A}E(\\mathbf{b})\n$$\n\n$$\nVar(\\mathbf{Ab}) = \\mathbf{A}Var(\\mathbf{b})\\mathbf{A}^T\n$$\n\n## Expected value of $\\hat{\\boldsymbol{\\beta}}$\n\nLet's take a look at the expected value of the least squares estimator. Given $E(\\boldsymbol{\\epsilon}) = \\mathbf{0}$,\n\n$$\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon})]} \\\\[8pt]\n& = \\class{fragment}{E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}] + E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]}\\\\[8pt]\n& = \\class{fragment}{\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^TE(\\boldsymbol{\\epsilon})} \\\\[8pt]\n & = \\class{fragment}{\\boldsymbol{\\beta}}\n\\end{aligned}\n$$\n\n------------------------------------------------------------------------\n\n## Expected value of $\\hat{\\boldsymbol{\\beta}}$\n\nThe least squares estimator $\\hat{\\boldsymbol{\\beta}}$ is an *unbiased* estimator of $\\boldsymbol{\\beta}$\n\n$$\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\n$$\n\n<br>\n\n. . .\n\nNow let's take a look at the variance\n\n## Variance of $\\hat{\\boldsymbol{\\beta}}$\n\n$$\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= Var((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) \\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]Var(\\mathbf{y})[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]^T }\\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]\\sigma^2_{\\epsilon}\\mathbf{I}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}}\n\\end{aligned}\n$$\n\n## Variance of $\\hat{\\boldsymbol{\\beta}}$\n\n$$\nVar(\\hat{\\boldsymbol{\\beta}}) =  \\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\n\nWe will show that $\\hat{\\boldsymbol{\\beta}}$ is the \"best\" estimator (has the lowest variance) among the class of linear unbiased estimators\n\n# What do we mean by \"linear\"?\n\n## \"Linear\" regression model\n\nWhat does it mean for a model to be a \"linear\" regression model?\n\n. . .\n\n-   Linear regression models are linear in the parameters, i.e. given an observation $y_i$\n\n    $$\n    y_i = \\beta_0 + \\beta_1f_1(x_{i1}) +  \\dots + \\beta_pf_p(x_{ip}) + \\epsilon_i\n    $$\n\n-   The functions $f_1, \\ldots, f_p$ can be non-linear as long as $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are linear in $Y$\n\n## Identify the linear regression model\n\nðŸ”— [edstem.org/us/courses/62513/discussion/648051](https://edstem.org/us/courses/62513/discussion/648051)\n\n\n\n```{=html}\n<iframe allowfullscreen frameborder=\"0\" height=\"100%\" mozallowfullscreen style=\"min-width: 500px; min-height: 355px\" src=\"https://edstem.org/us/courses/62513/discussion/648051\" width=\"100%\"></iframe>\n```\n\n\n## Identify the linear regression model\n\n::: extrapad\n1.  $y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i1}^2 + \\beta_3x_{i2}  + \\epsilon_i$\n\n2.  $y_i = \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i1}x_{i2} + \\epsilon_i$\n\n3.  $y_i = \\beta_0  + \\beta_1\\sin(x_{i1} + \\beta_2x_{i2}) + \\beta_3x_{i3} + \\epsilon_i$\n\n4.  $y_i = \\beta_0 + \\beta_1e^{x_{i1}} + \\beta_2e^{x_{i2}} + \\epsilon_i$\n\n5.  $y_i = \\exp(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3}) + \\epsilon_i$\n:::\n\n## Recap\n\n-   Computed and interpreted confidence interval for a single coefficient\n\n-   Showed some properties of $\\hat{\\boldsymbol{\\beta}}$\n\n-   Defined \"linear\" model\n",
    "supporting": [
      "10-prop-of-estimators_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}