{
  "hash": "9684c37d3c589caf4e03494a1a6d714a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Properties of estimators\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-10-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n\n\n## Announcements\n\n-   Project Proposal due Thursday, October 3 at 11:59pm\n\n-   Lab 03 due Thursday, October 3 at 11:59pm\n\n-   HW 02 due Thursday, October 3 at 11:59pm (released after class)\n\n-   Exam 01: Tuesday, October 8 (in class + take-home)\n\n    -   Lecture recordings available until the start of the in-class exam\n\n    -   Exam review on Thursday\n\n    -   Monday's lab: Exam office hours\n\n## Topics\n\n-   Properties of the least squares estimator\n\n::: callout-note\nThis is not a mathematical statistics class. There are semester-long courses that will go into these topics in much more detail; we will barely scratch the surface in this course.\n\nOur goals are to understand\n\n-   Estimators have properties\n\n-   Some properties of the least squares estimator and why they are useful\n:::\n\n# Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n## Motivation {.midi}\n\n::: incremental\n-   We have discussed how to use least squares to find an estimator of $\\hat{\\boldsymbol{\\beta}}$\n\n-   How do we know whether our least squares estimator is a \"good\" estimator?\n\n-   When we consider what makes an estimator \"good\", we'll look at three criteria:\n\n    -   Bias\n    -   Variance\n    -   Mean squared error\n\n-   We'll take a look at these over the course of a few lectures and motivate why we might prefer using least squares to compute $\\hat{\\boldsymbol{\\beta}}$ versus other methods\n:::\n\n## Bias and variance\n\nSuppose you are throwing darts at a target\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\n![Image source: [Analytics Vidhya](https://medium.com/analytics-vidhya/bias-variance-tradeoff-regularization-5543d2d1ad8a)](images/10/bias-variance.webp)\n:::\n\n::: {.column width=\"50%\"}\n-   **Ideal scenario**: Darts are clustered around the target (unbiased and low variance)\n\n-   **Ideal scenario**: Darts are clustered around the target (unbiased and low variance)\n\n-   **Acceptable scenario:** There's some trade-off between the bias and variance.\n:::\n:::\n\n## Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator\n\n-   Best Linear Unbiased Estimator (BLUE)\n\n<br>\n\n**Infinite sample (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator\n\n-   Efficient estimator\n\n# Finite sample properties\n\n## Unbiased estimator\n\nThe **bias** of an estimator is the difference between the estimator's expected value and the true value of the parameter\n\nLet $\\hat{\\theta}$ be an estimator of the parameter $\\theta$. Then\n\n$$\nBias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n$$\n\nAn estimator is **unbiased** if the bias is 0 and thus $E(\\hat{\\theta}) = \\theta$\n\n## Unbiased estimator\n\nGiven $E(\\boldsymbol{\\epsilon}) = \\mathbf{0}$,\n\n$$\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}] \\\\[8pt]\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon})] \\\\[8pt]\n& = E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}] + E[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}]\\\\[8pt]\n& = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^TE(\\boldsymbol{\\epsilon}) \\\\[8pt]\n & = \\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\nThe least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ is an unbiased estimator of $\\boldsymbol{\\beta}$\n\n## Variance of $\\hat{\\boldsymbol{\\beta}}$\n\n$$\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= Var((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}) \\\\[8pt]\n& = [(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]Var(\\mathbf{y})[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]^T \\\\[8pt]\n& = [(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T]\\sigma^2_{\\epsilon}\\mathbf{I}[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\[8pt]\n& = \\sigma^2_{\\epsilon}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\[8pt]\n& = \\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1}\n\\end{aligned}\n$$\n\n<!--# add link to math rule-->\n\n## \"Linear\" regression model\n\nWhat does it mean for a model to be a \"linear\" regression model?\n\n-   Linear regression models are linear in the parameters, i.e. given an observation $y_i$\n\n    $$\n    y_i = \\beta_0 + \\beta_1f_1(x_{i1}) +  \\dots + \\beta_pf_p(x_{ip}) + \\epsilon_i\n    $$\n\n-   The functions $f_1, \\ldots, f_p$ can be non-linear as long as $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are linear in $Y$\n\n---\n\n<br>\n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n## **Gauss-Markov Theorem**\n\nThe least-squares estimator of $\\boldsymbol{\\beta}$ in the model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ is given by $\\hat{\\boldsymbol{\\beta}}$. Given the errors have mean $\\mathbf{0}$ and variance $\\sigma^2_{\\epsilon}\\mathbf{I}$ , then $\\hat{\\boldsymbol{\\beta}}$ is **BLUE (best linear unbiased estimator)**.\n\n\"Best\" means $\\hat{\\boldsymbol{\\beta}}$ has the smallest variance among all linear unbiased estimators for $\\boldsymbol{\\beta}$ .\n:::\n\n## Gauss-Markov Theorem Proof \n\nSuppose $\\tilde{\\boldsymbol{\\beta}}$ is another linear unbiased estimator of $\\boldsymbol{\\beta}$ that can be expressed as $\\tilde{\\boldsymbol{\\beta}} = \\mathbf{Cy}$ , such that $\\hat{\\mathbf{y}} = \\mathbf{X}\\tilde{\\boldsymbol{\\beta}} = \\mathbf{XCy}$\n\n<br>\n\nLet $\\mathbf{C} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B}$ for a non-zero matrix of constants $\\mathbf{B}$.\n\n<br>\n\n::: question\nWhat is the dimension of $\\mathbf{B}$?\n:::\n\n## Gauss-Markov Theorem Proof \n\n$$\n\\tilde{\\boldsymbol{\\beta}} = \\mathbf{Cy} = ((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})\\mathbf{y}\n$$\n\nWe need to show\n\n-   $\\tilde{\\boldsymbol{\\beta}}$ is unbiased\n\n-   $Var(\\tilde{\\boldsymbol{\\beta}}) > Var(\\hat{\\boldsymbol{\\beta}})$\n\n## Gauss-Markov Theorem Proof \n\n$$\n\\begin{aligned}\nE(\\tilde{\\boldsymbol{\\beta}}) & = E[((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})\\mathbf{y}] \\\\\n& = E[((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})] \\\\\n& = E[((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta})] \\\\\n& = ((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta}) \\\\\n& = (\\mathbf{I} + \\mathbf{BX})\\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\n::: question\n-   What assumption(s) of the Gauss-Markov Theorem did we use?\n\n-   What must be true for $\\tilde{\\boldsymbol{\\beta}}$ to be unbiased?\n:::\n\n## Gauss-Markov Theorem Proof \n\n-   $\\mathbf{BX}$ must be the $\\mathbf{0}$ matrix (dimensions = $(p+1) \\times (p+1)$) in order for $\\tilde{\\boldsymbol{\\beta}}$ to be unbiased\n\n-   Now we need to find $Var(\\tilde{\\boldsymbol{\\beta}})$ and see how it compares to $Var(\\hat{\\boldsymbol{\\beta}})$\n\n## Gauss-Markov Theorem Proof  {.midi}\n\n$$\n\\begin{aligned}\nVar(\\tilde{\\boldsymbol{\\beta}}) &= Var[((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})\\mathbf{y}] \\\\\n& = ((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})Var(\\mathbf{y})((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T + \\mathbf{B})^T \\\\\n& = \\sigma^2_{\\epsilon}[(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{B}^T + \\mathbf{BX}(\\mathbf{X}^T\\mathbf{X})^{-1} + \\mathbf{BB}^T]\\\\\n& = \\sigma^2_\\epsilon(\\mathbf{X}^T\\mathbf{X})^{-1} + \\sigma^2_{\\epsilon}\\mathbf{BB}^T\\end{aligned}\n$$\n\n::: question\nWhat assumption(s) of the Gauss-Markov Theorem did we use?\n:::\n\n<!--# add math rule-->\n\n## Gauss-Markov Theorem Proof \n\nWe know that $\\mathbf{B}\\mathbf{B}^T$ transpose is symmetric. Let $\\mathbf{z}$ be a non-zero vector. Then,\n\n$$\n\\mathbf{z}^T\\mathbf{BB}^T\\mathbf{z} = (\\mathbf{B}^T\\mathbf{z})^T(\\mathbf{B}^T\\mathbf{z}) = ||\\mathbf{B}^T\\mathbf{z}||^2 \\geq 0\n$$\n\n::: question\nWhen is $||\\mathbf{B}^T\\mathbf{z}|| = 0$ ?\n:::\n\n. . .\n\nTherefore, we have shown that $Var(\\tilde{\\boldsymbol{\\beta}}) > Var(\\hat{\\boldsymbol{\\beta}})$ and have completed the proof.\n\n---\n\n<br>\n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n## **Gauss-Markov Theorem**\n\nThe least-squares estimator of $\\boldsymbol{\\beta}$ in the model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ is given by $\\hat{\\boldsymbol{\\beta}}$. Given the errors have mean $\\mathbf{0}$ and variance $\\sigma^2_{\\epsilon}\\mathbf{I}$ , then $\\hat{\\boldsymbol{\\beta}}$ is **BLUE (best linear unbiased estimator)**.\n\n\"Best\" means $\\hat{\\boldsymbol{\\beta}}$ has the smallest variance among all linear unbiased estimators for $\\boldsymbol{\\beta}$ .\n:::\n\n## Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator âœ…\n\n-   Best Linear Unbiased Estimator (BLUE) âœ…\n\n<br>\n\n**Infinite sample (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator\n\n-   Efficient estimator\n\n# Infinite sample properties\n\n## Mean squared error\n\nThe **mean squared error (MSE)** is the squared difference between the estimator and parameter.\n\n. . .\n\nLet $\\hat{\\theta}$ be an estimator of the parameter $\\theta$. Then\n\n$$\n\\begin{aligned}\nMSE(\\hat{\\theta}) &= E[(\\hat{\\theta} - \\theta)^2] \\\\\n& = E(\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta + \\theta^2) \\\\\n& = E(\\hat{\\theta}^2) - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n& = \\underbrace{E(\\hat{\\theta}^2) -  E(\\hat{\\theta})^2}_{Var(\\hat{\\theta})} + \\underbrace{E(\\hat{\\theta})^2 - 2\\theta E(\\hat{\\theta}) + \\theta^2}_{Bias(\\theta)^2}\n\\end{aligned}\n$$\n\n. . .\n\n## Mean squared error\n\n$$\nMSE(\\theta) = Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2\n$$\n\n<br>\n\n. . .\n\nThe least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ is unbiased, so $MSE(\\hat{\\boldsymbol{\\beta}}) = Var(\\hat{\\boldsymbol{\\beta}})$\n\n## Consistency \n\nAn estimator $\\hat{\\theta}$ is a consistent estimator of a parameter $\\theta$ if it converges in probability to $\\theta$. Given a sequence of estimators $\\hat{\\theta}_1, \\hat{\\theta}_2, . . .$, then for every $\\epsilon > 0$,\n\n$$\n\\displaystyle \\lim_{n\\to\\infty} P(|\\hat{\\theta}_n - \\theta| \\geq \\epsilon) = 0\n$$\n\n. . .\n\nThis means that as the sample size goes to $\\infty$ (and thus the sample information gets better and better), the estimator will be arbitrarily close to the parameter with high probability.\n\n<!--# casella berger pg. 468-->\n\n::: question\nWhy is this a useful property of an estimator?\n:::\n\n## Consistency \n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n**Theorem**\n\nAn estimator $\\hat{\\theta}$ is a consistent estimator of the parameter $\\theta$ if the sequence of estimators $\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots$ satisfies\n\n-   $\\lim_{n \\to \\infty} Var(\\hat{\\theta}) = 0$\n\n-   $\\lim_{n \\to \\infty} Bias(\\hat{\\theta}) = 0$\n:::\n\n## Consistency of $\\hat{\\boldsymbol{\\beta}}$ \n\n$Bias(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$, so $\\lim_{n \\to \\infty} Bias(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$\n\n<br>\n\n. . .\n\nNow we need to show that $\\lim_{n \\to \\infty} Var(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$\n\n::: question\n-   What is $Var(\\hat{\\boldsymbol{\\beta}})$?\n\n-   Does $Var(\\hat{\\boldsymbol{\\beta}}) \\to \\mathbf{0}$ as $n \\to \\infty$?\n:::\n\n## Efficiency \n\n-   The **efficiency** of an estimator is concerned with the asymptotic variance of an estimator.\n\n-   The estimator with the smallest variance is considered the most efficient.\n\n-   By the Gauss-Markov Theorem, we have shown that the least-squares estimator is the most efficient among linear unbiased estimators.\n\n## Recap\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator âœ…\n\n-   Best Linear Unbiased Estimator (BLUE) âœ…\n\n<br>\n\n**Infinite sample (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator âœ…\n\n-   Efficient estimator âœ…\n",
    "supporting": [
      "11-prop-of-estimators-pt2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}