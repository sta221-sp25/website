{
  "hash": "86daa0e7b4ff2e04b423fa5a1b3c188a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Properties of estimators\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2025-03-20\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 17-prop-of-estimators-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   HW 03 due TODAY at 11:59pm\n\n-   Project exploratory data analysis due TODAY at 11:59pm\n\n-   Statistics experience due April 22\n\n## Topics\n\n-   Properties of the least squares estimator\n\n::: callout-note\nThis is not a mathematical statistics class. There are semester-long courses that will go into these topics in much more detail; we will barely scratch the surface in this course.\n\nOur goals are to understand\n\n-   Estimators have properties\n\n-   A few properties of the least squares estimator and why they are useful\n:::\n\n\n# Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n## Motivation \n\n::: incremental\n-   We have discussed how to use least squares to find an estimator of $\\hat{\\boldsymbol{\\beta}}$\n\n-   How do we know whether our least squares estimator (or MLE)  is a \"good\" estimator?\n\n-   When we consider what makes an estimator \"good\", we'll look at three criteria:\n\n    -   Bias\n    -   Variance\n    -   Mean squared error\n:::\n\n## Bias and variance\n\nSuppose you are throwing darts at a target\n\n. . .\n\n::::: columns\n::: {.column width=\"50%\"}\n![Image source: [Analytics Vidhya](https://medium.com/analytics-vidhya/bias-variance-tradeoff-regularization-5543d2d1ad8a)](images/10/bias-variance.webp)\n:::\n\n::: {.column width=\"50%\"}\n-   **Unbiased**: Darts distributed around the target\n\n-   **Biased**: Darts systematically away from the target\n\n-   **Variance**: Darts could be widely spread (high variance) or generally clustered together (low variance)\n:::\n:::::\n\n## Bias and variance\n\n-   **Ideal scenario**: Darts are clustered around the target (unbiased and low variance)\n\n-   **Worst case scenario**: Darts are widely spread out and systematically far from the target (high bias and high variance)\n\n-   **Acceptable scenario:** There's some trade-off between the bias and variance. For example, it may be acceptable for the darts to be clustered around a point that is close to the target (low bias and low variance)\n\n## Bias and variance\n\n::: incremental\n-   Each time we take a sample of size $n$, we can find the least squares estimator (throw dart at target)\n\n-   Suppose we take many independent samples of size $n$ and find the least squares estimator for each sample (throw many darts at the target). Ideally,\n\n    -   The estimators are centered at the true parameter (unbiased)\n\n    -   The estimators are clustered around the true parameter (unbiased with low variance)\n:::\n\n## Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator\n\n-   Best Linear Unbiased Estimator (BLUE)\n\n<br>\n\n**Asymptotic (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator\n\n-   Efficient estimator\n\n- Asymptotic normality \n\n\n# Finite sample properties\n\n## Unbiased estimator\n\nThe **bias** of an estimator is the difference between the estimator's expected value and the true value of the parameter\n\nLet $\\hat{\\theta}$ be an estimator of the parameter $\\theta$. Then\n\n$$\nBias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\n$$\n\nAn estimator is **unbiased** if the bias is 0 and thus $E(\\hat{\\theta}) = \\theta$\n\n\n## Expected value of $\\hat{\\boldsymbol{\\beta}}$\n\nLet's take a look at the expected value of least-squares estimator:\n\n$$\n\\begin{aligned}\nE(\\hat{\\boldsymbol{\\beta}}) &= E[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}] \\\\[8pt]\n& = \\class{fragment}{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}E[\\mathbf{y}]} \\\\[8pt]\n& = \\class{fragment}{(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}}\\\\[8pt]\n & = \\class{fragment}{\\boldsymbol{\\beta}}\n\\end{aligned}\n$$\n\n\n## Expected value of $\\hat{\\boldsymbol{\\beta}}$\n\nThe least squares estimator $\\hat{\\boldsymbol{\\beta}}$ is an *unbiased* estimator of $\\boldsymbol{\\beta}$\n\n$$\nE(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\n$$\n\n\n\n## Variance of $\\hat{\\boldsymbol{\\beta}}$\n\n$$\n\\begin{aligned}\nVar(\\hat{\\boldsymbol{\\beta}}) &= Var((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}) \\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}]Var(\\mathbf{y})[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}]^\\mathsf{T} }\\\\[8pt]\n& = \\class{fragment}{[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}]\\sigma^2_{\\epsilon}\\mathbf{I}[\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}]} \\\\[8pt]\n& = \\class{fragment}{\\sigma^2_{\\epsilon}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}}\n\\end{aligned}\n$$\n\n. . .\n\nWe will show that $\\hat{\\boldsymbol{\\beta}}$ is the \"best\" estimator (has the lowest variance) among the class of linear unbiased estimators\n\n\n\n<br>\n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n## **Gauss-Markov Theorem**\n\nThe least-squares estimator of $\\boldsymbol{\\beta}$ in the model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ is given by $\\hat{\\boldsymbol{\\beta}}$. Given the errors have mean $\\mathbf{0}$ and variance $\\sigma^2_{\\epsilon}\\mathbf{I}$ , then $\\hat{\\boldsymbol{\\beta}}$ is **BLUE (best linear unbiased estimator)**.\n\n\"Best\" means $\\hat{\\boldsymbol{\\beta}}$ has the smallest variance among all linear unbiased estimators for $\\boldsymbol{\\beta}$ .\n:::\n\n## Gauss-Markov Theorem Proof\n\nSuppose $\\tilde{\\boldsymbol{\\beta}}$ is another linear unbiased estimator of $\\boldsymbol{\\beta}$ that can be expressed as $\\tilde{\\boldsymbol{\\beta}} = \\mathbf{Cy}$ , such that $\\hat{\\mathbf{y}} = \\mathbf{X}\\tilde{\\boldsymbol{\\beta}} = \\mathbf{XCy}$\n\n<br>\n\nLet $\\mathbf{C} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B}$ for a non-zero matrix $\\mathbf{B}$.\n\n<br>\n\n::: question\nWhat is the dimension of $\\mathbf{B}$?\n:::\n\n## Gauss-Markov Theorem Proof\n\n$$\n\\tilde{\\boldsymbol{\\beta}} = \\mathbf{Cy} = ((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})\\mathbf{y}\n$$\n\nWe need to show\n\n-   $\\tilde{\\boldsymbol{\\beta}}$ is unbiased\n\n-   $Var(\\tilde{\\boldsymbol{\\beta}}) > Var(\\hat{\\boldsymbol{\\beta}})$\n\n## Gauss-Markov Theorem Proof\n\n$$\n\\begin{aligned}\nE(\\tilde{\\boldsymbol{\\beta}}) & = E[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})\\mathbf{y}] \\\\\n& = E[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})] \\\\\n& = E[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta})] \\\\\n& = ((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})(\\mathbf{X}\\boldsymbol{\\beta}) \\\\\n& = (\\mathbf{I} + \\mathbf{BX})\\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\n::: question\n-   What assumption(s) of the Gauss-Markov Theorem did we use?\n\n-   What must be true for $\\tilde{\\boldsymbol{\\beta}}$ to be unbiased?\n:::\n\n## Gauss-Markov Theorem Proof\n\n-   $\\mathbf{BX}$ must be the $\\mathbf{0}$ matrix (dimension = $(p+1) \\times (p+1)$) in order for $\\tilde{\\boldsymbol{\\beta}}$ to be unbiased\n\n-   Now we need to find $Var(\\tilde{\\boldsymbol{\\beta}})$ and see how it compares to $Var(\\hat{\\boldsymbol{\\beta}})$\n\n## Gauss-Markov Theorem Proof {.midi}\n\n$$\n\\begin{aligned}\nVar(\\tilde{\\boldsymbol{\\beta}}) &= Var[((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})\\mathbf{y}] \\\\[8pt]\n& = ((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})Var(\\mathbf{y})((\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} + \\mathbf{B})^\\mathsf{T} \\\\[8pt]\n& = \\small{\\sigma^2_{\\epsilon}[(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} + (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T} \\mathbf{B}^\\mathsf{T} + \\mathbf{BX}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} + \\mathbf{BB}^\\mathsf{T}]}\\\\[8pt]\n& = \\sigma^2_\\epsilon(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} + \\sigma^2_{\\epsilon}\\mathbf{BB}^\\mathsf{T}\\end{aligned}\n$$\n\n::: question\nWhat assumption(s) of the Gauss-Markov Theorem did we use?\n:::\n\n<!--# add math rule-->\n\n## Gauss-Markov Theorem Proof\n\nWe have\n\n$$\nVar(\\tilde{\\boldsymbol{\\beta}}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} + \\sigma^2_\\epsilon \\mathbf{BB}^\\mathsf{T}\n$$\n\n. . .\n\nWe know that $\\sigma^2_{\\epsilon}\\mathbf{BB}^\\mathsf{T} \\geq \\mathbf{0}$.\n\n<br>\n\n. . .\n\n::: question\nWhen is $\\sigma^2_{\\epsilon}\\mathbf{BB}^\\mathsf{T} = \\mathbf{0}$?\n:::\n\n. . .\n\nTherefore, we have shown that $Var(\\tilde{\\boldsymbol{\\beta}}) > Var(\\hat{\\boldsymbol{\\beta}})$ and have completed the proof.\n\n------------------------------------------------------------------------\n\n<br>\n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n## **Gauss-Markov Theorem**\n\nThe least-squares estimator of $\\boldsymbol{\\beta}$ in the model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ is given by $\\hat{\\boldsymbol{\\beta}}$. Given the errors have mean $\\mathbf{0}$ and variance $\\sigma^2_{\\epsilon}\\mathbf{I}$ , then $\\hat{\\boldsymbol{\\beta}}$ is **BLUE (best linear unbiased estimator)**.\n\n\"Best\" means $\\hat{\\boldsymbol{\\beta}}$ has the smallest variance among all linear unbiased estimators for $\\boldsymbol{\\beta}$ .\n:::\n\n\n## Properties of $\\hat{\\boldsymbol{\\beta}}$\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator âœ…\n\n-   Best Linear Unbiased Estimator (BLUE) âœ…\n\n<br>\n\n**Asymptotic (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator\n\n-   Efficient estimator\n\n- Asymptotic normality \n\n# Asymptotic properties\n\n## Properties from the MLE\n\n- Recall that the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ is equal to the Maximum Likelihood Estimator $\\tilde{\\boldsymbol{\\beta}}$ \n\n- Maximum likelihood estimators have nice statistical properties and the $\\hat{\\boldsymbol{\\beta}}$ inherits all of these properties \n\n- Consistency\n\n- Efficiency \n\n- Asymptotic normality \n\n::: callout-note\n\nWe will define the properties here, and you will explore them in much more depth in STA 332: Statistical Inference\n\n:::\n\n## Mean squared error\n\nThe **mean squared error (MSE)** is the squared difference between the estimator and parameter.\n\n. . .\n\nLet $\\hat{\\theta}$ be an estimator of the parameter $\\theta$. Then\n\n$$\n\\begin{aligned}\nMSE(\\hat{\\theta}) &= E[(\\hat{\\theta} - \\theta)^2] \\\\\n& = E(\\hat{\\theta}^2 - 2\\hat{\\theta}\\theta + \\theta^2) \\\\\n& = E(\\hat{\\theta}^2) - 2\\theta E(\\hat{\\theta}) + \\theta^2 \\\\\n& = \\underbrace{E(\\hat{\\theta}^2) -  E(\\hat{\\theta})^2}_{Var(\\hat{\\theta})} + \\underbrace{E(\\hat{\\theta})^2 - 2\\theta E(\\hat{\\theta}) + \\theta^2}_{Bias(\\theta)^2}\n\\end{aligned}\n$$\n\n. . .\n\n## Mean squared error\n\n$$\nMSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + Bias(\\hat{\\theta})^2\n$$\n\n<br>\n\n. . .\n\nThe least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ is unbiased, so $$MSE(\\hat{\\boldsymbol{\\beta}}) = Var(\\hat{\\boldsymbol{\\beta}})$$\n\n## Consistency\n\nAn estimator $\\hat{\\theta}$ is a consistent estimator of a parameter $\\theta$ if it converges in probability to $\\theta$. Given a sequence of estimators $\\hat{\\theta}_1, \\hat{\\theta}_2, . . .$, then for every $\\epsilon > 0$,\n\n$$\n\\displaystyle \\lim_{n\\to\\infty} P(|\\hat{\\theta}_n - \\theta| \\geq \\epsilon) = 0\n$$\n\n. . .\n\nThis means that as the sample size goes to $\\infty$ (and thus the sample information gets better and better), the estimator will be arbitrarily close to the parameter with high probability.\n\n<!--# casella berger pg. 468-->\n\n::: question\nWhy is this a useful property of an estimator?\n:::\n\n## Consistency\n\n<br>\n\n<br>\n\n::: {.callout-important icon=\"false\"}\n**Theorem**\n\nAn estimator $\\hat{\\theta}$ is a consistent estimator of the parameter $\\theta$ if the sequence of estimators $\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots$ satisfies\n\n-   $\\lim_{n \\to \\infty} Var(\\hat{\\theta}) = 0$\n\n-   $\\lim_{n \\to \\infty} Bias(\\hat{\\theta}) = 0$\n:::\n\n## Consistency of $\\hat{\\boldsymbol{\\beta}}$\n\n$Bias(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$, so $\\lim_{n \\to \\infty} Bias(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$\n\n<br>\n\n. . .\n\nNow we need to show that $\\lim_{n \\to \\infty} Var(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$\n\n::: question\n-   What is $Var(\\hat{\\boldsymbol{\\beta}})$?\n\n-   Show $Var(\\hat{\\boldsymbol{\\beta}}) \\to \\mathbf{0}$ as $n \\to \\infty$.\n:::\n\n. . . \n\nTherefore $\\hat{\\boldsymbol{\\beta}}$ is a consistent estimator. \n\n## Efficiency\n\n::: incremental\n\n-   An estimator if **efficient** if it has the smallest variance among a class of estimators as $n \\rightarrow \\infty$ \n\n-   By the Gauss-Markov Theorem, we have shown that the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ is the most efficient among linear unbiased estimators.\n\n- Maximum Likelihood Estimators are the most efficient among all unbiased estimators. \n\n- Therefore, $\\hat{\\boldsymbol{\\beta}}$ is the most efficient among all unbiased estimators of $\\boldsymbol{\\beta}$\n\n:::\n\n## Asymptotic normality \n\n- Maximum Likelihood Estimators are **asymptotically normal**, meaning the distribution of an MLE is normal as $n \\rightarrow \\infty$ \n\n- Therefore, we know the distribution of $\\hat{\\boldsymbol{\\beta}}$ is normal when $n$ is large, regardless of the underlying data\n\n## Recap\n\n**Finite sample (** $n$ **)** **properties**\n\n-   Unbiased estimator âœ…\n\n-   Best Linear Unbiased Estimator (BLUE) âœ…\n\n<br>\n\n**Asymptotic (** $n \\rightarrow \\infty$ **) properties**\n\n-   Consistent estimator âœ…\n\n-   Efficient estimator âœ…\n\n- Asymptotic normality âœ…",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}