{
  "hash": "94e0ad917436facfbe7f9a045ff48c7d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum likelihood estimation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-10-10\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[üîó STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n## Announcements\n\n-   Office hours:\n\n    -   This week: Thursday - Friday\n\n    -   Next week: Wednesday - Friday\n\n-   No class next Monday or Tuesday\n\n<br>\n\n<center> üçÅ Have a good Fall Break! üçÅ </center>\n\n## Topics\n\n-   Likelihood\n\n-   Maximum likelihood estimation\n\n-   MLE for linear regression\n\n-   Properties of maximum likelihood estimator\n\n## Motivation  {.midi}\n\n::: incremental\n-   We can find the estimators of $\\boldsymbol{\\beta}$ and $\\sigma^2_{\\epsilon}$ for the model\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{10mm} \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2_\\epsilon\\mathbf{I})\n$$using least-squares estimation\n\n-   We have also shown some nice properties of the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$, given $E(\\boldsymbol{\\epsilon}) = \\mathbf{0}$ and $Var(\\boldsymbol{\\epsilon}) = \\sigma^2_{\\epsilon}\\mathbf{I}$\n\n-   Today we will introduce another way to find these estimators - **maximum likelihood estimation.** We will see...\n\n    -   the maximum likelihood estimators have nice properties\n\n    -   the least-squares estimator is equal to the maximum likelihood estimator when certain assumptions hold\n:::\n\n# Maximum likelihood estimation\n\n## Example: Shooting free throws\n\nSuppose a basketball player shoots a single free throw, such that the probability of making a basket is $p$\n\n. . .\n\n::: incremental\n-   What is the probability distribution for this random phenomenon?\n\n-   Suppose the probability is $p = 0.5$? What is the probability the player makes a single shot, given this value of $p$?\n\n-   Suppose the probability is $p = 0.8$? What is the probability the player makes a single shot, given this value of $p$?\n:::\n\n## Shooting three free throws\n\nSuppose the player shoots three free throws. They are all independent and the player has the same probability $p$ of making each shot.\n\nLet $B$ represent a made basket, and $M$ represent a missed basket. The player shoots three free throws with the outcome $BBM$.\n\n. . .\n\n::: incremental\n-   Suppose the probability is $p = 0.5$? What is the probability of observing the data $BBM$, given this value of $p$?\n\n-   Suppose the probability is $p = 0.3$? What is the probability of observing the data $BBM$, given this value of $p$ ?\n:::\n\n## Shooting three free throws\n\nSuppose the player shoots three free throws. They are all independent and the player has the same probability $p$ of making each shot.\n\nThe player shoots three free throws with the outcome $BBM$.\n\n. . .\n\n::: incremental\n-   How would you describe in words the probabilities we previously calculated?\n\n-   **New question:** What parameter value of $p$ do you think maximizes the\n    probability of observing this data?\n\n-   We will use a **likelihood** function to answer this question.\n:::\n\n## Likelihood\n\n::: incremental\n-   A **likelihood** is a function that tells us how likely we are to observe our data for a given parameter value (or values). <!--# Find a better definition - Casella Berger maybe?-->\n\n-   Note that this is **not** the same as the probability function.\n\n-   **Probability function**: Fixed parameter value(s) + input possible outcomes $\\Rightarrow$ probability of seeing the different outcomes given the parameter value(s)\n\n-   **Likelihood function**: Fixed data + input possible parameter values $\\Rightarrow$ probability of seeing the fixed data for each parameter value\n:::\n\n## Likelihood: shooting three free throws\n\nThe likelihood function for the probability of a basket $p$ given we observed $BBM$ when shooting three independent free throws is $$\nL(p|BBM) = p \\times p \\times (1 - p)\n$$\n\n<br>\n\n. . .\n\nThus, if the likelihood for $p = 0.8$ is\n\n$$\nL(p = 0.8|BBM) = 0.8 \\times 0.8 \\times (1 - 0.8) = 0.128\n$$\n\n## Likelihood: shooting three free throws\n\n-   What is the general formula for the likelihood function for $p$ given the observed data $BBM$?\n\n-   Why do we need to assume independence?\n\n-   Why does having identically distributed data simplify things?\n\n## Likelihood: shooting three free throws {.midi}\n\nThe likelihood function for $p$ given the data $BBM$ is\n\n$$\nL(p|BBM) = p \\times p \\times (1 - p) = p^2 \\times (1 - p)\n$$\n\n. . .\n\n::: incremental\n-   We want of the value of $p$ that maximizes this likelihood function, i.e., the value of $p$ that is most likely given the observed data.\n\n-   The process of finding this value is **maximum likelihood estimation**.\n\n-   There are three primary ways to find the maximum likelihood estimator\n\n    -   Approximate using a graph\n\n    -   Using calculus\n\n    -   Numerical approximation\n:::\n\n## Finding the MLE using graphs\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13-mle_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n::: question\nWhat do you think is the approximate value of the MLE of $p$ given the data?\n:::\n\n## Finding the MLE using calculus\n\n-   Find the MLE using the first derivative of the likelihood function.\n\n\n\n```{=html}\n<!-- -->\n```\n\n\n-   This can be tricky because of the Product Rule, so we can maximize the **log(Likelihood)** instead. The same value maximizes the likelihood and log(Likelihood).\n\n::: question\nUse calculus to find the MLE of $p$ given the data $BBM$.\n:::\n\n## Shooting $n$ free throws\n\nSuppose the player shoots $n$ free throws. They are all independent and the player has the same probability $p$ of making each shot.\n\nSuppose the player makes $k$ baskets out of the $n$ free throws. This is the observe data.\n\n. . .\n\n::: incremental\n-   What is the formula for the probability distribution to describe this random phenomenon?\n\n\n\n```{=html}\n<!-- -->\n```\n\n\n-   What is the formula for the likelihood function for $p$ given the observed data?\n\n-   For what value of $p$ do we maximize the likelihood given the observed data? Use calculus to find the response.\n:::\n\n## Why maximum likelihood estimation?  {.midi}\n\n-   *\"Maximum likelihood estimation is, by far, the most popular technique for deriving estimators.\"* [@casella2024statistical, pp. 315]\n\n-   MLEs have nice statistical properties. They are\n\n    -   Consistent\n\n    -   Efficient - Have the smallest MSE among all consistent estimators\n\n    -   Asymptotically normal\n\n. . .\n\n::: callout-note\nIf the normality assumption holds, the least squares estimator is the maximum likelihood estimator for $\\beta$. Therefore, it has all these properties of the MLE.\n:::\n\n# MLE in linear regression {.midi}\n\n## Linear regression\n\nRecall the linear model\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{10mm} \\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2_{\\epsilon}\\mathbf{I}) \n$$\n\n. . .\n\n::: incremental\n-   We have discussed least-squares estimation to find $\\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\sigma}_\\epsilon^2$\n-   We have discussed properties of $\\hat{\\boldsymbol{\\beta}}$ that depend on $E(\\boldsymbol{\\epsilon}) = \\mathbf{0}$ and $Var(\\boldsymbol{\\epsilon}) = \\sigma^2_{\\epsilon}\\mathbf{I}$\n-   We have used the fact that $\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\sigma^2_{\\epsilon}(\\mathbf{X}^T\\mathbf{X})^{-1})$ when doing hypothesis testing and confidence intervals.\n-   Now we will discuss how we know $\\hat{\\boldsymbol{\\beta}}$ is normally distributed, as we introduce MLE for linear regression\n:::\n\n## Simple linear regression model \n\nSuppose we have the simple linear regression (SLR) model\n\n$$\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\hspace{10mm} \\epsilon_i \\sim N(0, \\sigma^2_{\\epsilon})\n$$\n\nsuch that $\\epsilon_i$ are independently and identically distributed.\n\n<br>\n\n. . .\n\nWe can write this model in the form below and use this to find the MLE\n\n$$\ny_i | x_i \\sim N(\\beta_0 + \\beta x_i, \\sigma^2_{\\epsilon})\n$$\n\n## Side note: Normal distribution \n\nLet $X$ be a random variable, such that $X \\sim N(\\mu, \\sigma^2)$. Then the probability function is\n\n$$\nP(X = x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big\\{-{\\frac{1}{2\\sigma^2}(x - \\mu)^2}\\Big\\}\n$$\n\n## Likelihood for SLR\n\nThe likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\nL&(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} | x_i, \\dots, x_n, y_i, \\dots, y_n) \\\\[5pt]\n &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_\n\\epsilon^2}}\\exp\\Big\\{{-\\frac{1}{2\\sigma_\\epsilon^2}(y_i - [\\beta_0 + \\beta_1x_i])^2}\\Big\\} \\\\[10pt]\n& = (2\\pi\\sigma^2_{\\epsilon})^{-\\frac{n}{2}}\\exp\\Big\\{-\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\Big\\}\n\\end{aligned}\n$$\n\n## Log-likelihood for SLR\n\nThe log-likelihood function for $\\beta_0, \\beta_1, \\sigma^2_{\\epsilon}$ is\n\n$$\n\\begin{aligned}\n\\log &L(\\beta_0, \\beta_1, \\sigma^2_{\\epsilon} | x_i, \\dots, x_n, y_i, \\dots, y_n) \n  \\\\[8pt]\n& = -\\frac{n}{2}\\log(2\\pi\\sigma^2_{\\epsilon}) -\\frac{1}{2\\sigma^2_{\\epsilon}}\\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\n\\end{aligned}\n$$\n\n<br>\n\n. . .\n\nWe will use the log-likelihood function to find the MLEs\n\n## MLE for $\\beta_0$ \n\n1Ô∏è‚É£ Take derivative of $\\log L$ with respect to $\\beta_0$ and set it equal to 0\n\n$$\n\\frac{\\partial \\log L}{\\partial \\beta_0} = -\\frac{1}{2\\sigma^2_\\epsilon}\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_i)(-1) = 0\n$$\n\n## MLE for $\\beta_0$  {.midi}\n\n2Ô∏è‚É£ Find the $\\tilde{\\beta}_0$ that satisfies the equality on the previous slide\n\n. . .\n\nAfter a few steps...\n\n$$\n\\begin{aligned}\n&\\Rightarrow \\sum_{i=1}^ny_i - n\\tilde{\\beta}_0 - \\tilde{\\beta}_1\\sum_{i=1}^n x_i = 0 \\\\\n&\\Rightarrow \\sum_{i=1}^ny_i  - \\tilde{\\beta}_1\\sum_{i=1}^n x_i = n\\tilde{\\beta}_0 \\\\\n&\\Rightarrow \\frac{1}{n}\\sum_{i=1}^ny_i  - \\frac{1}{n}\\tilde{\\beta}_1\\sum_{i=1}^n x_i = \\tilde{\\beta}_0\n\\end{aligned}\n$$\n\n## MLE for $\\beta_0$  {.midi}\n\n3Ô∏è‚É£ We can use the second derivative to show we've found the maximum\n\n$$\n\\frac{\\partial^2 \\log L}{\\partial \\beta_0^2} = -\\frac{n}{2\\tilde{\\sigma}^2_\\epsilon}  < 0\n$$\n\n<br>\n\n. . .\n\nTherefore, we have found the maximum. Thus, MLE for $\\beta_0$ is\n\n$$\n\\tilde{\\beta}_0 = \\bar{y} - \\tilde{\\beta}_1\\bar{x}\n$$\n\n$$$$\n\n## MLE for $\\beta_1$ and $\\sigma^2_{\\epsilon}$ \n\nWe can use a similar process to find the MLEs for $\\beta_1$ and $\\sigma^2_{\\epsilon}$\n\n$$\n\\tilde{\\beta}_1 = \\frac{\\sum_{i=1}^n y_i(x_i - \\bar{x})}{\\sum_{i=1}^n(x_i - \\bar{x})^2}\n$$\n\n. . .\n\n$$\n\\tilde{\\sigma}^2_{\\epsilon} = \\frac{\\sum_{i=1}^n(y_i - \\tilde{\\beta}_0 - \\tilde{\\beta}_1x_i)^2}{n} = \\frac{\\sum_{i=1}^ne_i^2}{n}\n$$\n\n## Putting it all together\n\n::: incremental\n-   The MLEs $\\tilde{\\beta}_0$ and $\\tilde{\\beta}_1$ are equivalent to the least-squares estimators, when the errors follow independent and identical normal distributions\n\n-   This means the least-squares estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ and inherit all the nice properties of MLEs\n\n    -   Consistency\n    -   Efficiency - minimum variance among all consistent estimators\n    -   Asymptotically normal\n:::\n\n## Putting it all together {.incremental}\n\n-   From previous work, we also know estimators $\\tilde{\\beta}_0$ and $\\tilde{\\beta}_1$ are unbiased\n\n-   Note that the MLE $\\tilde{\\sigma}^2_{\\epsilon}$ is *asymptotically unbiased*\n\n    -   The estimate from least-squares $\\hat{\\sigma}_{\\epsilon}^2$ is unbiased\n\n## References\n",
    "supporting": [
      "13-mle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}