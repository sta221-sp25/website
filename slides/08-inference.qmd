---
title: "Inference for regression"
author: "Prof. Maria Tackett"
date: "2025-02-04"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
  html: 
    output-file: 08-inference-notes.html
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

options(scipen=999)
```

## Announcements

-   Lab 03 due **TODAY** at 11:59pm

-   [Click here](https://prodduke.sharepoint.com/:p:/s/ARCStaff839/EZ4PKTRTlCVMpFZiR6XoRycB4UUlRMuI2_Rda9hKxNZtsA) to learn more about the Academic Resource Center

-   [Statistics experience](../hw/stats-experience.html) due **Tuesday, April 22**

## Poll: Office hours availability

<center>

```{=html}
<iframe width="640px" height="480px" src="https://forms.office.com/r/DL8rBQ988y?embed=true" frameborder="0" marginwidth="0" marginheight="0" style="border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>
```

ðŸ”— <https://forms.office.com/r/DL8rBQ988y>

</center>

## Topics

-   Understand statistical inference in the context of regression

-   Describe the assumptions for regression

-   Understand connection between distribution of residuals and inferential procedures

-   Conduct inference on a single coefficient

## Computing setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(kableExtra)  
library(patchwork)   

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Data: NCAA Football expenditures {.midi}

Today's data come from [Equity in Athletics Data Analysis](https://ope.ed.gov/athletics/#/datafile/list) and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a [March 2022 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md).

We will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :

-   `total_exp_m`: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)

-   `enrollment_th`: Total student enrollment in the 2019 - 2020 academic year (in thousands)

-   `type`: institution type (Public or Private)

```{r}
#| include: false
#| eval: false

## code to make data set for these notes

sports <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-29/sports.csv') 

# filter data to only include D1 football for the year 2019

sports |>
  filter(sports == "Football", 
         classification_name == "NCAA Division I-FBS", year == 2019) |>
  mutate(type = if_else(sector_name == "Private nonprofit, 4-year or above", "Private", "Public"), 
         enrollment_th = ef_total_count / 1000,
         total_exp_m = total_exp_menwomen/ 1000000) |>
  select(year, institution_name, city_txt, state_cd, zip_text, type,
         enrollment_th, 
         total_exp_m) |> 
  write_csv("data/ncaa-football-exp.csv")


```

```{r}
football <- read_csv("data/ncaa-football-exp.csv")
```

## Univariate EDA

```{r}
#| echo: false

p1 <- ggplot(data = football, aes(x = total_exp_m)) + 
  geom_histogram(fill = "steelblue", color = "black", binwidth = 3) + 
  labs( x= "Total Football Expenditures (in $Millions)")

p2 <- ggplot(data = football, aes(x = enrollment_th)) + 
  geom_histogram(binwidth = 3, fill = "steelblue", color = "black") +
  labs(x = "Total Student Enrollment (in Thousands)")

p3 <- ggplot(data = football, aes(x = type)) + 
  geom_bar(fill = "steelblue", color = "black") + 
  labs(x = "Insitution Type")

p1 + (p2 / p3)
```

## Bivariate EDA

```{r}
#| echo: false
#| 
p4 <- ggplot(data = football, aes(x = enrollment_th, y = total_exp_m)) +
  geom_point() +
  labs(x = "Total Student Enrollment (in Thousands)", 
       y = "Total Football Expenditures (in $Millions)", 
       title = "Football Expenditures vs. Enrollment")

p5 <- ggplot(data = football, aes(x = type, y = total_exp_m, fill = type)) + 
  geom_boxplot() +
  labs(x = "Institution Type", 
       y = "",
       title = "Football Expenditure vs. Type") + 
  theme(legend.position = "none")

p4 + p5
```

## Regression model

```{r}
#| echo: true
exp_fit <- lm(total_exp_m ~ enrollment_th + type, data = football)
tidy(exp_fit) |>
  kable(digits = 3)
```

<br>

For every additional 1,000 students, we expect an institution's total expenditures on football to increase by \$780,000, on average, holding institution type constant.

## From sample to population {.midi}

> For every additional 1,000 students, we expect an institution's total expenditures on football to increase by \$780,000, on average, holding institution type constant.

. . .

::: incremental
-   This estimate is valid for the single sample of `r nrow(football)` higher education institutions in the 2019 - 2020 academic year.
-   But what if we're not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?
-   What if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?
:::

# Inference for regression

## Statistical inference

:::::: columns
:::: {.column width="40%"}
::: midi
-   **Statistical inference** provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be representative (ideally random) of the population we're interested in
:::
::::

::: {.column width="60%"}
![Image source: Eugene Morgan Â© Penn State](images/08/inference.png){fig-align="center"}
:::
::::::

## Inference for linear regression

-   **Inference based on ANOVA**

    -   Hypothesis test for the statistical significance of the overall regression model

    -   Hypothesis test for a subset of coefficients

-   **Inference for a single coefficient** $\beta_j$ (today's focus)

    -   Hypothesis test for a coefficient $\beta_j$

    -   Confidence interval for a coefficient $\beta_j$

## Linear regression model {.midi}

$$
\begin{aligned}
\mathbf{y} &= \text{Model} + \text{Error} \\[5pt]
&= f(\mathbf{X}) + \boldsymbol{\epsilon} \\[5pt]
&= E(\mathbf{y}|\mathbf{X}) + \mathbf{\epsilon} \\[5pt]
&= \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}
\end{aligned}
$$

. . .

::: incremental
-   We have discussed multiple ways to find the least squares estimates of $\boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\\beta_1\end{bmatrix}$

    -   None of these approaches depend on the distribution of $\boldsymbol{\epsilon}$

-   Now we will use statistical inference to draw conclusions about $\boldsymbol{\beta}$ that depend on particular assumptions about the distribution of $\boldsymbol{\epsilon}$
:::

## Linear regression model

$$\begin{aligned}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}, \hspace{8mm} \boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2_{\epsilon}\mathbf{I})
\end{aligned}
$$

such that the errors are independent and normally distributed.

. . .

-   **Independent**: Knowing the error term for one observation doesn't tell us about the error term for another observation
-   **Normally distributed**: The distribution follows a particular mathematical model that is unimodal and symmetric

## Distribution of error terms

The error terms follow a (multivariate) normal distribution with mean $\mathbf{0}$ and variance $\sigma^2\mathbf{I}$

$$f(\boldsymbol{\epsilon}) = \frac{1}{(2\pi)^{n/2}|\sigma^2_{\epsilon}\mathbf{I}|^{1/2}}\exp\Big\{-\frac{1}{2}(\boldsymbol{\epsilon} - \mathbf{0})^\mathsf{T}(\sigma^2_{\epsilon}\mathbf{I})^{-1}(\boldsymbol{\epsilon}- \mathbf{0})\Big\}$$

<br>

This probability density function is a mathematical function that tells us about the shape of the distribution.

## Describing random phenomena {.midi}

::: incremental
-   There is some uncertainty in the error terms (and thus the response variable), so we use mathematical models to describe that uncertainty.

-   Some terminology:

    -   **Sample space**: Set of all possible outcomes

    -   **Random variable**: Function (mapping) from the sample space onto real numbers

    -   **Event:** Subset of the sample space, i.e., a set of possible outcomes (possible values the random variable can take)

    -   **Probability density function:** Mathematical function that produces probability of occurrences for events in the sample space for a continuous random variable
:::

## Visualizing distribution of $\mathbf{y}|\mathbf{X}$ {.midi}

$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}

## Expected value {background-color="#ccddeb"}

Let $\mathbf{z} = \begin{bmatrix}z_1 \\ \vdots \\z_p\end{bmatrix}$ be a $p \times 1$ vector of random variables.

<br>

. . .

Then $E(\mathbf{z}) = E\begin{bmatrix}z_1 \\ \vdots \\ z_p\end{bmatrix} = \begin{bmatrix}E(z_1) \\ \vdots \\ E(z_p)\end{bmatrix}$

## Expected value {background-color="#ccddeb"}

Let $\mathbf{A}$ and $\mathbf{C}$ be $n \times p$ matrices of constants and $\mathbf{z}$ a $p \times 1$ vector of random variables. Then

$$
E(\mathbf{Az}) = \mathbf{A}E(\mathbf{z})
$$

<br>

. . .

$$
E(\mathbf{Az} + \mathbf{C}) = E(\mathbf{Az}) + E(\mathbf{C}) = \mathbf{A}E(\mathbf{z}) + \mathbf{C}
$$

## Expected value of the response

::: question
Show $$
E(\mathbf{y}|\mathbf{X}) = \mathbf{X}\boldsymbol{\beta}
$$
:::

## Variance {background-color="#ccddeb"}

\
Let $\mathbf{z} = \begin{bmatrix}z_1 \\ \vdots \\z_p\end{bmatrix}$ be a $p \times 1$ vector of random variables.

<br>

. . .

Then $Var(\mathbf{z}) = \begin{bmatrix}Var(z_1) & Cov(z_1, z_2) & \dots & Cov(z_1, z_p)\\ Cov(z_2, z_1) & Var(z_2) & \dots & Cov(z_2, z_p) \\ \vdots & \vdots & \dots & \cdot \\ Cov(z_p, z_1) & Cov(z_p, z_2) & \dots & Var(z_p)\end{bmatrix}$

## Variance {background-color="#ccddeb"}

Let $\mathbf{A}$ be an $n \times p$ matrix of constants and $\mathbf{z}$ a $p \times 1$ vector of random variables. Then

$$
Var(\mathbf{z}) = E[(\mathbf{z} - E(\mathbf{z}))(\mathbf{z} - E(\mathbf{z}))^\mathsf{T}]
$$

<br>

. . .

$$
\begin{aligned}
Var(\mathbf{Az}) &= E[(\mathbf{Az} - E(\mathbf{Az}))(\mathbf{Az} - E(\mathbf{Az}))^\mathsf{T}] \\[8pt]
& = \mathbf{A}Var(\mathbf{z})\mathbf{A}^\mathsf{T}
\end{aligned}
$$

## Variance of the response

::: question
Show

$$
Var(\mathbf{y}|\mathbf{X}) = \sigma^2_\epsilon\mathbf{I}
$$
:::

## Linear transformation of normal random variable {background-color="#ccddeb"}

Suppose $\mathbf{z}$ is (multivariate) normal random variable such that $\mathbf{z} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$

<br>

A linear transformation of $\mathbf{z}$ is also multivariate normal, such that

$$
\mathbf{A}\mathbf{z} + \mathbf{B} \sim N(\mathbf{A}\boldsymbol{\mu} + \mathbf{B}, \mathbf{A}\boldsymbol{\Sigma}\mathbf{A}^\mathsf{T})
$$

::: question
Explain why $\mathbf{y}|\mathbf{X}$ is normally distributed.
:::

## Assumptions for regression

::::: columns
::: {.column width="50%"}
$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}
:::

::: {.column width="50%"}
1.  **Linearity:** There is a linear relationship between the response and predictor variables.
2.  **Constant Variance:** The variability about the least squares line is generally constant.
3.  **Normality:** The distribution of the residuals is approximately normal.
4.  **Independence:** The residuals are independent from one another.
:::
:::::

## Estimating $\sigma^2_{\epsilon}$ {.midi}

-   Once we fit the model, we can use the residuals to estimate $\sigma_{\epsilon}^2$

-   The estimated value $\hat{\sigma}^2_{\epsilon}$ is needed for hypothesis testing and constructing confidence intervals for regression

$$
\hat{\sigma}^2_\epsilon = \frac{SSR}{n - p - 1} = \frac{\mathbf{e}^\mathsf{T}\mathbf{e}}{n-p-1} 
$$

. . .

-   The **regression standard error** $\hat{\sigma}_{\epsilon}$ is a measure of the average distance between the observations and regression line

$$
\hat{\sigma}_\epsilon = \sqrt{\frac{SSR}{n - p - 1}} 
$$

# Inference for a single coefficient

## Inference for $\beta_j$

We often want to conduct inference on individual model coefficients

-   **Hypothesis test:** Is there a linear relationship between the response and $x_j$?

-   **Confidence interval**: What is a plausible range of values $\beta_j$ can take?

. . .

But first we need to understand the distribution of $\hat{\beta}_j$

## Sampling distribution of $\hat{\beta}$ {.midi}

-   A **sampling distribution** is the probability distribution of a statistic for a large number of random samples of size $n$ from a population

-   The sampling distribution of $\hat{\boldsymbol{\beta}}$ is the probability distribution of the estimated coefficients if we repeatedly took samples of size $n$ and fit the regression model

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2_\epsilon(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1})
$$

. . .

The estimated coefficients $\hat{\boldsymbol{\beta}}$ are **normally distributed** with

$$
E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta} \hspace{10mm} Var(\hat{\boldsymbol{\beta}}) = \sigma^2_{\epsilon}(\boldsymbol{X}^\mathsf{T}\boldsymbol{X})^{-1}
$$

## Expected value of $\boldsymbol{\hat{\beta}}$

::: question
Show

$$E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$$
:::

## Sampling distribution of $\hat{\beta}_j$

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2_\epsilon(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1})
$$

Let $\mathbf{C} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}$. Then, for each coefficient $\hat{\beta}_j$,

::: incremental
-   $E(\hat{\beta}_j) = \boldsymbol{\beta}_j$, the $j^{th}$ element of $\boldsymbol{\beta}$

-   $Var(\hat{\beta}_j) = \sigma^2_{\epsilon}C_{jj}$

-   $Cov(\hat{\beta}_i, \hat{\beta}_j) = \sigma^2_{\epsilon}C_{ij}$
:::

## $SE(\hat{\boldsymbol{\beta}})$ for NCAA data

```{r}
X <- model.matrix(total_exp_m ~ enrollment_th + type, data = football)
sigma_sq <- glance(exp_fit)$sigma^2

var_beta <- sigma_sq * solve(t(X) %*% X)
var_beta
```

## $SE(\hat{\boldsymbol{\beta}})$ for NCAA data

```{r}
#| echo: false
tidy(exp_fit) |> kable(digits = 3)
```

<br>

```{r}
sqrt(diag(var_beta))
```

# Hypothesis test for $\beta_j$

## Steps for a hypothesis test

1.  State the null and alternative hypotheses.
2.  Calculate a test statistic.
3.  Calculate the p-value.
4.  State the conclusion.

## Hypothesis test for $\beta_j$: Hypotheses

We will generally test the hypotheses:

$$
\begin{aligned}
&H_0: \beta_j = 0 \\
&H_a: \beta_j \neq 0
\end{aligned}
$$

::: question
State these hypotheses in words.
:::

## Hypothesis test for $\beta_j$: Test statistic {.midi}

**Test statistic:** Number of standard errors the estimate is away from the null

$$
\text{Test Statstic} = \frac{\text{Estimate - Null}}{\text{Standard error}} \\
$$

. . .

If $\sigma^2_{\epsilon}$ was known, the test statistic would be

$$Z = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)} ~ = ~\frac{\hat{\beta}_j - 0}{\sqrt{\sigma^2_\epsilon C_{jj}}} ~\sim ~ N(0, 1)
$$

. . .

In general, $\sigma^2_{\epsilon}$ is [**not**]{.underline} known, so we use $\hat{\sigma}_{\epsilon}^2$ to calculate $SE(\hat{\beta}_j)$

$$T = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)} ~ = ~\frac{\hat{\beta}_j - 0}{\sqrt{\hat{\sigma}^2_\epsilon C_{jj}}} ~\sim ~ t_{n-p-1}
$$

## Hypothesis test for $\beta_j$: Test statistic

-   The test statistic $T$ follows a $t$ distribution with $n - p -1$ degrees of freedom.

-   We need to account for the additional variability introduced by calculating $SE(\hat{\beta}_j)$ using an estimated value instead of a constant

## *t* vs. N(0,1)

```{r}
#| label: fig-normal-t-curves
#| fig-cap: Standard normal vs. t distributions
#| echo: false

colors <- c("N(0,1)" = "black", 
            "t, df = 2" = "red", 
            "t, df = 5" = "blue",
            "t, df = 10" = "darkgreen", 
            "t, df = 30" = "purple")
ggplot() + 
  xlim(-5, 5) + 
  geom_function(fun = dnorm, aes(color = "N(0,1)")) + 
  geom_function(fun = dt,args = list(df = 2), aes(color = "t, df = 2")) +
  geom_function(fun = dt,args = list(df = 5), aes(color = "t, df = 5")) + 
  geom_function(fun = dt,args = list(df = 10), aes(color = "t, df = 10"))  +
  geom_function(fun = dt,args = list(df = 30), aes(color ="t, df = 30")) + 
  scale_color_manual(values = colors) +
    labs(x = "", y = "", color = "") + 
  theme_bw()
```

## Hypothesis test for $\beta_j$: P-value

The **p-value** is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed

$$
p-value = P(|t| > |\text{test statistic}|),
$$

calculated from a $t$ distribution with $n- p - 1$ degrees of freedom

. . .

::: question
Why do we take into account "extreme" on both the high and low ends?
:::

## Understanding the p-value

| Magnitude of p-value    | Interpretation                        |
|:------------------------|:--------------------------------------|
| p-value \< 0.01         | strong evidence against $H_0$         |
| 0.01 \< p-value \< 0.05 | moderate evidence against $H_0$       |
| 0.05 \< p-value \< 0.1  | weak evidence against $H_0$           |
| p-value \> 0.1          | effectively no evidence against $H_0$ |

<br>

**These are general guidelines. The strength of evidence depends on the context of the problem.**

## Hypothesis test for $\beta_j$: Conclusion

**There are two parts to the conclusion**

-   Make a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( $\alpha$ level)

    -   If $\text{P-value} < \alpha$: Reject $H_0$

    -   If $\text{P-value} \geq \alpha$: Fail to reject $H_0$

-   State the conclusion in the context of the data

# Application exercise

::: appex
ðŸ“‹ [https://sta221-sp25.netlify.app/ae/ae-03-inference](../ae/ae-03-inference){.uri}
:::

## Recap

-   Introduced statistical inference in the context of regression

-   Described the assumptions for regression

-   Connected the distribution of residuals and inferential procedures

-   Conducted inference on a single coefficient
