---
title: "Model diagnostics"
author: "Prof. Maria Tackett"
date: "2024-10-17"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
filters:
  - parse-latex
bibliography: references.bib
---

## Announcements

-   Exam corrections (optional) due Thursday, October 24 at 11:59pm [on Canvas](https://canvas.duke.edu/courses/38867/assignments/169519)

-   Labs resume on Monday

-   Project: Exploratory data analysis due October 31

-   Statistics experience due Tuesday, November 26

## Computing set up

```{r}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(patchwork)   
library(viridis)

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Topics

-   Review: Maximum likelihood estimation

-   Influential points

-   Model diagnostics

    -   Leverage

    -   Studentized residuals

    -   Cook's Distance

# Maximum likelihood estimation

## Likelihood

::: incremental
-   A **likelihood** is a function that tells us how likely we are to observe our data for a given parameter value (or values). <!--# Find a better definition - Casella Berger maybe?-->

-   Note that this is **not** the same as the probability function.

    -   **Probability function**: Fixed parameter value(s) + input possible outcomes $\Rightarrow$ probability of seeing the different outcomes given the parameter value(s)

    -   **Likelihood function**: Fixed data + input possible parameter values $\Rightarrow$ probability of seeing the fixed data for each parameter value
:::

## Maximum likelihood estimation

-   Maximum likelihood estimation is the process of finding the values of the parameters that maximize the likelihood function , i.e., the values that are most likely given the observed data.

-   There are three primary ways to find the maximum likelihood estimator

    -   Approximate using a graph
    -   Using calculus
    -   Numerical approximation

## Simple linear regression model

Suppose we have the simple linear regression (SLR) model

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i, \hspace{10mm} \epsilon_i \sim N(0, \sigma^2_{\epsilon})
$$

such that $\epsilon_i$ are independently and identically distributed.

<br>

. . .

We can write this model in the form below and use this to find the MLE

$$
y_i | x_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2_{\epsilon})
$$

## Likelihood for SLR

The likelihood function for $\beta_0, \beta_1, \sigma^2_{\epsilon}$ is

$$
\begin{aligned}
L&(\beta_0, \beta_1, \sigma^2_{\epsilon} | x_i, \dots, x_n, y_i, \dots, y_n) \\[5pt]
 &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_
\epsilon^2}}\exp\Big\{{-\frac{1}{2\sigma_\epsilon^2}(y_i - [\beta_0 + \beta_1x_i])^2}\Big\} \\[10pt]
& = (2\pi\sigma^2_{\epsilon})^{-\frac{n}{2}}\exp\Big\{-\frac{1}{2\sigma^2_{\epsilon}}\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)^2\Big\}
\end{aligned}
$$

## Log-likelihood for SLR

The log-likelihood function for $\beta_0, \beta_1, \sigma^2_{\epsilon}$ is

$$
\begin{aligned}
\log &L(\beta_0, \beta_1, \sigma^2_{\epsilon} | x_i, \dots, x_n, y_i, \dots, y_n) 
  \\[8pt]
& = -\frac{n}{2}\log(2\pi\sigma^2_{\epsilon}) -\frac{1}{2\sigma^2_{\epsilon}}\sum_{i=1}^n(y_i - \beta_0 - \beta_1x_i)^2
\end{aligned}
$$

<br>

. . .

We will use the log-likelihood function to find the MLEs

## MLE for $\beta_0,\beta_1, \sigma^2_{\epsilon}$

$$
\tilde{\beta}_0 = \frac{1}{n}\sum_{i=1}^ny_i  - \frac{1}{n}\tilde{\beta}_1\sum_{i=1}^n x_i 
$$

<br>

$$
\tilde{\beta}_1 = \frac{\sum_{i=1}^n y_i(x_i - \bar{x})}{\sum_{i=1}^n(x_i - \bar{x})^2}
$$

<br>

$$
\tilde{\sigma}^2_{\epsilon} = \frac{\sum_{i=1}^n(y_i - \tilde{\beta}_0 - \tilde{\beta}_1x_i)^2}{n} = \frac{\sum_{i=1}^ne_i^2}{n}
$$

## MLE for linear regression in matrix form {.midi}

$$
L(\boldsymbol{\beta}, \sigma^2_{\epsilon} | \mathbf{X}, \mathbf{y}) = \frac{1}{(2\pi)^{n/2}\sigma^n_{\epsilon}}\exp\Big\{-\frac{1}{2\sigma^2_{\epsilon}}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\Big\}
$$

. . .

$$
\log L(\boldsymbol{\beta}, \sigma^2_\epsilon | \mathbf{X}, \mathbf{y}) = -\frac{n}{2}\log(2\pi) - n \log(\sigma_{\epsilon}) - \frac{1}{2\sigma^2_{\epsilon}}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\mathbf{\beta})
$$

. . .

::: question
1.  For a fixed value of $\sigma_\epsilon$ , we know that $\log L$ is maximized when what is true about $(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$ ?
2.  What does this tell us about the relationship between the MLE and least-squares estimator for $\boldsymbol{\beta}$?
:::

## Why maximum likelihood estimation?

-   *"Maximum likelihood estimation is, by far, the most popular technique for deriving estimators."* [@casella2024statistical, pp. 315]

-   MLEs have nice statistical properties. They are

    -   Consistent

    -   Efficient - Have the smallest MSE among all consistent estimators

    -   Asymptotically normal

## Putting it all together

::: incremental
-   The MLE $\tilde{\boldsymbol{\beta}}$ is equivalent to the least-squares estimator $\hat{\boldsymbol{\beta}}$ , when the errors follow independent and identical normal distributions

-   This means the least-squares estimator $\hat{\mathbf{\boldsymbol{\beta}}}$ and inherit all the nice properties of MLEs

    -   Consistency
    -   Efficiency - minimum variance among all consistent estimators
    -   Asymptotically normal
:::

## Putting it all together {.incremental}

-   From previous work, we also know $\hat{\boldsymbol{\beta}}$ is unbiased and thus the MLE $\tilde{\boldsymbol{\beta}}$ is unbiased
-   Note that the MLE $\tilde{\sigma}^2_{\epsilon}$ is *asymptotically unbiased*
    -   The estimate from least-squares $\hat{\sigma}_{\epsilon}^2$ is unbiased

# Model diagnostics

## Data: Duke lemurs {.midi}

Today's data contains a subset of the original Duke Lemur data set available in the [TidyTuesday GitHub repo](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-08-24/readme.md). This data includes information on â€œyoung adultâ€ lemurs from the [Coquerelâ€™s sifaka species](https://lemur.duke.edu/discover/meet-the-lemurs/coquerels-sifaka/) (PCOQ), the largest species at the Duke Lemur Center. The analysis will focus on the following variables:

-   `age_at_wt_mo`: Age in days: Age of the animal when the weight was taken, in days (Wei ght_Date-DOB)

-   `weight_g`: Weight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights \>500g generally to the nearest 1-20g.

**The goal of the analysis is to use the age of the lemurs to understand variability in the weight.**

```{r}
#| echo: false

lemurs <- read_csv("data/lemurs-pcoq.csv")
```

## EDA

```{r}
#| echo: false

p1 <- ggplot(data = lemurs, aes(x = weight_g)) + 
  geom_histogram(fill = "steelblue", color = "black") + 
  labs(x = "",
       title = "Weight in grams")

p2 <- ggplot(data = lemurs, aes(x = age_at_wt_mo)) + 
  geom_histogram(fill = "steelblue", color = "black") + 
  labs(x = "",
       title = "Age in months")

p3 <- ggplot(data = lemurs, aes(x = age_at_wt_mo, y = weight_g)) + 
  geom_point() + 
  labs(x = "Age in months",
       y = "Weight in grams", 
       title = "Weight vs. Age if PCOQ lemurs")

(p1  + p2) / p3

```

## Fit model

```{r}
lemurs_fit <- lm(weight_g ~ age_at_wt_mo, data = lemurs)

tidy(lemurs_fit) |> 
  kable(digits = 3)
```

## Model conditions

```{r}
#| echo: false
lemurs_aug <- augment(lemurs_fit)

ggplot(data = lemurs_aug, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, linetype = 2, color = "red") + 
  labs(x = "Fitted values", 
       y = "Residuals", 
       title = "Residuals vs. Fitted Values")
```

-   Linearity

-   Constant variance

## Model conditions

```{r}
#| echo: false

ggplot(data = lemurs_aug, aes(x = .resid)) +
  geom_histogram(fill = "steelblue", color = "black") + 
  labs(x = "Residuals", 
       title = "Distribution of residuals")
```

-   Normality

-   Independence

## Model diagnostics

```{r}
lemurs_aug <- augment(lemurs_fit)

lemurs_aug |> slice(1:10)
```

## Model diagnostics in R {.midi}

Use the `augment()` function in the broom package to output the model diagnostics (along with the predicted values and residuals)

-   response and predictor variables in the model
-   `.fitted`: predicted values
-   `.se.fit`: standard errors of predicted values
-   `.resid`: residuals
-   `.hat`: leverage
-   `.sigma`: estimate of residual standard deviation when the corresponding observation is dropped from model
-   `.cooksd`: Cook's distance
-   `.std.resid`: standardized residuals

## Influential Point

An observation is **influential** if removing has a noticeable impact on the regression coefficients

```{r,echo=F}
set.seed(12)
n <- 20
x <- c(runif(n,0,1))
y <- 3*x + rnorm(n,0,.5)
new.pt <- data.frame(x=2,y=0)
x.new <- c(x,2)
y.new <- c(y,0)
data <- bind_cols(x=x.new,y=y.new)
p1<- ggplot(data=data,aes(x=x,y=y))+geom_point(alpha =0.5)  + 
              geom_point(data=new.pt,color="red",size=4,shape=17) + 
  geom_smooth(method="lm",se=F) + 
  labs(title = "With Influential Point")+ theme_light()+
  theme(title=element_text(hjust=0.5,size=14)) + 
  scale_x_continuous(limits = c(0,2)) 

data2 <- bind_cols(x=x,y=y)
p2 <- ggplot(data=data2,aes(x=x,y=y))+geom_point(alpha=0.5) + geom_smooth(method="lm",se=F) + 
  labs(title="Without Influential Point") + 
  scale_x_continuous(limits = c(0, 2)) + theme_light() + theme(title=element_text(hjust=0.5,size=14))  
p1 + p2
```

## Influential points

::: incremental
-   Influential points have a noticeable impact on the coefficients and standard errors used for inference
-   These points can sometimes be identified in a scatterplot if there is only one predictor variable
    -   This is often not the case when there are multiple predictors
-   We will use measures to quantify an individual observation's influence on the regression model
    -   **leverage**, **standardized residuals**, and **Cook's distance**
:::

# Leverage

## Hat matrix

-   Recall the **hat matrix** $\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$

-   We've seen that $\mathbf{H}$ is used to compute $Var(\hat{\mathbf{y}}) = \sigma^2_{\epsilon}\mathbf{H}$ and $Var(\mathbf{e}) = \sigma^2_{\epsilon}(\mathbf{I} - \mathbf{H})$

-   An element of $\mathbf{H}$, $h_{ij}$, is the leverage of the observation $y_i$ on the fitted values $\hat{y}_{ij}$

## Leverage

-   We focus on the diagonal elements

    $$
    h_{ii} = \mathbf{x}_i^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{x}_i
    $$such that $\mathbf{x}^T_i$ is the $i^{th}$ row of $\mathbf{X}$

-   $h_{ii}$ is the **leverage**: a measure of the distance of the $i^{th}$ observation from the center (or centroid) of $x$ space

-   Observations with large values of $h_{ii}$ are far away from the typical value (or combination of values) of the <u>predictors</u> in the data

## Large leverage

-   The sum of the leverages for all points is $p + 1$, where $p$ is the number of predictors in the model . More specifically

    $$
    \sum_{i=1}^n h_{ii} = \text{rank}(\mathbf{H}) = \text{rank}(\mathbf{X}) = p+1
    $$

-   The average value of leverage, $h_{ii}$, is $\bar{h} =  \frac{(p+1)}{n}$

-   An observation has **large leverage** if $$h_{ii} > \frac{2(p+1)}{n}$$

## Lemurs: Leverage

```{r}
h_threshold <- 2 * 2 / nrow(lemurs)
h_threshold
```

```{r}
lemurs_aug |>
  filter(.hat > h_threshold)
```

. . .

::: question
Why do you think these points have large leverage?
:::

## Let's look at the data

```{r}
#| echo: false

lemurs_aug %>%
  mutate(large_h = if_else(.hat > h_threshold, "Yes", "No"),
         large_h = factor(large_h, levels = c("Yes", "No")),
         obs_num = row_number()) |>
  ggplot(aes(x = obs_num, y = age_at_wt_mo)) +
  geom_point(aes(color = large_h), size = 2) + 
  labs(x = "Observation number",
    y = "Age at weight (in months)", title = "Age of PCOQ lemurs", 
    color = "Large h") +
  #theme(legend.position = "none") + 
  scale_color_viridis(discrete = T, end = 0.9)
```

## Large leverage

If there is point with high leverage, ask

-   `r emo::ji("question")` Is there a data entry error?

-   `r emo::ji("question")` Is this observation within the scope of individuals for which you want to make predictions and draw conclusions?

-   `r emo::ji("question")` Is this observation impacting the estimates of the model coefficients? (Need more information!)

. . .

Just because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore we need to check other measures.

## 

# Scaled residuals

## Scaled residuals

-   What is the best way to identify outlier points that don't fit the pattern from the regression line?
    -   Look for points that have large residuals
-   We can rescale residuals and put them on a common scale to more easily identify "large" residuals
-   We will consider two types of scaled residuals: standardized residuals and studentized residuals

## Standardized residuals

-   The variance of the residuals can be estimated by the mean squared residuals (MSR) $= \frac{SSR}{n - p - 1} = \hat{\sigma}^2_{\epsilon}$

-   We can use MSR to compute **standardized residuals**

    $$
    std.res_i = \frac{e_i}{\sqrt{MSR}}
    $$

-   Standardized residuals are produced by `augment()` in the column `.std.resid`

------------------------------------------------------------------------

## Studentized residuals

-   MSR is an approximation of the variance of the residuals.

-   The variance of the residuals is $Var(\mathbf{e}) = \sigma^2_{\epsilon}(\mathbf{I} - \mathbf{H})$

    -   The variance of the $i^{th}$ residual is $Var(e_i) = \sigma^2_{\epsilon}(1 - h_{ii})$

-   The **studentized residual** is the residual rescaled by the more exact calculation for variance

$$
r_i = \frac{e_{i}}{\sqrt{\hat{\sigma}^2_{\epsilon}(1 - h_{ii})}}
$$

-   Standardized and studentized residuals provide similar information about which points are outliers in the response.

## Using standardized residuals

We can examine the standardized residuals directly from the output from the `augment()` function

```{r}
#| echo: false
#| label: std residuals vs fitted


ggplot(data = lemurs_aug, aes(x = .fitted, y = .std.resid)) + 
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype =2)+
  labs(x  = "Fitted values",
       y = "Std. Residuals",
       title = "Standardized residuals vs. fitted")
```

-   An observation is a *potential outlier* if its standardized residual is beyond $\pm 3$

## Digging in to the data

Let's look at the value of the response variable to better understand potential outliers

```{r}
#| echo: false
lemurs_aug %>%
  mutate(large_resid = if_else(abs(.std.resid) > 3, "Yes", "No"),
         large_resid = factor(large_resid, levels = c("Yes", "No")),
         obs_num = row_number()) |>
  ggplot(aes(x = obs_num, y = weight_g)) +
  geom_point(aes(color = large_resid), size = 2) + 
  labs(x = "Observation number",
    y = "Weight (in grams)", title = "Weight of PCOQ lemurs", 
    color = "Large std. residual") +
  #theme(legend.position = "none") + 
  scale_color_viridis(discrete = T, end = 0.9)
```

# Cook's Distance

## Motivating Cook's Distance

-   An observation's influence on the regression line depends on

    -   How close it lies to the general trend of the data

    -   Its leverage

-   **Cook's Distance** is a statistic that includes both of these components to measure an observation's overall impact on the model

## Cook's Distance

Cook's distance for the $i^{th}$ observation is

$$
D_i = \frac{r^2_i}{p + 1}\Big(\frac{h_{ii}}{1 - h_{ii}}\Big)
$$

. . .

This measure is a combination of

-   How well the model fits the $i^{th}$ observation (magnitude of residuals)

-   How far the $i^{th}$ observation is from the rest of the data (where the point is in the $x$ space)

## Using Cook's Distance

-   An observation with large $D_i$ is said to have a strong influence on the predicted values

-   General thresholds .An observation with

    -   $D_i > 0.5$ is **moderately influential**

    -   $D_i > 1$ is **very influential**

## Cook's Distance

Cook's Distance is in the column `.cooksd` in the output from the `augment()` function

```{r}
#| echo: false

lemurs_aug |>
  mutate(obs_num = row_number()) |>
  ggplot(aes(x = obs_num, y = .cooksd)) + 
  geom_point() + 
  geom_hline(yintercept = 1,color = "red") +
  labs(x = "Observation number",
       y = "Cook's Distance")
```

## Using these measures

-   Standardized residuals, leverage, and Cook's Distance should all be examined together

-   Examine plots of the measures to identify observations that are outliers, high leverage, and may potentially impact the model.

## What to do with outliers/influential points?

-   First consider if the outlier is a result of a data entry error.

-   If not, you may consider dropping an observation if it's an outlier in the predictor variables if...

    -   It is meaningful to drop the observation given the context of the problem

    -   You intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions

## What to do with outliers/influential points?

-   It is generally **not** good practice to drop observations that ar outliers in the value of the response variable

    -   These are legitimate observations and should be in the model

    -   You can try transformations or increasing the sample size by collecting more data

-   A general strategy when there are influential points is to fit the model with and without the influential points and compare the outcomes

------------------------------------------------------------------------

## Recap

-   Reviewed Maximum likelihood estimation

-   Influential points

-   Model diagnostics

    -   Leverage

    -   Studentized residuals

    -   Cook's Distance

## References
