---
title: "SLR: Matrix representation"
author: "Prof. Maria Tackett"
date: "2025-01-21"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
  html: 
    output-file: 04-slr-matrix-notes.html
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"

execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
```

## Announcements

-   Lab 01 due on **TODAY at 11:59pm**

    -   Push work to GitHub repo

    -   Submit final PDF on Gradescope + mark pages for each question

-   HW 01 will be assigned on Thursday

## Topics

-   Application exercise on model assessment
-   Matrix representation of simple linear regression
    -   Model form
    -   Least square estimate
    -   Predicted (fitted) values
    -   Residuals

```{r packages}
#| echo: false
#| message: false
#| warning: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling (includes broom, yardstick, and other packages)
library(openintro)   # for the duke_forest dataset
library(scales)      # for pretty axis labels
library(knitr)       # for pretty tables
library(patchwork)   # arrange plots

# set default theme for ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

# Model assessment

## Two statistics {.midi}

-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)

    $$
    RMSE = \sqrt{\frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n}} = \sqrt{\frac{\sum_{i=1}^ne_i^2}{n}}
    $$

-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)

$$R^2 = \frac{SSM}{SST} = 1 - \frac{SSR}{SST}$$

# Application exercise

::: appex
ðŸ“‹ [sta221-sp25.netlify.app/ae/ae-01-model-assessment.html](../ae/ae-01-model-assessment.html){.uri}

Open `ae-01` from last class. Complete Part 2.
:::

# Matrix representation of simple linear regression

## SLR: Statistical model (population)

When we have a quantitative response, $Y$, and a single quantitative predictor, $X$, we can use a **simple linear regression** model to describe the relationship between $Y$ and $X$.

$$Y = \beta_0 + \beta_1 X + \epsilon$$

<br>

-   $\beta_1$: Population (true) slope of the relationship between $X$ and $Y$
-   $\beta_0$: Population (true) intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error terms centered at 0 with variance $\sigma^2_{\epsilon}$

## SLR in matrix form

The simple linear regression model to describe the relationship between the response $Y$ and single quantitative predictor $X$ can be represented as

::: equation
$$
\large{\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}}
$$
:::

## SLR in matrix form 

$$
\underbrace{
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} }_
{\mathbf{y}} \hspace{3mm}
= 
\hspace{3mm}
\underbrace{
\begin{bmatrix}
1 &x_1 \\
\vdots &  \vdots \\
1 &  x_n
\end{bmatrix}
}_{\mathbf{X}}
\hspace{2mm}
\underbrace{
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
}_{\boldsymbol{\beta}}
\hspace{3mm}
+
\hspace{3mm}
\underbrace{
\begin{bmatrix}
\epsilon_1 \\
\vdots\\
\epsilon_n
\end{bmatrix}
}_\boldsymbol{\epsilon}
$$

<br>

::: question
What are the dimensions of $\mathbf{y}$, $\mathbf{X}$, $\boldsymbol{\beta}$, and $\boldsymbol{\epsilon}$?
:::

## Derive least squares estimator for $\boldsymbol{\beta}$

**Goal**: Find estimator $\hat{\boldsymbol{\beta}}= \begin{bmatrix}\hat{\beta}_0 \\ \hat{\beta}_1 \end{bmatrix}$ that minimizes the sum of squared errors $$
\sum_{i=1}^n \epsilon_i^2 = \mathbf{\epsilon}^\mathsf{T}\mathbf{\epsilon} = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$

## Gradient {.midi background-color="#ccddeb"}

Let $\mathbf{x} = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\x_k\end{bmatrix}$be a $k \times 1$ vector and $f(\mathbf{x})$ be a function of $\mathbf{x}$.

. . .

Then $\nabla_\mathbf{x}f$, the **gradient** of $f$ with respect to $\mathbf{x}$ is

$$
\nabla_\mathbf{x}f = \begin{bmatrix}\frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_k}\end{bmatrix}
$$

## Property 1 {background-color="#ccddeb"}

Let $\mathbf{x}$ be a $k \times 1$ vector and $\mathbf{z}$ be a $k \times 1$ vector, such that $\mathbf{z}$ is not a function of $\mathbf{x}$ .

The gradient of $\mathbf{x}^\mathsf{T}\mathbf{z}$ with respect to $\mathbf{x}$ is

$$
\nabla_\mathbf{x} \hspace{1mm} \mathbf{x}^\mathsf{T}\mathbf{z} = \mathbf{z}
$$

## Side note: Property 1 {background-color="#ccddeb"}

$$
\begin{aligned}
\mathbf{x}^\mathsf{T}\mathbf{z} &= \class{fragment}{\begin{bmatrix}x_1 & x_2 & \dots &x_k\end{bmatrix}
 \begin{bmatrix}z_1 \\ z_2 \\ \vdots \\z_k\end{bmatrix}} \\[10pt]
 &\class{fragment}{= x_1z_1 + x_2z_2 + \dots + x_kz_k} \\
&\class{fragment}{= \sum_{i=1}^k x_iz_i}
\end{aligned}
$$

## Side note: Property 1 {.midi background-color="#ccddeb"}

$$
\nabla_\mathbf{x}\hspace{1mm}\mathbf{x}^\mathsf{T}\mathbf{z} = \class{fragment}{\begin{bmatrix}\frac{\partial \mathbf{x}^\mathsf{T}\mathbf{z}}{\partial x_1} \\ \frac{\partial \mathbf{x}^\mathsf{T}\mathbf{z}}{\partial x_2} \\ \vdots \\ \frac{\partial \mathbf{x}^\mathsf{T}\mathbf{z}}{\partial x_k}\end{bmatrix}}  
= \class{fragment}{\begin{bmatrix}\frac{\partial}{\partial x_1} (x_1z_1 + x_2z_2 + \dots + x_kz_k) \\ \frac{\partial}{\partial x_2} (x_1z_1 + x_2z_2 + \dots + x_kz_k)\\ \vdots \\ \frac{\partial}{\partial x_k} (x_1z_1 + x_2z_2 + \dots + x_kz_k)\end{bmatrix}}
 = \class{fragment}{\begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_k\end{bmatrix} = \mathbf{z}}
$$

## Property 2 {background-color="#ccddeb"}

Let $\mathbf{x}$ be a $k \times 1$ vector and $\mathbf{A}$ be a $k \times k$ matrix, such that $\mathbf{A}$ is not a function of $\mathbf{x}$ .

Then the gradient of $\mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x}$ with respect to $\mathbf{x}$ is

$$
\nabla_\mathbf{x} \hspace{1mm} \mathbf{x}^\mathsf{T}\mathbf{A}\mathbf{x} = (\mathbf{A}\mathbf{x} + \mathbf{A}^\mathsf{T} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\mathsf{T})\mathbf{x}
$$

If $\mathbf{A}$ is symmetric, then

$$
(\mathbf{A} + \mathbf{A}^\mathsf{T})\mathbf{x} = 2\mathbf{A}\mathbf{x}
$$

::: question
Proof in HW 01.
:::

## Derive least squares estimator

Find $\hat{\boldsymbol{\beta}}$ that minimizes

$$
\begin{aligned}
\boldsymbol{\epsilon}^\mathsf{T}\boldsymbol{\epsilon} &= (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\[10pt]
&= (\mathbf{y}^\mathsf{T} - \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T})(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\\[10pt]
&=\mathbf{y}^\mathsf{T}\mathbf{y} - \mathbf{y}^\mathsf{T}\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{y} + \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}\\[10pt]
&=\mathbf{y}^\mathsf{T}\mathbf{y} - 2\boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{y} + \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}
\end{aligned}
$$

## Derive least squares estimator

$$\begin{aligned}
 \nabla_{\beta}\boldsymbol{\epsilon}^\mathsf{T}\boldsymbol{\epsilon} &= \nabla_{\boldsymbol{\beta}}( \mathbf{y}^\mathsf{T}\mathbf{y} - 2\boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{y} + \boldsymbol{\beta}^\mathsf{T}\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}) \\[10pt]
 & = -2\mathbf{X}^\mathsf{T}\mathbf{y} + \mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}
 \end{aligned}
$$<br>

Find $\hat{\boldsymbol{\beta}}$ that satisfies

$$
-2\mathbf{X}^\mathsf{T}\mathbf{y} + \mathbf{X}^\mathsf{T}\mathbf{X}\hat{\boldsymbol{\beta}} = \mathbf{0}
$$

::: equation
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}$$
:::

# Did we find a minimum? 

## Hessian matrix {background-color="#ccddeb"}

The **Hessian** matrix, $\nabla_\mathbf{x}^2f$ is a $k \times k$ matrix of partial second derivatives

$$
\nabla_{\mathbf{x}}^2f = \begin{bmatrix} \frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2f}{\partial x_1\partial x_k} \\ 
\frac{\partial^2f}{\partial\ x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} & \dots & \frac{\partial^2f}{\partial x_2 \partial x_k} \\
\vdots & \vdots & \ddots & \vdots \\ 
\frac{\partial^2f}{\partial x_k\partial x_1} & \frac{\partial^2f}{\partial x_k\partial x_2} & \dots & \frac{\partial^2f}{\partial x_k^2} \end{bmatrix}
$$

## Using the Hessian matrix {background-color="add8e6"}

If the Hessian matrix is...

-   positive-definite, then we have found a minimum.

-   negative-definite, then we have found a maximum.

-   neither positive or negative-definite, then we have found a saddle point

## Did we find a minimum? 

$$
\begin{aligned}
\nabla^2_{\boldsymbol{\beta}} \boldsymbol{\epsilon}^\mathsf{T}\boldsymbol{\epsilon} &= \nabla_{\boldsymbol{\beta}} (-2\mathbf{X}^\mathsf{T}\mathbf{y} + 2\mathbf{X}^\mathsf{T}\mathbf{X}\boldsymbol{\beta}) \\[10pt]
&{=-2\nabla_{\boldsymbol{\beta}}(\mathbf{X}^\mathsf{T}\mathbf{y}) + 2\nabla_{\boldsymbol{\beta}}(\mathbf{X}^\mathsf{T}\mathbf{X}\mathbf{\beta})} \\[10pt]
 &{\propto \mathbf{X}^\mathsf{T}\mathbf{X}}
\end{aligned}
$$

::: question
Show that $\mathbf{X}^\mathsf{T}\mathbf{X}$ is positive definite in HW 01.
:::

# Predicted values and residuals

## Predicted (fitted) values

Now that we have $\hat{\boldsymbol{\beta}}$, let's predict values of $\mathbf{y}$ using the model

$$
\hat{\mathbf{y}} = \mathbf{X}\hat{\boldsymbol{\beta}} = \underbrace{\mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}}_{\mathbf{H}}\mathbf{y} = \mathbf{H}\mathbf{y}
$$

. . .

::: equation
**Hat matrix**: $\mathbf{H} = \mathbf{X}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}$
:::

. . .

-   $\mathbf{H}$ is an $n\times n$ matrix
-   Maps vector of observed values $\mathbf{y}$ to a vector of fitted values $\hat{\mathbf{y}}$
-   It is only a function of $\mathbf{X}$ not $\mathbf{y}$

## Residuals

Recall that the residuals are the difference between the observed and predicted values

$$
\begin{aligned}
\mathbf{e} &= \mathbf{y} - \hat{\mathbf{y}}\\[10pt]
&\class{fragment}{ = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}} \\[10pt]
&\class{fragment}{ = \mathbf{y} - \mathbf{H}\mathbf{y}} \\[20pt]
\class{fragment}{\color{#993399}{\mathbf{e}}} &\class{fragment}{\color{#993399}{=(\mathbf{I} - \mathbf{H})\mathbf{y}}} \\[10pt]
\end{aligned}
$$

## Recap

-   Introduced matrix representation for simple linear regression

    -   Model form
    -   Least square estimate
    -   Predicted (fitted) values
    -   Residuals
