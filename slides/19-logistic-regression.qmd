---
title: "Logistic Regression"
author: "Kat Husar"
date: "2025-03-27"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Spring 2025](https://sta221-sp25.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs: 
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
  html: 
    output-file: 19-logistic-regression-notes.html
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

## Announcements {.midi}

-   Project Presentations in lab on Friday, March 28

    -   Check email for presentation order and feedback assignments

-   Statistics experience due April 22

## Questions from this week's content?

<center>

```{=html}
<iframe width="640px" height="480px" src="https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTBw0r2_bKCVMs9lST1_-2sxUQ1JTSFBZNlFMWUJZSDcwTUdaVzgwWUhBMC4u&embed=true" frameborder="0" marginwidth="0" marginheight="0" style="border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>
```

</center>

## Topics

-   Logistic regression for binary response variable

-   Use logistic regression model to calculate predicted odds and probabilities

-   Interpret the coefficients of a logistic regression model with

    -   a single categorical predictor
    -   a single quantitative predictor
    -   multiple predictors

## Computational setup

```{r}
#| warning: false

# load packages
library(tidyverse)
library(tidymodels)
library(knitr)
library(Stat2Data) #contains data set
library(patchwork)

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

# Recap

## Do teenagers get 7+ hours of sleep? {.small}

::::: columns
::: {.column width="40%"}
Students in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.

`Sleep7`

1: yes

0: no
:::

::: {.column width="60%"}
```{r}
#| echo: false
data(YouthRisk2009) #from Stat2Data package
sleep <- YouthRisk2009 |>
  as_tibble() |>
  filter(!is.na(Age), !is.na(Sleep7))
sleep |>
  relocate(Age, Sleep7)
```
:::
:::::

## Let's fit a linear regression model

**Outcome:** $Y$ = 1: yes, 0: no

```{r}
#| echo: false

ggplot(sleep, aes(x = Age, y = Sleep7)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Getting 7+ hours of sleep")
```

## Let's use proportions

**Outcome:** Probability of getting 7+ hours of sleep

```{r}
#| echo: false

sleep_age <- sleep |>
  group_by(Age) |>
  summarise(prop = mean(Sleep7))

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method = "lm",fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)")
```

## What happens if we zoom out?

**Outcome:** Probability of getting 7+ hours of sleep

```{r}
#| echo: false

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method = "lm",fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)") +
  xlim(1, 40) +
  ylim(-1, 2)
```

ðŸ›‘ *This model produces predictions outside of 0 and 1.*

## Let's try another model

```{r}
#| label: logistic-model-plot
#| echo: false

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method ="glm", method.args = list(family = "binomial"), 
              fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)") +
  xlim(1, 40) +
  ylim(-0.5, 1.5)
```

*âœ… This model (called a **logistic regression model**) only produces predictions between 0 and 1.*

# Probabilities and odds

## Binary response variable

::: incremental
-   $Y = 1: \text{ yes}, 0: \text{ no}$
-   $\pi$: **probability** that $Y=1$, i.e., $P(Y = 1)$
-   $\frac{\pi}{1-\pi}$: **odds** that $Y = 1$
-   $\log\big(\frac{\pi}{1-\pi}\big)$: **log odds**
-   Go from $\pi$ to $\log\big(\frac{\pi}{1-\pi}\big)$ using the **logit transformation**
:::

## From odds to probabilities {.incremental}

(1) **Logistic model**: log odds = $\log\big(\frac{\pi}{1-\pi}\big) = \mathbf{X}\boldsymbol{\beta}$
(2) **Odds =** $\exp\big\{\log\big(\frac{\pi}{1-\pi}\big)\big\} = \frac{\pi}{1-\pi}$
(3) Combining (1) and (2) with what we saw earlier

. . .

$$\text{probability} = \pi = \frac{\exp\{\mathbf{X}\boldsymbol{\beta}\}}{1 + \exp\{\mathbf{X}\boldsymbol{\beta}\}}$$

## Sigmoid Function

We call this function relating the probability to the predictors a **sigmoid function,** $$
\sigma(x) = \frac{\exp\{x\}}{1 + \exp\{x\}}= \frac{1}{1+\text{exp}\{-x\}}.$$

## Sigmoid Function

```{r}
#| echo: false
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

# Generate data for plotting
x_values <- seq(-10, 10, by = 0.1)
y_values <- sigmoid(x_values)
data <- data.frame(x = x_values, y = y_values)

# Plot the sigmoid function
ggplot(data, aes(x = x, y = y)) +
  geom_line(color = "blue") +
  labs(x = expression(x), y = expression(sigma(x))) +
  ggtitle("Sigmoid Function")
```

. . .

# Logistic regression

## Logistic regression model {.midi}

**Logit form**: $$\text{logit}(\pi) = \log\big(\frac{\pi}{1-\pi}\big) = \mathbf{X}\boldsymbol{\beta}$$

. . .

**Probability form**:

$$
\pi = \frac{\exp\{\mathbf{X}\boldsymbol{\beta}\}}{1 + \exp\{\mathbf{X}\boldsymbol{\beta} \}}  
$$

. . .

**Logit and sigmoid link functions are inverses of each other.**

::: callout-note
More on link functions later, if time permits
:::

## Goal

We want to use our data to estimate $\boldsymbol{\beta}$ (find $\hat{\boldsymbol{\beta}}$) and obtain the model:

$$
\hat\pi = \frac{\exp\{\mathbf{X}\hat{\boldsymbol\beta}\}}{ 1 + \exp\{\mathbf{X}\hat{\boldsymbol\beta}\}}
$$

In this modeling scheme, one typically finds $\hat{\boldsymbol{\beta}}$ by maximizing the **likelihood function**.

## Linear Regression vs. Logistic Regression

::::: columns
::: {.column width="50%"}
**Linear regression:**

-   Quantitative outcome

-   $y_i = x_i^\top \boldsymbol{\beta} + \epsilon_i$

-   $E[Y_i] = x_i^\top \boldsymbol{\beta}$

-   Estimate $\boldsymbol\beta$

-   Use $\hat{\boldsymbol\beta}$ to predict $\hat y_i$
:::

::: {.column width="50%"}
**Logistic regression:**

-   Binary outcome

-   $\log\left(\frac{\pi_i}{1-\pi_i}\right) = x_i^\top \boldsymbol{\beta}$

-   $E[Y_i] = \pi_i$

-   Estimate $\boldsymbol\beta$

-   Use $\hat{\boldsymbol\beta}$ to predict $\hat \pi_i$
:::
:::::

## Likelihood function for $\boldsymbol{\beta}$

-   $P(Y_i = 1) = \pi_i$. What likelihood function should we use?

. . .

-   $f(y_i | x_i, \boldsymbol{\beta}) = \pi_i^{y_i} (1-\pi_i)^{1-y_i}$

. . .

-   $Y_i$'s are independent, so

$$f(y_1, \dots, y_n) = \prod_{i=1}^n \pi_i^{y_i} (1-\pi_i)^{1-y_i}$$

## Likelihood

The likelihood function for $\boldsymbol\beta$ is

$$
\begin{aligned}
L&(\boldsymbol\beta| x_1, \dots, x_n, y_1, \dots, y_n) \\[5pt]
 &=  \prod_{i=1}^n \pi_i^{y_i} (1-\pi_i)^{1-y_i} \\[10pt]
\end{aligned}
$$ <br>

. . .

We will use the log-likelihood function to find the MLEs

## Log-likelihood

The log-likelihood function for $\boldsymbol\beta$ is

$$
\begin{aligned}
\log &L(\boldsymbol\beta | x_1, \dots, x_n, y_1, \dots, y_n) 
  \\[8pt]
& = \sum_{i=1}^n\log(\pi_i^{y_i}(1-\pi_i)^{1-y_i})\\
&= \sum_{i=1}^n\left(y_i\log (\pi_i) + (1-y_i)\log(1-\pi_i)\right)\\
\end{aligned}
$$

## Log-likelihood

Plugging in $\pi_i = \frac{\exp\{x_i^\top \boldsymbol\beta\}}{1+\exp\{x_i^\top \boldsymbol\beta\}}$ and simplifying, we get:

$$
\begin{aligned}\log &L(\boldsymbol\beta | x_1, \dots, x_n, y_1, \dots, y_n) \\
&= \sum_{i=1}^n y_i x_i^\top \boldsymbol\beta - \sum_{i=1}^n \log(1+ \exp\{x_i^\top \beta\})
\end{aligned}
$$

## Finding the MLE

-   Taking the derivative:

$$
\begin{aligned}
\frac{\partial \log L}{\partial \boldsymbol\beta} =\sum_{i=1}^n y_i x_i^\top
&- \sum_{i=1}^n \frac{\exp\{x_i^\top \boldsymbol\beta\} x_i^\top}{1+\exp\{x_i^\top \boldsymbol\beta\}}
\end{aligned}
$$

. . .

-   If we set this to zero, there is no closed form solution.

. . .

-   R uses numerical approximation to find the MLE.

# Example

## Risk of coronary heart disease {.midi}

This data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.

-   `high_risk`: 1 = High risk of having heart disease in next 10 years, 0 = Not high risk of having heart disease in next 10 years

-   `age`: Age at exam time (in years)

-   `education`: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College

```{r}
#| echo: false

heart_disease <- read_csv(here::here("slides", "data/framingham.csv"))|>
  select(age, education, TenYearCHD) |>
  drop_na() |>
  mutate(high_risk = as_factor(TenYearCHD), 
         education = as_factor(education))
```

## Data: `heart_disease`

```{r}
#| echo: false
heart_disease <- heart_disease |>
  mutate(
    high_risk_names = if_else(high_risk == "1", "High risk", "Not high risk"),
    education_names = case_when(
      education == "1" ~ "Some high school",
      education == "2" ~ "High school or GED",
      education == "3" ~ "Some college or vocational school",
      education == "4" ~ "College"
    ),
    education_names = fct_relevel(education_names, "Some high school", "High school or GED", "Some college or vocational school", "College")
  )


heart_disease |>
  select(age, education, high_risk)
```

## Univariate EDA

```{r}
#| echo: false

p1 <- ggplot(heart_disease, aes(x = high_risk)) +
  geom_bar(fill = "steelblue", color = "black")+
  labs(x = "",
       title = "High risk of heart disease")

p2 <- ggplot(heart_disease, aes(x = age)) +
  geom_histogram(fill = "steelblue", color = "black", binwidth = 2) +
  labs(x = "",
       title = "Age")

p3 <- ggplot(heart_disease, aes(x = education)) +
  geom_bar(fill = "steelblue", color = "black")+
  labs(x = "",
       title = "Education")

p1  + (p2 / p3)
```

## Bivariate EDA 

```{r}
#| echo: false

p1 <- ggplot(heart_disease, aes(x = high_risk, y = age)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "High risk - 1: yes, 0: no",
       y = "Age", 
       title = "High risk vs. age")

p2 <- ggplot(heart_disease, aes(x = education, fill = high_risk)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "Education",
    fill = "High risk", 
    title = "High risk vs. education") +
  scale_fill_viridis_d() +
  theme(legend.position = "bottom")

p1 + p2
```

## Bivariate EDA code

```{r}
#| eval: false 

{r}
#| echo: false

p1 <- ggplot(heart_disease, aes(x = high_risk, y = age)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "High risk - 1: yes, 0: no",
       y = "Age")

p2 <- ggplot(heart_disease, aes(x = education, fill = high_risk)) +
  geom_bar(position = "fill", color = "black") +
  labs(x = "Education",
    fill = "High risk") +
  scale_fill_viridis_d() +
  theme(legend.position = "bottom")

p1 + p2

```

## Let's fit the model

::: small
```{r}
#| echo: true
#| code-line-numbers: "1|2|3"

heart_edu_age_fit <- glm(high_risk ~ age + education, 
                         data  = heart_disease, 
                         family = "binomial")
```

```{r}
#| label: heart-edu-age-tidy
#| echo: false
tidy(heart_edu_age_fit) |>
  kable(digits = 3)
```

$$
\begin{aligned}
\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) =& -5.385 + 0.073 \times \text{age} - 0.242\times \text{education}_2 \\
&- 0.235\times\text{education}_3 - 0.020 \times\text{education}_4
\end{aligned}
$$ where $\hat{\pi}$ is the predicted probability of being high risk of having heart disease in the next 10 years
:::

## Interpretation in terms of log-odds

::: small
```{r}
#| ref.label: heart-edu-age-tidy
#| echo: false
```
:::

`education4`: The **log-odds** of being high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, **holding age constant.**

. . .

::: callout-warning
We would not use the interpretation in terms of log-odds in practice.
:::

## Interpretation in terms of log-odds

::: small
```{r}
#| ref.label: heart-edu-age-tidy
#| echo: false
```
:::

`age`: For each additional year in age, the log-odds of being high risk for heart disease are expected to increase by 0.073, **holding education level constant.**

. . .

::: callout-warning
We would not use the interpretation in terms of log-odds in practice.
:::

## Interpretation in terms of odds {.midi}

::: small
```{r}
#| ref.label: heart-edu-age-tidy
#| echo: false
```
:::

`education4`: The **odds** of being high risk for heart disease for those with a college degree are expected to be `r round(exp(-0.020),3)` (exp{-0.020}) **times** the odds for those with some high school, **holding age constant**.

::: callout-note
In logistic regression with 2+ predictors, $exp\{\hat{\beta}_j\}$ is often called the **adjusted odds ratio (AOR).**
:::

## Interpretation in terms of odds {.midi}

::: small
```{r}
#| ref.label: heart-edu-age-tidy
#| echo: false
```
:::

`age`: For each additional year in age, the odds being high risk for heart disease are expected to multiply by a factor of `r round(exp(0.073),2)` (`exp(0.073)`), **holding education level constant**.

**Alternate interpretation:** For each additional year in age, the odds of being high risk for heart disease are expected to increase by 8%.

::: callout-note
In logistic regression with 2+ predictors, $exp\{\hat{\beta}_j\}$ is often called the **adjusted odds ratio (AOR).**
:::

# Generalized Linear Models

## Introduction to GLMs

::: incremental
-   Wider class of models.
-   Response variable does not have to be continuous and/or normal.
-   Variance does not have to be constant
-   Still need to specify distribution of outcome variable (randomness).
-   Does not require a linear relationship between response and explanatory variable. Instead, assumes linear relationship between the transformed expected response (ex. $\text{logit}(\pi_i)$) and predictors.
:::

## Generalization of Linear Model

::: midi
**Linear model**

$E[Y_i] = \mu_i = x_i^\top\boldsymbol\beta$.

$Y_i \overset{ind}{\sim} N(\mu_i, \sigma^2)$.
:::

. . .

::: midi
**GLM**

$g\left(E[Y_i]\right) = g(\mu_i)  = x_i^\top \beta$. Alternatively, $\mu_i \sim g^{-1}(x_i^\top \beta)$.

$Y_i \overset{ind}{\sim} f(\mu_i)$.
:::

. . .

::: callout-note
We call $g$ a link function
:::

## Examples of link functions

**Linear regression**

$g(\mu_i) = \mu_i$ and $Y_i\sim N(\mu_i,\sigma^2)$.

. . .

**Logistic regression**

$g(\pi_i) = \text{logit}(\pi_i)$ (note, $E[Y_i] = \pi_i$). $Y_i \sim \text{Bernoulli}(\pi_i)$. Alternatively, $\pi_i = \sigma(x_i^\top\boldsymbol\beta)$ where $\sigma$ is the sigmoid function.

. . .

**Probit model**

$\pi_i = \Phi(x_i^\top\boldsymbol\beta)$, where $\Phi$ is the cdf of standard normal. $Y_i \sim \text{Bernoulli}(\pi_i)$. $g(\pi_i) = \Phi^{-1}(\pi_i)$ is called a probit link.

# Prediction

## Predicted log odds {.midi}

```{r}
heart_disease_aug = 
  augment(heart_edu_age_fit) 
heart_disease_aug|> select(.fitted, .resid)|>
  head(6)
```

. . .

**For observation 1**

$$\text{predicted odds} = \hat{\omega} = \frac{\hat{\pi}}{1-\hat{\pi}} = \exp\{-2.548\} = 0.078$$

## Predicted probabilities {.midi}

```{r}

heart_disease_aug$predicted_prob <- 
  predict.glm(heart_edu_age_fit, heart_disease, type = "response")
heart_disease_aug|>
  select(.fitted,predicted_prob) |>
  head(6)
```

. . .

**For observation 1**

$$\text{predicted probability} = \hat{\pi} = \frac{\exp\{-2.548\}}{1 + \exp\{-2.548\}} = 0.073$$

## Predicted classes

```{r}
# Convert probabilities to binary predictions (0 or 1)
heart_disease_aug <- heart_disease_aug |>
  mutate(predicted_class =  ifelse(predicted_prob > 0.5, 1, 0))
heart_disease_aug |>
  select(predicted_prob, predicted_class) 

```

## Observed vs. predicted

::: question
What does the following table show?
:::

```{r}
heart_disease_aug|>
  count(high_risk, predicted_class)
```

. . .

::: question
The `predicted_class` is the class with the probability of occurring higher than 0.5. What is a limitation to using this method to determine the predicted class?
:::

## Recap

-   Reviewed the relationship between odds and probabilities

-   Introduced logistic regression for binary response variable

-   Interpreted the coefficients of a logistic regression model with multiple predictors

-   Introduced generalized linear model

## Questions from this week's content?

<center>

```{=html}
<iframe width="640px" height="480px" src="https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTBw0r2_bKCVMs9lST1_-2sxUQ1JTSFBZNlFMWUJZSDcwTUdaVzgwWUhBMC4u&embed=true" frameborder="0" marginwidth="0" marginheight="0" style="border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>
```

</center>
